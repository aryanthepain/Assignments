{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4924c10",
   "metadata": {},
   "source": [
    "# Fisher's Linear Discriminant (LDA) — from scratch\n",
    "\n",
    "Implement Fisher's criterion and LDA step-by-step. We'll compute projection vector w that maximizes class separation, visualize projections and classification, and discuss common pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb417f5c",
   "metadata": {},
   "source": [
    "## What this notebook contains\n",
    "\n",
    "- Derivation in code: within-class scatter, between-class scatter\n",
    "- Fit/predict functions implemented in numpy\n",
    "- Examples on synthetic data and (optionally) Iris dataset\n",
    "- FAQs and practical notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02743184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f3e39",
   "metadata": {},
   "source": [
    "## Implementation notes\n",
    "\n",
    "Fisher's LDA for two classes finds direction w that maximizes (w^T S_B w) / (w^T S_W w) where S_B is between-class scatter and S_W is within-class scatter. For two classes w is proportional to S_W^{-1}(m1 - m2). We'll implement the two-class case from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b462032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_lda_fit(X, y, regularize=1e-6):\n",
    "    \"\"\"Fit two-class Fisher LDA.\n",
    "    X: (n_samples, d)\n",
    "    y: labels (n_samples,) with exactly two distinct classes\n",
    "    returns:\n",
    "      w: projection vector (d,)\n",
    "      threshold: scalar on projected axis (midpoint of class projected means)\n",
    "      stats: dict with class means and scatter matrices\n",
    "    regularize: small diagonal addition to S_W for numerical stability\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    classes = np.unique(y)\n",
    "    if classes.shape[0] != 2:\n",
    "        raise ValueError('This simple implementation handles exactly two classes.')\n",
    "    c0, c1 = classes\n",
    "    X0 = X[y == c0]\n",
    "    X1 = X[y == c1]\n",
    "    m0 = X0.mean(axis=0)\n",
    "    m1 = X1.mean(axis=0)\n",
    "    # within-class scatter\n",
    "    S_w = np.cov(X0, rowvar=False, bias=True) * X0.shape[0] + np.cov(X1, rowvar=False, bias=True) * X1.shape[0]\n",
    "    # regularize\n",
    "    S_w += regularize * np.eye(S_w.shape[0])\n",
    "    # direction\n",
    "    w = np.linalg.solve(S_w, (m1 - m0))\n",
    "    # scale doesn't matter for direction, but keep it normalized for convenience\n",
    "    w = w / np.linalg.norm(w)\n",
    "    # projections and threshold (midpoint between projected means)\n",
    "    proj0 = X0.dot(w)\n",
    "    proj1 = X1.dot(w)\n",
    "    mean0 = proj0.mean(); mean1 = proj1.mean()\n",
    "    threshold = 0.5 * (mean0 + mean1)\n",
    "    stats = {'m0': m0, 'm1': m1, 'S_w': S_w, 'proj_mean0': mean0, 'proj_mean1': mean1}\n",
    "    return w, threshold, stats\n",
    "\n",
    "def fisher_predict(X, w, threshold):\n",
    "    proj = X.dot(w)\n",
    "    # predict class 1 if projection > threshold\n",
    "    return (proj > threshold).astype(int), proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b20040",
   "metadata": {},
   "source": [
    "### Synthetic demo (two Gaussian clouds)\n",
    "\n",
    "Create two 2D Gaussian clusters, fit LDA, visualize the separating direction and projected histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad65603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic data\n",
    "np.random.seed(10)\n",
    "n = 200\n",
    "X0 = np.random.multivariate_normal([-1.5, 0], [[0.6, 0.2],[0.2, 0.6]], size=n//2)\n",
    "X1 = np.random.multivariate_normal([1.0, 0.5], [[0.5, -0.15],[-0.15, 0.5]], size=n//2)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0]*(n//2) + [1]*(n//2))\n",
    "\n",
    "w, thr, stats = fisher_lda_fit(X, y)\n",
    "preds, proj = fisher_predict(X, w, thr)\n",
    "\n",
    "# plot data and line for direction w\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X0[:,0], X0[:,1], label='class 0', alpha=0.6)\n",
    "plt.scatter(X1[:,0], X1[:,1], label='class 1', alpha=0.6)\n",
    "# plot direction through overall mean\n",
    "mean_all = X.mean(axis=0)\n",
    "line_x = np.linspace(mean_all[0]-4, mean_all[0]+4, 50)\n",
    "line_y = mean_all[1] + (w[1]/w[0])*(line_x - mean_all[0])\n",
    "plt.plot(line_x, line_y, color='k', linestyle='--', label='Fisher direction')\n",
    "plt.legend()\n",
    "plt.title('Data and Fisher direction')\n",
    "\n",
    "# projections histogram\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(proj[y==0], bins=25, alpha=0.6, label='class 0')\n",
    "plt.hist(proj[y==1], bins=25, alpha=0.6, label='class 1')\n",
    "plt.axvline(thr, color='k', linestyle='--', label='threshold')\n",
    "plt.legend()\n",
    "plt.title('Projections on w')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# accuracy\n",
    "acc = (preds == y).mean()\n",
    "print(f'Classification accuracy (simple threshold on projection): {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c76e8",
   "metadata": {},
   "source": [
    "### Optional: Iris dataset (multi-class)\n",
    "\n",
    "Fisher's LDA generalizes to multiple classes; one common approach is to compute projection vectors from generalized eigenvalue problem S_B v = lambda S_W v. In this notebook we implemented two-class explicitly. Below we provide a short sketch code: try extending to multiple classes as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93940dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick attempt at using sklearn's iris if available (optional)\n",
    "try:\n",
    "    from sklearn import datasets\n",
    "    iris = datasets.load_iris()\n",
    "    X_iris = iris.data[:, :2]  # take first two features for easy plotting\n",
    "    y_iris = iris.target\n",
    "    print('Iris loaded: demo left as exercise — this notebook focuses on two-class LDA.')\n",
    "except Exception as e:\n",
    "    print('sklearn not available in this environment or failed to import:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d99e9",
   "metadata": {},
   "source": [
    "## Common questions (FAQ)\n",
    "\n",
    "Q: Why regularize S_W?\n",
    "A: To avoid singular matrices when classes have fewer samples than dimensions or data is nearly collinear. Add a small diagonal term.\n",
    "\n",
    "Q: How does LDA differ from PCA?\n",
    "A: PCA is unsupervised and seeks directions of maximum variance. LDA is supervised and seeks directions that maximize class separability.\n",
    "\n",
    "Q: Multi-class LDA?\n",
    "A: Solve generalized eigenproblem S_B v = lambda S_W v and pick top eigenvectors; dimensionality limited to (C-1) where C is number of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab30a2",
   "metadata": {},
   "source": [
    "## Exercises / next steps\n",
    "\n",
    "- Implement the multi-class generalized eigenvalue LDA\n",
    "- Add cross-validation for regularization strength\n",
    "- Compare Fisher LDA classification to logistic regression on the same dataset\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
