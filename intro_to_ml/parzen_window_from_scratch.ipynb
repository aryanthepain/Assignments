{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e22e3d",
   "metadata": {},
   "source": [
    "# Parzen Window (Kernel Density Estimation) — from scratch\n",
    "\n",
    "Implementing Parzen window (a.k.a. kernel density estimation) step-by-step. This notebook focuses on intuition, a clean numpy implementation, visual checks, and small applications (anomaly detection, simple classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab3c31a",
   "metadata": {},
   "source": [
    "## What this notebook contains\n",
    "\n",
    "- A minimal, readable Parzen (KDE) implementation in numpy\n",
    "- 1D and 2D demos with plots\n",
    "- Common questions and troubleshooting notes\n",
    "- Basic application: anomaly detection using low-density threshold\n",
    "\n",
    "Style: educational, with inline comments and Q&A blocks for typical implementation questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper for 2D plotting\n",
    "from matplotlib import cm\n",
    "\n",
    "# set a default random seed for reproducibility\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83da6f",
   "metadata": {},
   "source": [
    "## Parzen estimator (1D and 2D)\n",
    "\n",
    "We'll implement a flexible Parzen estimator that supports Gaussian and uniform kernels. The Parzen estimate at a point x is the averaged kernel values centered at each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16797800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(dist_sq, h):\n",
    "    # dist_sq: squared Euclidean distance\n",
    "    # h: bandwidth (scalar)\n",
    "    # returns kernel value (unnormalized for multidimensional case we divide appropriately)\n",
    "    d = 1  # we'll divide by (sqrt(2*pi)*h)**d later in 1D/2D cases explicitly\n",
    "    return np.exp(-0.5 * dist_sq / (h**2))\n",
    "\n",
    "def uniform_kernel(dist_sq, h):\n",
    "    # dist_sq: squared Euclidean distance\n",
    "    # uniform kernel: 1 inside radius h, 0 outside\n",
    "    return (dist_sq <= h**2).astype(float)\n",
    "\n",
    "def parzen_pdf(X_train, X_eval, h=0.5, kernel='gaussian'):\n",
    "    \"\"\"Estimate density at points X_eval given training points X_train using Parzen windows.\n",
    "    X_train: (n_samples, d)\n",
    "    X_eval: (m_points, d)\n",
    "    h: bandwidth (scalar)\n",
    "    kernel: 'gaussian' or 'uniform'\n",
    "    returns: (m_points,) density estimates\n",
    "    \"\"\"\n",
    "    X_train = np.asarray(X_train)\n",
    "    X_eval = np.asarray(X_eval)\n",
    "    n, d = X_train.shape\n",
    "    m = X_eval.shape[0]\n",
    "    out = np.zeros(m, dtype=float)\n",
    "    if kernel == 'gaussian':\n",
    "        # Normalizing constant for d dimensions: (2*pi)^(d/2) * h^d\n",
    "        norm = (2 * np.pi)**(d/2) * (h**d)\n",
    "        for i in range(m):\n",
    "            diff = X_train - X_eval[i]  # (n, d)\n",
    "            dist_sq = np.sum(diff**2, axis=1)\n",
    "            k = gaussian_kernel(dist_sq, h)\n",
    "            out[i] = np.sum(k) / (n * norm)\n",
    "    elif kernel == 'uniform':\n",
    "        # Volume of d-dim ball of radius h: for 1D it's 2h, for 2D it's pi*h^2\n",
    "        if d == 1:\n",
    "            vol = 2*h\n",
    "        elif d == 2:\n",
    "            vol = np.pi * (h**2)\n",
    "        else:\n",
    "            # keep general but warn: uniform kernel normalization for high-d is rarely used here\n",
    "            vol = (2*np.pi)**(d/2) * (h**d)  # fallback\n",
    "        for i in range(m):\n",
    "            diff = X_train - X_eval[i]\n",
    "            dist_sq = np.sum(diff**2, axis=1)\n",
    "            k = uniform_kernel(dist_sq, h)\n",
    "            out[i] = np.sum(k) / (n * vol)\n",
    "    else:\n",
    "        raise ValueError('Unknown kernel: choose gaussian or uniform')\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecaeb99",
   "metadata": {},
   "source": [
    "### 1D demo: visualize Parzen estimate vs true distribution\n",
    "\n",
    "We'll sample from a mixture of Gaussians and compare the estimated density for different bandwidths. Questions to keep in mind: How does bandwidth h affect smoothness? What happens when h is too small or too large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04726709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1D data: mixture of gaussians\n",
    "np.random.seed(1)\n",
    "n = 200\n",
    "x1 = np.random.normal(-2, 0.5, size=n//2)\n",
    "x2 = np.random.normal(2, 0.8, size=n//2)\n",
    "X = np.concatenate([x1, x2])[:, None]  # shape (n,1)\n",
    "\n",
    "# evaluation points\n",
    "xs = np.linspace(-6, 6, 400)[:, None]\n",
    "\n",
    "for h in [0.1, 0.4, 1.0]:\n",
    "    p_hat = parzen_pdf(X, xs, h=h, kernel='gaussian')\n",
    "    plt.plot(xs[:,0], p_hat, label=f'h={h}')\n",
    "# plot train points (rug)\n",
    "plt.scatter(X[:,0], np.zeros_like(X[:,0]) - 0.002, marker='|', color='k', s=40)\n",
    "plt.legend()\n",
    "plt.title('Parzen density estimate (1D) — effect of bandwidth')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612dac4",
   "metadata": {},
   "source": [
    "### 2D demo: contour estimate\n",
    "\n",
    "Use a mixture of 2D Gaussians and visualize contour of estimated density. This is a common check to see modes and smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D demo\n",
    "np.random.seed(2)\n",
    "n = 300\n",
    "# two 2D Gaussian clusters\n",
    "c1 = np.random.multivariate_normal(mean=[-2, -1], cov=[[0.6, 0.2],[0.2, 0.6]], size=n//2)\n",
    "c2 = np.random.multivariate_normal(mean=[2, 2], cov=[[0.8, -0.3],[-0.3, 0.8]], size=n//2)\n",
    "X2 = np.vstack([c1, c2])\n",
    "\n",
    "# grid\n",
    "xx, yy = np.meshgrid(np.linspace(-6,6,120), np.linspace(-6,6,120))\n",
    "grid = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "# estimate density on grid\n",
    "p_grid = parzen_pdf(X2, grid, h=0.7, kernel='gaussian')\n",
    "p_grid = p_grid.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.contourf(xx, yy, p_grid, levels=30, cmap=cm.viridis)\n",
    "plt.scatter(X2[:,0], X2[:,1], s=10, edgecolor='k', alpha=0.6)\n",
    "plt.title('Parzen (KDE) contour (2D) — Gaussian kernel, h=0.7')\n",
    "plt.xlabel('x1'); plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a3545",
   "metadata": {},
   "source": [
    "## Basic application: anomaly detection\n",
    "\n",
    "Flag points as anomalies if their estimated density is below a chosen threshold. This is simple but effective for low-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ddc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create normal points and some anomalies\n",
    "np.random.seed(3)\n",
    "clean = np.random.normal(0, 1, size=(300,1))\n",
    "anoms = np.array([[5.0],[ -5.0], [3.5]])  # clear outliers\n",
    "X_ad = np.vstack([clean, anoms])\n",
    "\n",
    "# estimate densities\n",
    "p_est = parzen_pdf(clean, X_ad, h=0.6, kernel='gaussian')\n",
    "\n",
    "# choose threshold as low quantile of clean densities\n",
    "thr = np.quantile(parzen_pdf(clean, clean, h=0.6), 0.02)\n",
    "labels = p_est < thr\n",
    "\n",
    "print('Threshold (2nd percentile):', thr)\n",
    "for xi, pi, is_anom in zip(X_ad, p_est, labels):\n",
    "    print(f'x={xi[0]:.2f}, p~={pi:.3e}, anomaly={bool(is_anom)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2e804",
   "metadata": {},
   "source": [
    "## Common implementation questions (FAQ)\n",
    "\n",
    "Q: How do I select bandwidth h?\n",
    "A: There's no free lunch. For Gaussian kernels, rules of thumb (Silverman's rule) exist for 1D; cross-validation is practical. Inspect plots for under/oversmoothing.\n",
    "\n",
    "Q: Why does Parzen get worse in high dimensions?\n",
    "A: Curse of dimensionality: required samples grow exponentially. Bandwidth selection and kernel choice become fragile.\n",
    "\n",
    "Q: Can I use this for classification?\n",
    "A: Yes — estimate class-conditional densities p(x|y) with Parzen for each class and apply Bayes rule to classify.\n",
    "\n",
    "Q: Performance tips\n",
    "- Use vectorized linear algebra and broadcast where possible\n",
    "- For large data, use FFT-based KDE or libraries (scipy, sklearn) or approximate methods (KD-trees)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c8eb3",
   "metadata": {},
   "source": [
    "## Next steps / exercises\n",
    "\n",
    "- Implement Parzen with Gaussian kernel but using vectorized broadcasting for speed\n",
    "- Add cross-validation to choose h\n",
    "- Try class-conditional Parzen for simple classification on synthetic labeled data\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
