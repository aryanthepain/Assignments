{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2p37fE9Fc02"
      },
      "source": [
        "# Generating the sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vznTNRnhziVm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.5000],\n",
            "        [0.9000, 0.7000],\n",
            "        [0.4000, 0.6000],\n",
            "        [0.3000, 0.4000],\n",
            "        [1.1000, 0.8000],\n",
            "        [0.6000, 0.9000]])\n",
            "\n",
            "y\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Sample dataset (X: input, y: labels)\n",
        "X = torch.tensor([[1.0, 0.5],\n",
        "                  [0.9, 0.7],\n",
        "                  [0.4, 0.6],\n",
        "                  [0.3, 0.4],\n",
        "                  [1.1, 0.8],\n",
        "                  [0.6, 0.9]], dtype=torch.float32)\n",
        "\n",
        "y = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1]], dtype=torch.float32)\n",
        "\n",
        "print(\"X\")\n",
        "print(type(X)) \n",
        "print(X)\n",
        "print(\"\\ny\")\n",
        "print(type(y))\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho4gkN7hlIn3"
      },
      "source": [
        "# Standardization + Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "egMKpa5siM4E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[1.0000, 0.5000],\n",
            "        [0.9000, 0.7000],\n",
            "        [0.4000, 0.6000],\n",
            "        [0.3000, 0.4000],\n",
            "        [1.1000, 0.8000],\n",
            "        [0.6000, 0.9000]])\n",
            "\n",
            "data_standardized\n",
            "<class 'numpy.ndarray'>\n",
            "[[ 0.93724032 -0.87831011]\n",
            " [ 0.60644953  0.29276996]\n",
            " [-1.04750392 -0.2927699 ]\n",
            " [-1.37829461 -1.46385014]\n",
            " [ 1.26803111  0.87831017]\n",
            " [-0.38592244  1.46385003]]\n",
            "\n",
            "data_normalized\n",
            "<class 'numpy.ndarray'>\n",
            "[[0.87499997 0.2       ]\n",
            " [0.74999994 0.6       ]\n",
            " [0.12499999 0.40000006]\n",
            " [0.         0.        ]\n",
            " [1.         0.80000006]\n",
            " [0.37500001 1.        ]]\n",
            "\n",
            "X\n",
            "<class 'torch.Tensor'>\n",
            "tensor([[0.8750, 0.2000],\n",
            "        [0.7500, 0.6000],\n",
            "        [0.1250, 0.4000],\n",
            "        [0.0000, 0.0000],\n",
            "        [1.0000, 0.8000],\n",
            "        [0.3750, 1.0000]])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "print(\"X\")\n",
        "print(type(X))\n",
        "print(X)\n",
        "\n",
        "# Z-score Scaling (Standardization) data to have mean = 0 and std = 1\n",
        "standard_scaler = StandardScaler()\n",
        "data_standardized = standard_scaler.fit_transform(X)\n",
        "print(\"\\ndata_standardized\")\n",
        "print(type(data_standardized))\n",
        "print(data_standardized)\n",
        "\n",
        "# Min-Max Scaling (Normalization)  want values scaled to a specific range\n",
        "min_max_scaler = MinMaxScaler()\n",
        "data_normalized = min_max_scaler.fit_transform(data_standardized)\n",
        "print(\"\\ndata_normalized\")\n",
        "print(type(data_normalized))\n",
        "print(data_normalized)\n",
        "\n",
        "X = torch.tensor(data_normalized, dtype=torch.float32)\n",
        "print(\"\\nX\")\n",
        "print(type(X))\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l788XGfLlQo9"
      },
      "source": [
        "# Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ileFUyXnoBLu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created successfully\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 3)  # First layer (2 inputs, 3 neurons)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(3, 2)  # Output layer (3 inputs, 2 output)\n",
        "        self.act2 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # model.state_dict() returns the state of the model from where we can get all the parameters and other model related info.\n",
        "        initial_params = self.state_dict()\n",
        "\n",
        "        # Print parameters\n",
        "        for name, param in initial_params.items():\n",
        "          print(f\"\\nParameters of {name}: \\n{param}\\n\")\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        print(f\"\\nActivation after first hidden layer: \\n{x}\")  # Print activations of the first layer\n",
        "        x = self.act1(x)\n",
        "        print(f\"\\nNon linearity after first Relu layer: \\n{x}\")  # Print non linearity of the first layer\n",
        "        x = self.fc2(x)\n",
        "        print(f\"\\nActivation after second output layer: \\n{x}\")  # Print activations of the second layer\n",
        "        x = self.act2(x)\n",
        "        print(f\"\\nOutput after second output layer (sigmoid): \\n{x}\")  # Print output of the model\n",
        "        return x\n",
        "\n",
        "# Instantiate model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "print(\"Model created successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq4BpJxrn5np"
      },
      "source": [
        "# Training & evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzol35NLzvxo",
        "outputId": "0289b61b-ad1f-4d28-f464-cadac4dc3a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X :\n",
            " tensor([[0.8750, 0.2000],\n",
            "        [0.7500, 0.6000],\n",
            "        [0.1250, 0.4000],\n",
            "        [0.0000, 0.0000],\n",
            "        [1.0000, 0.8000],\n",
            "        [0.3750, 1.0000]])\n",
            "y :\n",
            " tensor([[1., 0.],\n",
            "        [1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 1.],\n",
            "        [1., 0.],\n",
            "        [0., 1.]])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4259,  0.3217],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0214, -0.1681]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5900, -0.5869,  0.2450])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2627,  0.3793, -0.1927],\n",
            "        [ 0.0893, -0.3822,  0.3059]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4352,  0.5357])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1531, -0.7534,  0.2301],\n",
            "        [-0.0776, -0.4745,  0.1601],\n",
            "        [-0.4081, -0.3897,  0.1804],\n",
            "        [-0.5900, -0.5869,  0.2450],\n",
            "        [ 0.0932, -0.4371,  0.1319],\n",
            "        [-0.1087, -0.1142,  0.0849]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2301],\n",
            "        [0.0000, 0.0000, 0.1601],\n",
            "        [0.0000, 0.0000, 0.1804],\n",
            "        [0.0000, 0.0000, 0.2450],\n",
            "        [0.0932, 0.0000, 0.1319],\n",
            "        [0.0000, 0.0000, 0.0849]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4795,  0.6060],\n",
            "        [-0.4660,  0.5847],\n",
            "        [-0.4699,  0.5908],\n",
            "        [-0.4824,  0.6106],\n",
            "        [-0.4361,  0.5843],\n",
            "        [-0.4515,  0.5616]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3824, 0.6470],\n",
            "        [0.3856, 0.6421],\n",
            "        [0.3846, 0.6436],\n",
            "        [0.3817, 0.6481],\n",
            "        [0.3927, 0.6421],\n",
            "        [0.3890, 0.6368]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [1/10], Loss(item): 0.7275259494781494, Loss: 0.7275259494781494\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0085, -0.0068],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0612,  0.0203]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0085,  0.0000,  0.0329])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0047,  0.0000, -0.0104],\n",
            "        [ 0.0050,  0.0000,  0.0129]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0570,  0.0716])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4259,  0.3217],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0214, -0.1681]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5900, -0.5869,  0.2450])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2627,  0.3793, -0.1927],\n",
            "        [ 0.0893, -0.3822,  0.3059]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4352,  0.5357])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4269,  0.3227],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0204, -0.1691]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5890, -0.5869,  0.2440])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2637,  0.3793, -0.1917],\n",
            "        [ 0.0883, -0.3822,  0.3049]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4342,  0.5347])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 2:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4269,  0.3227],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0204, -0.1691]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5890, -0.5869,  0.2440])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2637,  0.3793, -0.1917],\n",
            "        [ 0.0883, -0.3822,  0.3049]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4342,  0.5347])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1510, -0.7534,  0.2280],\n",
            "        [-0.0753, -0.4745,  0.1578],\n",
            "        [-0.4066, -0.3897,  0.1789],\n",
            "        [-0.5890, -0.5869,  0.2440],\n",
            "        [ 0.0960, -0.4371,  0.1291],\n",
            "        [-0.1063, -0.1142,  0.0825]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2280],\n",
            "        [0.0000, 0.0000, 0.1578],\n",
            "        [0.0000, 0.0000, 0.1789],\n",
            "        [0.0000, 0.0000, 0.2440],\n",
            "        [0.0960, 0.0000, 0.1291],\n",
            "        [0.0000, 0.0000, 0.0825]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4779,  0.6042],\n",
            "        [-0.4644,  0.5828],\n",
            "        [-0.4684,  0.5892],\n",
            "        [-0.4809,  0.6091],\n",
            "        [-0.4336,  0.5825],\n",
            "        [-0.4500,  0.5598]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3828, 0.6466],\n",
            "        [0.3859, 0.6417],\n",
            "        [0.3850, 0.6432],\n",
            "        [0.3820, 0.6477],\n",
            "        [0.3933, 0.6416],\n",
            "        [0.3894, 0.6364]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [2/10], Loss(item): 0.7272266745567322, Loss: 0.7272266745567322\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0086, -0.0069],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0609,  0.0202]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0086,  0.0000,  0.0327])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0049,  0.0000, -0.0101],\n",
            "        [ 0.0051,  0.0000,  0.0126]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0568,  0.0714])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4269,  0.3227],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0204, -0.1691]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5890, -0.5869,  0.2440])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2637,  0.3793, -0.1917],\n",
            "        [ 0.0883, -0.3822,  0.3049]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4342,  0.5347])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4279,  0.3237],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0194, -0.1701]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5880, -0.5869,  0.2430])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2647,  0.3793, -0.1907],\n",
            "        [ 0.0873, -0.3822,  0.3039]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4332,  0.5337])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 3:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4279,  0.3237],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0194, -0.1701]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5880, -0.5869,  0.2430])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2647,  0.3793, -0.1907],\n",
            "        [ 0.0873, -0.3822,  0.3039]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4332,  0.5337])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1489, -0.7534,  0.2259],\n",
            "        [-0.0729, -0.4745,  0.1554],\n",
            "        [-0.4051, -0.3897,  0.1773],\n",
            "        [-0.5880, -0.5869,  0.2430],\n",
            "        [ 0.0988, -0.4371,  0.1263],\n",
            "        [-0.1039, -0.1142,  0.0801]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2259],\n",
            "        [0.0000, 0.0000, 0.1554],\n",
            "        [0.0000, 0.0000, 0.1773],\n",
            "        [0.0000, 0.0000, 0.2430],\n",
            "        [0.0988, 0.0000, 0.1263],\n",
            "        [0.0000, 0.0000, 0.0801]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4762,  0.6023],\n",
            "        [-0.4628,  0.5809],\n",
            "        [-0.4670,  0.5876],\n",
            "        [-0.4795,  0.6075],\n",
            "        [-0.4311,  0.5807],\n",
            "        [-0.4484,  0.5580]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3831, 0.6462],\n",
            "        [0.3863, 0.6413],\n",
            "        [0.3853, 0.6428],\n",
            "        [0.3824, 0.6474],\n",
            "        [0.3939, 0.6412],\n",
            "        [0.3897, 0.6360]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [3/10], Loss(item): 0.7269282937049866, Loss: 0.7269282937049866\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0087, -0.0070],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0606,  0.0200]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0087,  0.0000,  0.0324])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0050,  0.0000, -0.0099],\n",
            "        [ 0.0053,  0.0000,  0.0124]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0566,  0.0712])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4279,  0.3237],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0194, -0.1701]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5880, -0.5869,  0.2430])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2647,  0.3793, -0.1907],\n",
            "        [ 0.0873, -0.3822,  0.3039]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4332,  0.5337])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4289,  0.3247],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0184, -0.1711]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5870, -0.5869,  0.2420])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2657,  0.3793, -0.1897],\n",
            "        [ 0.0863, -0.3822,  0.3029]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4322,  0.5327])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 4:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4289,  0.3247],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0184, -0.1711]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5870, -0.5869,  0.2420])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2657,  0.3793, -0.1897],\n",
            "        [ 0.0863, -0.3822,  0.3029]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4322,  0.5327])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1468, -0.7534,  0.2238],\n",
            "        [-0.0706, -0.4745,  0.1531],\n",
            "        [-0.4036, -0.3897,  0.1758],\n",
            "        [-0.5870, -0.5869,  0.2420],\n",
            "        [ 0.1016, -0.4371,  0.1235],\n",
            "        [-0.1015, -0.1142,  0.0777]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2238],\n",
            "        [0.0000, 0.0000, 0.1531],\n",
            "        [0.0000, 0.0000, 0.1758],\n",
            "        [0.0000, 0.0000, 0.2420],\n",
            "        [0.1016, 0.0000, 0.1235],\n",
            "        [0.0000, 0.0000, 0.0777]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4746,  0.6005],\n",
            "        [-0.4612,  0.5790],\n",
            "        [-0.4655,  0.5859],\n",
            "        [-0.4781,  0.6060],\n",
            "        [-0.4286,  0.5788],\n",
            "        [-0.4469,  0.5562]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3835, 0.6458],\n",
            "        [0.3867, 0.6408],\n",
            "        [0.3857, 0.6424],\n",
            "        [0.3827, 0.6470],\n",
            "        [0.3945, 0.6408],\n",
            "        [0.3901, 0.6356]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [4/10], Loss(item): 0.7266309261322021, Loss: 0.7266309261322021\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0088, -0.0070],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0603,  0.0199]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0088,  0.0000,  0.0322])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0051,  0.0000, -0.0097],\n",
            "        [ 0.0054,  0.0000,  0.0121]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0564,  0.0710])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4289,  0.3247],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0184, -0.1711]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5870, -0.5869,  0.2420])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2657,  0.3793, -0.1897],\n",
            "        [ 0.0863, -0.3822,  0.3029]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4322,  0.5327])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4299,  0.3257],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0174, -0.1721]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5860, -0.5869,  0.2410])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2667,  0.3793, -0.1887],\n",
            "        [ 0.0853, -0.3822,  0.3019]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4312,  0.5317])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 5:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4299,  0.3257],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0174, -0.1721]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5860, -0.5869,  0.2410])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2667,  0.3793, -0.1887],\n",
            "        [ 0.0853, -0.3822,  0.3019]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4312,  0.5317])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1448, -0.7534,  0.2218],\n",
            "        [-0.0682, -0.4745,  0.1507],\n",
            "        [-0.4020, -0.3897,  0.1743],\n",
            "        [-0.5860, -0.5869,  0.2410],\n",
            "        [ 0.1044, -0.4371,  0.1207],\n",
            "        [-0.0992, -0.1142,  0.0754]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2218],\n",
            "        [0.0000, 0.0000, 0.1507],\n",
            "        [0.0000, 0.0000, 0.1743],\n",
            "        [0.0000, 0.0000, 0.2410],\n",
            "        [0.1044, 0.0000, 0.1207],\n",
            "        [0.0000, 0.0000, 0.0754]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4730,  0.5986],\n",
            "        [-0.4596,  0.5772],\n",
            "        [-0.4640,  0.5843],\n",
            "        [-0.4766,  0.6044],\n",
            "        [-0.4261,  0.5770],\n",
            "        [-0.4454,  0.5544]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3839, 0.6453],\n",
            "        [0.3871, 0.6404],\n",
            "        [0.3860, 0.6421],\n",
            "        [0.3830, 0.6467],\n",
            "        [0.3951, 0.6404],\n",
            "        [0.3905, 0.6352]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [5/10], Loss(item): 0.726334810256958, Loss: 0.726334810256958\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0089, -0.0071],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0600,  0.0198]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0089,  0.0000,  0.0320])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0053,  0.0000, -0.0094],\n",
            "        [ 0.0056,  0.0000,  0.0118]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0562,  0.0708])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4299,  0.3257],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0174, -0.1721]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5860, -0.5869,  0.2410])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2667,  0.3793, -0.1887],\n",
            "        [ 0.0853, -0.3822,  0.3019]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4312,  0.5317])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4309,  0.3267],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0164, -0.1731]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5850, -0.5869,  0.2400])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2677,  0.3793, -0.1877],\n",
            "        [ 0.0843, -0.3822,  0.3009]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4302,  0.5307])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 6:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4309,  0.3267],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0164, -0.1731]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5850, -0.5869,  0.2400])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2677,  0.3793, -0.1877],\n",
            "        [ 0.0843, -0.3822,  0.3009]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4302,  0.5307])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1427, -0.7534,  0.2197],\n",
            "        [-0.0659, -0.4745,  0.1484],\n",
            "        [-0.4005, -0.3897,  0.1728],\n",
            "        [-0.5850, -0.5869,  0.2400],\n",
            "        [ 0.1072, -0.4371,  0.1179],\n",
            "        [-0.0968, -0.1142,  0.0730]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2197],\n",
            "        [0.0000, 0.0000, 0.1484],\n",
            "        [0.0000, 0.0000, 0.1728],\n",
            "        [0.0000, 0.0000, 0.2400],\n",
            "        [0.1072, 0.0000, 0.1179],\n",
            "        [0.0000, 0.0000, 0.0730]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4714,  0.5968],\n",
            "        [-0.4580,  0.5753],\n",
            "        [-0.4626,  0.5827],\n",
            "        [-0.4752,  0.6029],\n",
            "        [-0.4236,  0.5752],\n",
            "        [-0.4439,  0.5526]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3843, 0.6449],\n",
            "        [0.3875, 0.6400],\n",
            "        [0.3864, 0.6417],\n",
            "        [0.3834, 0.6463],\n",
            "        [0.3957, 0.6400],\n",
            "        [0.3908, 0.6347]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [6/10], Loss(item): 0.726039707660675, Loss: 0.726039707660675\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0090, -0.0072],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0597,  0.0197]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0090,  0.0000,  0.0318])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0054,  0.0000, -0.0092],\n",
            "        [ 0.0057,  0.0000,  0.0116]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0560,  0.0706])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4309,  0.3267],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0164, -0.1731]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5850, -0.5869,  0.2400])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2677,  0.3793, -0.1877],\n",
            "        [ 0.0843, -0.3822,  0.3009]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4302,  0.5307])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4319,  0.3277],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0154, -0.1741]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5840, -0.5869,  0.2390])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2687,  0.3793, -0.1867],\n",
            "        [ 0.0833, -0.3822,  0.2999]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4292,  0.5297])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 7:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4319,  0.3277],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0154, -0.1741]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5840, -0.5869,  0.2390])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2687,  0.3793, -0.1867],\n",
            "        [ 0.0833, -0.3822,  0.2999]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4292,  0.5297])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1406, -0.7534,  0.2176],\n",
            "        [-0.0635, -0.4745,  0.1461],\n",
            "        [-0.3990, -0.3897,  0.1712],\n",
            "        [-0.5840, -0.5869,  0.2390],\n",
            "        [ 0.1100, -0.4371,  0.1151],\n",
            "        [-0.0944, -0.1142,  0.0706]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2176],\n",
            "        [0.0000, 0.0000, 0.1461],\n",
            "        [0.0000, 0.0000, 0.1712],\n",
            "        [0.0000, 0.0000, 0.2390],\n",
            "        [0.1100, 0.0000, 0.1151],\n",
            "        [0.0000, 0.0000, 0.0706]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4698,  0.5949],\n",
            "        [-0.4564,  0.5735],\n",
            "        [-0.4611,  0.5810],\n",
            "        [-0.4738,  0.6013],\n",
            "        [-0.4211,  0.5733],\n",
            "        [-0.4423,  0.5508]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3847, 0.6445],\n",
            "        [0.3878, 0.6396],\n",
            "        [0.3867, 0.6413],\n",
            "        [0.3837, 0.6460],\n",
            "        [0.3963, 0.6395],\n",
            "        [0.3912, 0.6343]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [7/10], Loss(item): 0.7257458567619324, Loss: 0.7257458567619324\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0091, -0.0073],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0594,  0.0195]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0091,  0.0000,  0.0315])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0055,  0.0000, -0.0089],\n",
            "        [ 0.0059,  0.0000,  0.0113]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0558,  0.0704])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4319,  0.3277],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0154, -0.1741]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5840, -0.5869,  0.2390])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2687,  0.3793, -0.1867],\n",
            "        [ 0.0833, -0.3822,  0.2999]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4292,  0.5297])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4329,  0.3287],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0144, -0.1751]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5830, -0.5869,  0.2380])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2697,  0.3793, -0.1857],\n",
            "        [ 0.0823, -0.3822,  0.2990]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4282,  0.5287])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 8:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4329,  0.3287],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0144, -0.1751]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5830, -0.5869,  0.2380])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2697,  0.3793, -0.1857],\n",
            "        [ 0.0823, -0.3822,  0.2990]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4282,  0.5287])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1385, -0.7534,  0.2156],\n",
            "        [-0.0612, -0.4745,  0.1437],\n",
            "        [-0.3974, -0.3897,  0.1697],\n",
            "        [-0.5830, -0.5869,  0.2380],\n",
            "        [ 0.1128, -0.4371,  0.1123],\n",
            "        [-0.0920, -0.1142,  0.0683]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2156],\n",
            "        [0.0000, 0.0000, 0.1437],\n",
            "        [0.0000, 0.0000, 0.1697],\n",
            "        [0.0000, 0.0000, 0.2380],\n",
            "        [0.1128, 0.0000, 0.1123],\n",
            "        [0.0000, 0.0000, 0.0683]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4682,  0.5931],\n",
            "        [-0.4549,  0.5716],\n",
            "        [-0.4597,  0.5794],\n",
            "        [-0.4724,  0.5998],\n",
            "        [-0.4186,  0.5715],\n",
            "        [-0.4408,  0.5491]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3850, 0.6441],\n",
            "        [0.3882, 0.6391],\n",
            "        [0.3871, 0.6409],\n",
            "        [0.3841, 0.6456],\n",
            "        [0.3969, 0.6391],\n",
            "        [0.3915, 0.6339]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [8/10], Loss(item): 0.7254530787467957, Loss: 0.7254530787467957\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0092, -0.0073],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0591,  0.0194]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0092,  0.0000,  0.0313])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0057,  0.0000, -0.0087],\n",
            "        [ 0.0060,  0.0000,  0.0110]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0556,  0.0702])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4329,  0.3287],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0144, -0.1751]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5830, -0.5869,  0.2380])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2697,  0.3793, -0.1857],\n",
            "        [ 0.0823, -0.3822,  0.2990]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4282,  0.5287])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4339,  0.3297],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0134, -0.1761]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5820, -0.5869,  0.2370])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2708,  0.3793, -0.1847],\n",
            "        [ 0.0812, -0.3822,  0.2980]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4272,  0.5277])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 9:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4339,  0.3297],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0134, -0.1761]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5820, -0.5869,  0.2370])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2708,  0.3793, -0.1847],\n",
            "        [ 0.0812, -0.3822,  0.2980]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4272,  0.5277])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1364, -0.7534,  0.2135],\n",
            "        [-0.0588, -0.4745,  0.1414],\n",
            "        [-0.3959, -0.3897,  0.1682],\n",
            "        [-0.5820, -0.5869,  0.2370],\n",
            "        [ 0.1156, -0.4371,  0.1095],\n",
            "        [-0.0896, -0.1142,  0.0659]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2135],\n",
            "        [0.0000, 0.0000, 0.1414],\n",
            "        [0.0000, 0.0000, 0.1682],\n",
            "        [0.0000, 0.0000, 0.2370],\n",
            "        [0.1156, 0.0000, 0.1095],\n",
            "        [0.0000, 0.0000, 0.0659]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4666,  0.5913],\n",
            "        [-0.4533,  0.5698],\n",
            "        [-0.4582,  0.5778],\n",
            "        [-0.4709,  0.5983],\n",
            "        [-0.4161,  0.5697],\n",
            "        [-0.4393,  0.5473]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3854, 0.6437],\n",
            "        [0.3886, 0.6387],\n",
            "        [0.3874, 0.6406],\n",
            "        [0.3844, 0.6453],\n",
            "        [0.3975, 0.6387],\n",
            "        [0.3919, 0.6335]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [9/10], Loss(item): 0.7251613140106201, Loss: 0.7251613140106201\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0093, -0.0074],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0588,  0.0193]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0093,  0.0000,  0.0311])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0058,  0.0000, -0.0085],\n",
            "        [ 0.0062,  0.0000,  0.0107]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0554,  0.0700])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4339,  0.3297],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0134, -0.1761]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5820, -0.5869,  0.2370])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2708,  0.3793, -0.1847],\n",
            "        [ 0.0812, -0.3822,  0.2980]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4272,  0.5277])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4349,  0.3307],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0124, -0.1771]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5810, -0.5869,  0.2360])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2718,  0.3793, -0.1837],\n",
            "        [ 0.0802, -0.3822,  0.2970]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4262,  0.5267])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 10:\n",
            "\n",
            "Parameters of fc1.weight: \n",
            "tensor([[ 0.4349,  0.3307],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0124, -0.1771]])\n",
            "\n",
            "\n",
            "Parameters of fc1.bias: \n",
            "tensor([-0.5810, -0.5869,  0.2360])\n",
            "\n",
            "\n",
            "Parameters of fc2.weight: \n",
            "tensor([[ 0.2718,  0.3793, -0.1837],\n",
            "        [ 0.0802, -0.3822,  0.2970]])\n",
            "\n",
            "\n",
            "Parameters of fc2.bias: \n",
            "tensor([-0.4262,  0.5267])\n",
            "\n",
            "\n",
            "Activation after first hidden layer: \n",
            "tensor([[-0.1343, -0.7534,  0.2114],\n",
            "        [-0.0564, -0.4745,  0.1390],\n",
            "        [-0.3944, -0.3897,  0.1667],\n",
            "        [-0.5810, -0.5869,  0.2360],\n",
            "        [ 0.1184, -0.4371,  0.1067],\n",
            "        [-0.0872, -0.1142,  0.0635]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Non linearity after first Relu layer: \n",
            "tensor([[0.0000, 0.0000, 0.2114],\n",
            "        [0.0000, 0.0000, 0.1390],\n",
            "        [0.0000, 0.0000, 0.1667],\n",
            "        [0.0000, 0.0000, 0.2360],\n",
            "        [0.1184, 0.0000, 0.1067],\n",
            "        [0.0000, 0.0000, 0.0635]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "Activation after second output layer: \n",
            "tensor([[-0.4650,  0.5895],\n",
            "        [-0.4517,  0.5680],\n",
            "        [-0.4568,  0.5762],\n",
            "        [-0.4695,  0.5968],\n",
            "        [-0.4136,  0.5679],\n",
            "        [-0.4378,  0.5455]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Output after second output layer (sigmoid): \n",
            "tensor([[0.3858, 0.6432],\n",
            "        [0.3890, 0.6383],\n",
            "        [0.3877, 0.6402],\n",
            "        [0.3847, 0.6449],\n",
            "        [0.3981, 0.6383],\n",
            "        [0.3923, 0.6331]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "Epoch [10/10], Loss(item): 0.7248708605766296, Loss: 0.7248708605766296\n",
            "\n",
            "Gradient of fc1.weight after backward pass: \n",
            "tensor([[-0.0094, -0.0075],\n",
            "        [ 0.0000,  0.0000],\n",
            "        [ 0.0585,  0.0191]])\n",
            "\n",
            "Gradient of fc1.bias after backward pass: \n",
            "tensor([-0.0094,  0.0000,  0.0309])\n",
            "\n",
            "Gradient of fc2.weight after backward pass: \n",
            "tensor([[-0.0059,  0.0000, -0.0082],\n",
            "        [ 0.0063,  0.0000,  0.0105]])\n",
            "\n",
            "Gradient of fc2.bias after backward pass: \n",
            "tensor([-0.0552,  0.0698])\n",
            "\n",
            "Weights of fc1.weight before update: \n",
            "tensor([[ 0.4349,  0.3307],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0124, -0.1771]])\n",
            "\n",
            "Weights of fc1.bias before update: \n",
            "tensor([-0.5810, -0.5869,  0.2360])\n",
            "\n",
            "Weights of fc2.weight before update: \n",
            "tensor([[ 0.2718,  0.3793, -0.1837],\n",
            "        [ 0.0802, -0.3822,  0.2970]])\n",
            "\n",
            "Weights of fc2.bias before update: \n",
            "tensor([-0.4262,  0.5267])\n",
            "\n",
            "Weights of fc1.weight after update: \n",
            "tensor([[ 0.4359,  0.3317],\n",
            "        [-0.3262,  0.5951],\n",
            "        [ 0.0114, -0.1781]])\n",
            "\n",
            "Weights of fc1.bias after update: \n",
            "tensor([-0.5800, -0.5869,  0.2350])\n",
            "\n",
            "Weights of fc2.weight after update: \n",
            "tensor([[ 0.2728,  0.3793, -0.1828],\n",
            "        [ 0.0792, -0.3822,  0.2960]])\n",
            "\n",
            "Weights of fc2.bias after update: \n",
            "tensor([-0.4252,  0.5257])\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "print('X :\\n', X)\n",
        "print('y :\\n', y)\n",
        "\n",
        "no_of_epoch = 10\n",
        "\n",
        "for epoch in range(no_of_epoch):  # Just 3 epochs for illustration\n",
        "    print(f\"\\n\\n\\n\\nEpoch {epoch+1}:\")\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y)\n",
        "\n",
        "    # Print loss for tracking\n",
        "    print(f\"\\nEpoch [{epoch+1}/{no_of_epoch}], Loss(item): {loss.item()}, Loss: {loss}\")\n",
        "\n",
        "    # Zero gradients before the backward pass\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass (computing gradients)\n",
        "    loss.backward()\n",
        "\n",
        "    # Print gradients for each layer\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            print(f\"\\nGradient of {name} after backward pass: \\n{param.grad}\")\n",
        "\n",
        "    # Print weights before the update\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"\\nWeights of {name} before update: \\n{param.data}\")\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print weights after the update\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"\\nWeights of {name} after update: \\n{param.data}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mini Batch gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/6000], Train Loss: 0.6258, Test Loss: 0.5080\n",
            "Epoch [2/6000], Train Loss: 0.6253, Test Loss: 0.5067\n",
            "Epoch [3/6000], Train Loss: 0.6243, Test Loss: 0.5060\n",
            "Epoch [4/6000], Train Loss: 0.6238, Test Loss: 0.5052\n",
            "Epoch [5/6000], Train Loss: 0.6231, Test Loss: 0.5043\n",
            "Epoch [6/6000], Train Loss: 0.6225, Test Loss: 0.5035\n",
            "Epoch [7/6000], Train Loss: 0.6219, Test Loss: 0.5027\n",
            "Epoch [8/6000], Train Loss: 0.6213, Test Loss: 0.5020\n",
            "Epoch [9/6000], Train Loss: 0.6206, Test Loss: 0.5013\n",
            "Epoch [10/6000], Train Loss: 0.6200, Test Loss: 0.5006\n",
            "Epoch [11/6000], Train Loss: 0.6197, Test Loss: 0.4996\n",
            "Epoch [12/6000], Train Loss: 0.6190, Test Loss: 0.4986\n",
            "Epoch [13/6000], Train Loss: 0.6182, Test Loss: 0.4979\n",
            "Epoch [14/6000], Train Loss: 0.6177, Test Loss: 0.4972\n",
            "Epoch [15/6000], Train Loss: 0.6170, Test Loss: 0.4965\n",
            "Epoch [16/6000], Train Loss: 0.6165, Test Loss: 0.4958\n",
            "Epoch [17/6000], Train Loss: 0.6159, Test Loss: 0.4951\n",
            "Epoch [18/6000], Train Loss: 0.6155, Test Loss: 0.4944\n",
            "Epoch [19/6000], Train Loss: 0.6149, Test Loss: 0.4937\n",
            "Epoch [20/6000], Train Loss: 0.6145, Test Loss: 0.4928\n",
            "Epoch [21/6000], Train Loss: 0.6139, Test Loss: 0.4919\n",
            "Epoch [22/6000], Train Loss: 0.6132, Test Loss: 0.4913\n",
            "Epoch [23/6000], Train Loss: 0.6126, Test Loss: 0.4907\n",
            "Epoch [24/6000], Train Loss: 0.6121, Test Loss: 0.4901\n",
            "Epoch [25/6000], Train Loss: 0.6116, Test Loss: 0.4894\n",
            "Epoch [26/6000], Train Loss: 0.6112, Test Loss: 0.4885\n",
            "Epoch [27/6000], Train Loss: 0.6105, Test Loss: 0.4880\n",
            "Epoch [28/6000], Train Loss: 0.6100, Test Loss: 0.4875\n",
            "Epoch [29/6000], Train Loss: 0.6095, Test Loss: 0.4868\n",
            "Epoch [30/6000], Train Loss: 0.6089, Test Loss: 0.4863\n",
            "Epoch [31/6000], Train Loss: 0.6085, Test Loss: 0.4856\n",
            "Epoch [32/6000], Train Loss: 0.6079, Test Loss: 0.4849\n",
            "Epoch [33/6000], Train Loss: 0.6075, Test Loss: 0.4843\n",
            "Epoch [34/6000], Train Loss: 0.6070, Test Loss: 0.4836\n",
            "Epoch [35/6000], Train Loss: 0.6066, Test Loss: 0.4828\n",
            "Epoch [36/6000], Train Loss: 0.6060, Test Loss: 0.4821\n",
            "Epoch [37/6000], Train Loss: 0.6054, Test Loss: 0.4816\n",
            "Epoch [38/6000], Train Loss: 0.6050, Test Loss: 0.4809\n",
            "Epoch [39/6000], Train Loss: 0.6047, Test Loss: 0.4802\n",
            "Epoch [40/6000], Train Loss: 0.6040, Test Loss: 0.4796\n",
            "Epoch [41/6000], Train Loss: 0.6035, Test Loss: 0.4792\n",
            "Epoch [42/6000], Train Loss: 0.6032, Test Loss: 0.4784\n",
            "Epoch [43/6000], Train Loss: 0.6027, Test Loss: 0.4776\n",
            "Epoch [44/6000], Train Loss: 0.6021, Test Loss: 0.4771\n",
            "Epoch [45/6000], Train Loss: 0.6018, Test Loss: 0.4764\n",
            "Epoch [46/6000], Train Loss: 0.6012, Test Loss: 0.4758\n",
            "Epoch [47/6000], Train Loss: 0.6007, Test Loss: 0.4753\n",
            "Epoch [48/6000], Train Loss: 0.6003, Test Loss: 0.4748\n",
            "Epoch [49/6000], Train Loss: 0.5999, Test Loss: 0.4742\n",
            "Epoch [50/6000], Train Loss: 0.5994, Test Loss: 0.4736\n",
            "Epoch [51/6000], Train Loss: 0.5989, Test Loss: 0.4730\n",
            "Epoch [52/6000], Train Loss: 0.5985, Test Loss: 0.4724\n",
            "Epoch [53/6000], Train Loss: 0.5981, Test Loss: 0.4717\n",
            "Epoch [54/6000], Train Loss: 0.5975, Test Loss: 0.4712\n",
            "Epoch [55/6000], Train Loss: 0.5971, Test Loss: 0.4708\n",
            "Epoch [56/6000], Train Loss: 0.5966, Test Loss: 0.4702\n",
            "Epoch [57/6000], Train Loss: 0.5962, Test Loss: 0.4698\n",
            "Epoch [58/6000], Train Loss: 0.5957, Test Loss: 0.4692\n",
            "Epoch [59/6000], Train Loss: 0.5953, Test Loss: 0.4688\n",
            "Epoch [60/6000], Train Loss: 0.5949, Test Loss: 0.4683\n",
            "Epoch [61/6000], Train Loss: 0.5944, Test Loss: 0.4678\n",
            "Epoch [62/6000], Train Loss: 0.5940, Test Loss: 0.4672\n",
            "Epoch [63/6000], Train Loss: 0.5935, Test Loss: 0.4667\n",
            "Epoch [64/6000], Train Loss: 0.5931, Test Loss: 0.4662\n",
            "Epoch [65/6000], Train Loss: 0.5927, Test Loss: 0.4656\n",
            "Epoch [66/6000], Train Loss: 0.5922, Test Loss: 0.4650\n",
            "Epoch [67/6000], Train Loss: 0.5918, Test Loss: 0.4644\n",
            "Epoch [68/6000], Train Loss: 0.5916, Test Loss: 0.4636\n",
            "Epoch [69/6000], Train Loss: 0.5909, Test Loss: 0.4630\n",
            "Epoch [70/6000], Train Loss: 0.5906, Test Loss: 0.4623\n",
            "Epoch [71/6000], Train Loss: 0.5901, Test Loss: 0.4617\n",
            "Epoch [72/6000], Train Loss: 0.5896, Test Loss: 0.4611\n",
            "Epoch [73/6000], Train Loss: 0.5892, Test Loss: 0.4606\n",
            "Epoch [74/6000], Train Loss: 0.5888, Test Loss: 0.4600\n",
            "Epoch [75/6000], Train Loss: 0.5883, Test Loss: 0.4594\n",
            "Epoch [76/6000], Train Loss: 0.5880, Test Loss: 0.4588\n",
            "Epoch [77/6000], Train Loss: 0.5875, Test Loss: 0.4581\n",
            "Epoch [78/6000], Train Loss: 0.5870, Test Loss: 0.4577\n",
            "Epoch [79/6000], Train Loss: 0.5868, Test Loss: 0.4569\n",
            "Epoch [80/6000], Train Loss: 0.5863, Test Loss: 0.4562\n",
            "Epoch [81/6000], Train Loss: 0.5857, Test Loss: 0.4556\n",
            "Epoch [82/6000], Train Loss: 0.5853, Test Loss: 0.4550\n",
            "Epoch [83/6000], Train Loss: 0.5848, Test Loss: 0.4545\n",
            "Epoch [84/6000], Train Loss: 0.5846, Test Loss: 0.4538\n",
            "Epoch [85/6000], Train Loss: 0.5840, Test Loss: 0.4534\n",
            "Epoch [86/6000], Train Loss: 0.5835, Test Loss: 0.4528\n",
            "Epoch [87/6000], Train Loss: 0.5831, Test Loss: 0.4522\n",
            "Epoch [88/6000], Train Loss: 0.5826, Test Loss: 0.4516\n",
            "Epoch [89/6000], Train Loss: 0.5822, Test Loss: 0.4512\n",
            "Epoch [90/6000], Train Loss: 0.5820, Test Loss: 0.4504\n",
            "Epoch [91/6000], Train Loss: 0.5814, Test Loss: 0.4498\n",
            "Epoch [92/6000], Train Loss: 0.5809, Test Loss: 0.4492\n",
            "Epoch [93/6000], Train Loss: 0.5805, Test Loss: 0.4486\n",
            "Epoch [94/6000], Train Loss: 0.5800, Test Loss: 0.4482\n",
            "Epoch [95/6000], Train Loss: 0.5798, Test Loss: 0.4474\n",
            "Epoch [96/6000], Train Loss: 0.5792, Test Loss: 0.4467\n",
            "Epoch [97/6000], Train Loss: 0.5787, Test Loss: 0.4461\n",
            "Epoch [98/6000], Train Loss: 0.5783, Test Loss: 0.4455\n",
            "Epoch [99/6000], Train Loss: 0.5778, Test Loss: 0.4449\n",
            "Epoch [100/6000], Train Loss: 0.5773, Test Loss: 0.4443\n",
            "Epoch [101/6000], Train Loss: 0.5769, Test Loss: 0.4438\n",
            "Epoch [102/6000], Train Loss: 0.5765, Test Loss: 0.4432\n",
            "Epoch [103/6000], Train Loss: 0.5760, Test Loss: 0.4427\n",
            "Epoch [104/6000], Train Loss: 0.5756, Test Loss: 0.4421\n",
            "Epoch [105/6000], Train Loss: 0.5751, Test Loss: 0.4414\n",
            "Epoch [106/6000], Train Loss: 0.5746, Test Loss: 0.4409\n",
            "Epoch [107/6000], Train Loss: 0.5742, Test Loss: 0.4402\n",
            "Epoch [108/6000], Train Loss: 0.5737, Test Loss: 0.4397\n",
            "Epoch [109/6000], Train Loss: 0.5732, Test Loss: 0.4392\n",
            "Epoch [110/6000], Train Loss: 0.5728, Test Loss: 0.4384\n",
            "Epoch [111/6000], Train Loss: 0.5724, Test Loss: 0.4377\n",
            "Epoch [112/6000], Train Loss: 0.5719, Test Loss: 0.4370\n",
            "Epoch [113/6000], Train Loss: 0.5714, Test Loss: 0.4363\n",
            "Epoch [114/6000], Train Loss: 0.5710, Test Loss: 0.4356\n",
            "Epoch [115/6000], Train Loss: 0.5704, Test Loss: 0.4349\n",
            "Epoch [116/6000], Train Loss: 0.5700, Test Loss: 0.4344\n",
            "Epoch [117/6000], Train Loss: 0.5695, Test Loss: 0.4338\n",
            "Epoch [118/6000], Train Loss: 0.5691, Test Loss: 0.4330\n",
            "Epoch [119/6000], Train Loss: 0.5685, Test Loss: 0.4324\n",
            "Epoch [120/6000], Train Loss: 0.5681, Test Loss: 0.4316\n",
            "Epoch [121/6000], Train Loss: 0.5676, Test Loss: 0.4309\n",
            "Epoch [122/6000], Train Loss: 0.5671, Test Loss: 0.4302\n",
            "Epoch [123/6000], Train Loss: 0.5666, Test Loss: 0.4295\n",
            "Epoch [124/6000], Train Loss: 0.5661, Test Loss: 0.4290\n",
            "Epoch [125/6000], Train Loss: 0.5656, Test Loss: 0.4282\n",
            "Epoch [126/6000], Train Loss: 0.5651, Test Loss: 0.4275\n",
            "Epoch [127/6000], Train Loss: 0.5646, Test Loss: 0.4268\n",
            "Epoch [128/6000], Train Loss: 0.5641, Test Loss: 0.4261\n",
            "Epoch [129/6000], Train Loss: 0.5636, Test Loss: 0.4254\n",
            "Epoch [130/6000], Train Loss: 0.5631, Test Loss: 0.4247\n",
            "Epoch [131/6000], Train Loss: 0.5626, Test Loss: 0.4239\n",
            "Epoch [132/6000], Train Loss: 0.5621, Test Loss: 0.4231\n",
            "Epoch [133/6000], Train Loss: 0.5617, Test Loss: 0.4222\n",
            "Epoch [134/6000], Train Loss: 0.5612, Test Loss: 0.4214\n",
            "Epoch [135/6000], Train Loss: 0.5606, Test Loss: 0.4205\n",
            "Epoch [136/6000], Train Loss: 0.5599, Test Loss: 0.4201\n",
            "Epoch [137/6000], Train Loss: 0.5594, Test Loss: 0.4193\n",
            "Epoch [138/6000], Train Loss: 0.5589, Test Loss: 0.4186\n",
            "Epoch [139/6000], Train Loss: 0.5584, Test Loss: 0.4179\n",
            "Epoch [140/6000], Train Loss: 0.5580, Test Loss: 0.4170\n",
            "Epoch [141/6000], Train Loss: 0.5573, Test Loss: 0.4162\n",
            "Epoch [142/6000], Train Loss: 0.5568, Test Loss: 0.4156\n",
            "Epoch [143/6000], Train Loss: 0.5564, Test Loss: 0.4147\n",
            "Epoch [144/6000], Train Loss: 0.5557, Test Loss: 0.4139\n",
            "Epoch [145/6000], Train Loss: 0.5552, Test Loss: 0.4132\n",
            "Epoch [146/6000], Train Loss: 0.5548, Test Loss: 0.4123\n",
            "Epoch [147/6000], Train Loss: 0.5541, Test Loss: 0.4116\n",
            "Epoch [148/6000], Train Loss: 0.5537, Test Loss: 0.4107\n",
            "Epoch [149/6000], Train Loss: 0.5529, Test Loss: 0.4097\n",
            "Epoch [150/6000], Train Loss: 0.5521, Test Loss: 0.4087\n",
            "Epoch [151/6000], Train Loss: 0.5512, Test Loss: 0.4079\n",
            "Epoch [152/6000], Train Loss: 0.5504, Test Loss: 0.4072\n",
            "Epoch [153/6000], Train Loss: 0.5495, Test Loss: 0.4064\n",
            "Epoch [154/6000], Train Loss: 0.5487, Test Loss: 0.4054\n",
            "Epoch [155/6000], Train Loss: 0.5478, Test Loss: 0.4047\n",
            "Epoch [156/6000], Train Loss: 0.5470, Test Loss: 0.4037\n",
            "Epoch [157/6000], Train Loss: 0.5462, Test Loss: 0.4027\n",
            "Epoch [158/6000], Train Loss: 0.5453, Test Loss: 0.4019\n",
            "Epoch [159/6000], Train Loss: 0.5444, Test Loss: 0.4009\n",
            "Epoch [160/6000], Train Loss: 0.5436, Test Loss: 0.3999\n",
            "Epoch [161/6000], Train Loss: 0.5427, Test Loss: 0.3991\n",
            "Epoch [162/6000], Train Loss: 0.5420, Test Loss: 0.3980\n",
            "Epoch [163/6000], Train Loss: 0.5412, Test Loss: 0.3969\n",
            "Epoch [164/6000], Train Loss: 0.5404, Test Loss: 0.3957\n",
            "Epoch [165/6000], Train Loss: 0.5394, Test Loss: 0.3947\n",
            "Epoch [166/6000], Train Loss: 0.5386, Test Loss: 0.3940\n",
            "Epoch [167/6000], Train Loss: 0.5377, Test Loss: 0.3930\n",
            "Epoch [168/6000], Train Loss: 0.5369, Test Loss: 0.3922\n",
            "Epoch [169/6000], Train Loss: 0.5361, Test Loss: 0.3912\n",
            "Epoch [170/6000], Train Loss: 0.5354, Test Loss: 0.3901\n",
            "Epoch [171/6000], Train Loss: 0.5345, Test Loss: 0.3891\n",
            "Epoch [172/6000], Train Loss: 0.5338, Test Loss: 0.3879\n",
            "Epoch [173/6000], Train Loss: 0.5330, Test Loss: 0.3868\n",
            "Epoch [174/6000], Train Loss: 0.5321, Test Loss: 0.3860\n",
            "Epoch [175/6000], Train Loss: 0.5312, Test Loss: 0.3851\n",
            "Epoch [176/6000], Train Loss: 0.5306, Test Loss: 0.3839\n",
            "Epoch [177/6000], Train Loss: 0.5297, Test Loss: 0.3830\n",
            "Epoch [178/6000], Train Loss: 0.5288, Test Loss: 0.3820\n",
            "Epoch [179/6000], Train Loss: 0.5281, Test Loss: 0.3813\n",
            "Epoch [180/6000], Train Loss: 0.5273, Test Loss: 0.3802\n",
            "Epoch [181/6000], Train Loss: 0.5265, Test Loss: 0.3794\n",
            "Epoch [182/6000], Train Loss: 0.5259, Test Loss: 0.3782\n",
            "Epoch [183/6000], Train Loss: 0.5250, Test Loss: 0.3772\n",
            "Epoch [184/6000], Train Loss: 0.5242, Test Loss: 0.3762\n",
            "Epoch [185/6000], Train Loss: 0.5234, Test Loss: 0.3754\n",
            "Epoch [186/6000], Train Loss: 0.5226, Test Loss: 0.3744\n",
            "Epoch [187/6000], Train Loss: 0.5218, Test Loss: 0.3735\n",
            "Epoch [188/6000], Train Loss: 0.5211, Test Loss: 0.3726\n",
            "Epoch [189/6000], Train Loss: 0.5203, Test Loss: 0.3716\n",
            "Epoch [190/6000], Train Loss: 0.5197, Test Loss: 0.3704\n",
            "Epoch [191/6000], Train Loss: 0.5187, Test Loss: 0.3694\n",
            "Epoch [192/6000], Train Loss: 0.5180, Test Loss: 0.3684\n",
            "Epoch [193/6000], Train Loss: 0.5172, Test Loss: 0.3674\n",
            "Epoch [194/6000], Train Loss: 0.5164, Test Loss: 0.3664\n",
            "Epoch [195/6000], Train Loss: 0.5157, Test Loss: 0.3654\n",
            "Epoch [196/6000], Train Loss: 0.5149, Test Loss: 0.3644\n",
            "Epoch [197/6000], Train Loss: 0.5141, Test Loss: 0.3636\n",
            "Epoch [198/6000], Train Loss: 0.5134, Test Loss: 0.3626\n",
            "Epoch [199/6000], Train Loss: 0.5126, Test Loss: 0.3616\n",
            "Epoch [200/6000], Train Loss: 0.5118, Test Loss: 0.3608\n",
            "Epoch [201/6000], Train Loss: 0.5111, Test Loss: 0.3597\n",
            "Epoch [202/6000], Train Loss: 0.5105, Test Loss: 0.3586\n",
            "Epoch [203/6000], Train Loss: 0.5096, Test Loss: 0.3576\n",
            "Epoch [204/6000], Train Loss: 0.5088, Test Loss: 0.3566\n",
            "Epoch [205/6000], Train Loss: 0.5082, Test Loss: 0.3554\n",
            "Epoch [206/6000], Train Loss: 0.5073, Test Loss: 0.3545\n",
            "Epoch [207/6000], Train Loss: 0.5065, Test Loss: 0.3536\n",
            "Epoch [208/6000], Train Loss: 0.5057, Test Loss: 0.3527\n",
            "Epoch [209/6000], Train Loss: 0.5050, Test Loss: 0.3517\n",
            "Epoch [210/6000], Train Loss: 0.5042, Test Loss: 0.3508\n",
            "Epoch [211/6000], Train Loss: 0.5035, Test Loss: 0.3500\n",
            "Epoch [212/6000], Train Loss: 0.5028, Test Loss: 0.3490\n",
            "Epoch [213/6000], Train Loss: 0.5020, Test Loss: 0.3480\n",
            "Epoch [214/6000], Train Loss: 0.5012, Test Loss: 0.3470\n",
            "Epoch [215/6000], Train Loss: 0.5007, Test Loss: 0.3458\n",
            "Epoch [216/6000], Train Loss: 0.4998, Test Loss: 0.3449\n",
            "Epoch [217/6000], Train Loss: 0.4991, Test Loss: 0.3438\n",
            "Epoch [218/6000], Train Loss: 0.4982, Test Loss: 0.3429\n",
            "Epoch [219/6000], Train Loss: 0.4975, Test Loss: 0.3420\n",
            "Epoch [220/6000], Train Loss: 0.4968, Test Loss: 0.3411\n",
            "Epoch [221/6000], Train Loss: 0.4961, Test Loss: 0.3403\n",
            "Epoch [222/6000], Train Loss: 0.4953, Test Loss: 0.3394\n",
            "Epoch [223/6000], Train Loss: 0.4946, Test Loss: 0.3384\n",
            "Epoch [224/6000], Train Loss: 0.4940, Test Loss: 0.3373\n",
            "Epoch [225/6000], Train Loss: 0.4931, Test Loss: 0.3364\n",
            "Epoch [226/6000], Train Loss: 0.4925, Test Loss: 0.3353\n",
            "Epoch [227/6000], Train Loss: 0.4916, Test Loss: 0.3344\n",
            "Epoch [228/6000], Train Loss: 0.4909, Test Loss: 0.3335\n",
            "Epoch [229/6000], Train Loss: 0.4901, Test Loss: 0.3326\n",
            "Epoch [230/6000], Train Loss: 0.4894, Test Loss: 0.3317\n",
            "Epoch [231/6000], Train Loss: 0.4888, Test Loss: 0.3308\n",
            "Epoch [232/6000], Train Loss: 0.4881, Test Loss: 0.3297\n",
            "Epoch [233/6000], Train Loss: 0.4873, Test Loss: 0.3289\n",
            "Epoch [234/6000], Train Loss: 0.4866, Test Loss: 0.3278\n",
            "Epoch [235/6000], Train Loss: 0.4858, Test Loss: 0.3270\n",
            "Epoch [236/6000], Train Loss: 0.4852, Test Loss: 0.3260\n",
            "Epoch [237/6000], Train Loss: 0.4844, Test Loss: 0.3253\n",
            "Epoch [238/6000], Train Loss: 0.4836, Test Loss: 0.3244\n",
            "Epoch [239/6000], Train Loss: 0.4829, Test Loss: 0.3235\n",
            "Epoch [240/6000], Train Loss: 0.4822, Test Loss: 0.3227\n",
            "Epoch [241/6000], Train Loss: 0.4815, Test Loss: 0.3218\n",
            "Epoch [242/6000], Train Loss: 0.4809, Test Loss: 0.3207\n",
            "Epoch [243/6000], Train Loss: 0.4802, Test Loss: 0.3197\n",
            "Epoch [244/6000], Train Loss: 0.4794, Test Loss: 0.3191\n",
            "Epoch [245/6000], Train Loss: 0.4786, Test Loss: 0.3182\n",
            "Epoch [246/6000], Train Loss: 0.4779, Test Loss: 0.3173\n",
            "Epoch [247/6000], Train Loss: 0.4773, Test Loss: 0.3165\n",
            "Epoch [248/6000], Train Loss: 0.4766, Test Loss: 0.3156\n",
            "Epoch [249/6000], Train Loss: 0.4760, Test Loss: 0.3150\n",
            "Epoch [250/6000], Train Loss: 0.4755, Test Loss: 0.3143\n",
            "Epoch [251/6000], Train Loss: 0.4749, Test Loss: 0.3136\n",
            "Epoch [252/6000], Train Loss: 0.4743, Test Loss: 0.3130\n",
            "Epoch [253/6000], Train Loss: 0.4737, Test Loss: 0.3122\n",
            "Epoch [254/6000], Train Loss: 0.4731, Test Loss: 0.3113\n",
            "Epoch [255/6000], Train Loss: 0.4724, Test Loss: 0.3107\n",
            "Epoch [256/6000], Train Loss: 0.4718, Test Loss: 0.3099\n",
            "Epoch [257/6000], Train Loss: 0.4711, Test Loss: 0.3093\n",
            "Epoch [258/6000], Train Loss: 0.4705, Test Loss: 0.3087\n",
            "Epoch [259/6000], Train Loss: 0.4700, Test Loss: 0.3080\n",
            "Epoch [260/6000], Train Loss: 0.4693, Test Loss: 0.3075\n",
            "Epoch [261/6000], Train Loss: 0.4687, Test Loss: 0.3069\n",
            "Epoch [262/6000], Train Loss: 0.4680, Test Loss: 0.3061\n",
            "Epoch [263/6000], Train Loss: 0.4675, Test Loss: 0.3055\n",
            "Epoch [264/6000], Train Loss: 0.4668, Test Loss: 0.3047\n",
            "Epoch [265/6000], Train Loss: 0.4663, Test Loss: 0.3039\n",
            "Epoch [266/6000], Train Loss: 0.4657, Test Loss: 0.3033\n",
            "Epoch [267/6000], Train Loss: 0.4651, Test Loss: 0.3026\n",
            "Epoch [268/6000], Train Loss: 0.4644, Test Loss: 0.3020\n",
            "Epoch [269/6000], Train Loss: 0.4638, Test Loss: 0.3012\n",
            "Epoch [270/6000], Train Loss: 0.4632, Test Loss: 0.3005\n",
            "Epoch [271/6000], Train Loss: 0.4628, Test Loss: 0.2996\n",
            "Epoch [272/6000], Train Loss: 0.4621, Test Loss: 0.2992\n",
            "Epoch [273/6000], Train Loss: 0.4614, Test Loss: 0.2985\n",
            "Epoch [274/6000], Train Loss: 0.4608, Test Loss: 0.2978\n",
            "Epoch [275/6000], Train Loss: 0.4603, Test Loss: 0.2971\n",
            "Epoch [276/6000], Train Loss: 0.4598, Test Loss: 0.2963\n",
            "Epoch [277/6000], Train Loss: 0.4590, Test Loss: 0.2956\n",
            "Epoch [278/6000], Train Loss: 0.4585, Test Loss: 0.2950\n",
            "Epoch [279/6000], Train Loss: 0.4578, Test Loss: 0.2944\n",
            "Epoch [280/6000], Train Loss: 0.4572, Test Loss: 0.2937\n",
            "Epoch [281/6000], Train Loss: 0.4568, Test Loss: 0.2931\n",
            "Epoch [282/6000], Train Loss: 0.4562, Test Loss: 0.2925\n",
            "Epoch [283/6000], Train Loss: 0.4556, Test Loss: 0.2920\n",
            "Epoch [284/6000], Train Loss: 0.4550, Test Loss: 0.2915\n",
            "Epoch [285/6000], Train Loss: 0.4543, Test Loss: 0.2908\n",
            "Epoch [286/6000], Train Loss: 0.4539, Test Loss: 0.2900\n",
            "Epoch [287/6000], Train Loss: 0.4532, Test Loss: 0.2895\n",
            "Epoch [288/6000], Train Loss: 0.4526, Test Loss: 0.2889\n",
            "Epoch [289/6000], Train Loss: 0.4521, Test Loss: 0.2881\n",
            "Epoch [290/6000], Train Loss: 0.4514, Test Loss: 0.2874\n",
            "Epoch [291/6000], Train Loss: 0.4508, Test Loss: 0.2868\n",
            "Epoch [292/6000], Train Loss: 0.4503, Test Loss: 0.2862\n",
            "Epoch [293/6000], Train Loss: 0.4497, Test Loss: 0.2856\n",
            "Epoch [294/6000], Train Loss: 0.4491, Test Loss: 0.2851\n",
            "Epoch [295/6000], Train Loss: 0.4486, Test Loss: 0.2843\n",
            "Epoch [296/6000], Train Loss: 0.4479, Test Loss: 0.2838\n",
            "Epoch [297/6000], Train Loss: 0.4474, Test Loss: 0.2830\n",
            "Epoch [298/6000], Train Loss: 0.4467, Test Loss: 0.2825\n",
            "Epoch [299/6000], Train Loss: 0.4463, Test Loss: 0.2819\n",
            "Epoch [300/6000], Train Loss: 0.4457, Test Loss: 0.2813\n",
            "Epoch [301/6000], Train Loss: 0.4451, Test Loss: 0.2808\n",
            "Epoch [302/6000], Train Loss: 0.4445, Test Loss: 0.2802\n",
            "Epoch [303/6000], Train Loss: 0.4439, Test Loss: 0.2796\n",
            "Epoch [304/6000], Train Loss: 0.4434, Test Loss: 0.2789\n",
            "Epoch [305/6000], Train Loss: 0.4428, Test Loss: 0.2782\n",
            "Epoch [306/6000], Train Loss: 0.4421, Test Loss: 0.2777\n",
            "Epoch [307/6000], Train Loss: 0.4416, Test Loss: 0.2772\n",
            "Epoch [308/6000], Train Loss: 0.4411, Test Loss: 0.2766\n",
            "Epoch [309/6000], Train Loss: 0.4405, Test Loss: 0.2760\n",
            "Epoch [310/6000], Train Loss: 0.4399, Test Loss: 0.2755\n",
            "Epoch [311/6000], Train Loss: 0.4394, Test Loss: 0.2748\n",
            "Epoch [312/6000], Train Loss: 0.4388, Test Loss: 0.2743\n",
            "Epoch [313/6000], Train Loss: 0.4382, Test Loss: 0.2738\n",
            "Epoch [314/6000], Train Loss: 0.4376, Test Loss: 0.2732\n",
            "Epoch [315/6000], Train Loss: 0.4371, Test Loss: 0.2727\n",
            "Epoch [316/6000], Train Loss: 0.4366, Test Loss: 0.2722\n",
            "Epoch [317/6000], Train Loss: 0.4360, Test Loss: 0.2717\n",
            "Epoch [318/6000], Train Loss: 0.4355, Test Loss: 0.2711\n",
            "Epoch [319/6000], Train Loss: 0.4349, Test Loss: 0.2704\n",
            "Epoch [320/6000], Train Loss: 0.4342, Test Loss: 0.2699\n",
            "Epoch [321/6000], Train Loss: 0.4337, Test Loss: 0.2693\n",
            "Epoch [322/6000], Train Loss: 0.4331, Test Loss: 0.2688\n",
            "Epoch [323/6000], Train Loss: 0.4327, Test Loss: 0.2684\n",
            "Epoch [324/6000], Train Loss: 0.4320, Test Loss: 0.2679\n",
            "Epoch [325/6000], Train Loss: 0.4315, Test Loss: 0.2674\n",
            "Epoch [326/6000], Train Loss: 0.4310, Test Loss: 0.2669\n",
            "Epoch [327/6000], Train Loss: 0.4303, Test Loss: 0.2664\n",
            "Epoch [328/6000], Train Loss: 0.4299, Test Loss: 0.2660\n",
            "Epoch [329/6000], Train Loss: 0.4293, Test Loss: 0.2656\n",
            "Epoch [330/6000], Train Loss: 0.4287, Test Loss: 0.2650\n",
            "Epoch [331/6000], Train Loss: 0.4281, Test Loss: 0.2644\n",
            "Epoch [332/6000], Train Loss: 0.4276, Test Loss: 0.2637\n",
            "Epoch [333/6000], Train Loss: 0.4269, Test Loss: 0.2631\n",
            "Epoch [334/6000], Train Loss: 0.4265, Test Loss: 0.2625\n",
            "Epoch [335/6000], Train Loss: 0.4260, Test Loss: 0.2622\n",
            "Epoch [336/6000], Train Loss: 0.4253, Test Loss: 0.2617\n",
            "Epoch [337/6000], Train Loss: 0.4247, Test Loss: 0.2611\n",
            "Epoch [338/6000], Train Loss: 0.4243, Test Loss: 0.2607\n",
            "Epoch [339/6000], Train Loss: 0.4236, Test Loss: 0.2602\n",
            "Epoch [340/6000], Train Loss: 0.4232, Test Loss: 0.2596\n",
            "Epoch [341/6000], Train Loss: 0.4225, Test Loss: 0.2590\n",
            "Epoch [342/6000], Train Loss: 0.4220, Test Loss: 0.2585\n",
            "Epoch [343/6000], Train Loss: 0.4215, Test Loss: 0.2581\n",
            "Epoch [344/6000], Train Loss: 0.4210, Test Loss: 0.2575\n",
            "Epoch [345/6000], Train Loss: 0.4204, Test Loss: 0.2570\n",
            "Epoch [346/6000], Train Loss: 0.4198, Test Loss: 0.2565\n",
            "Epoch [347/6000], Train Loss: 0.4193, Test Loss: 0.2559\n",
            "Epoch [348/6000], Train Loss: 0.4188, Test Loss: 0.2554\n",
            "Epoch [349/6000], Train Loss: 0.4182, Test Loss: 0.2549\n",
            "Epoch [350/6000], Train Loss: 0.4176, Test Loss: 0.2544\n",
            "Epoch [351/6000], Train Loss: 0.4172, Test Loss: 0.2541\n",
            "Epoch [352/6000], Train Loss: 0.4166, Test Loss: 0.2536\n",
            "Epoch [353/6000], Train Loss: 0.4160, Test Loss: 0.2531\n",
            "Epoch [354/6000], Train Loss: 0.4155, Test Loss: 0.2526\n",
            "Epoch [355/6000], Train Loss: 0.4149, Test Loss: 0.2522\n",
            "Epoch [356/6000], Train Loss: 0.4144, Test Loss: 0.2517\n",
            "Epoch [357/6000], Train Loss: 0.4139, Test Loss: 0.2513\n",
            "Epoch [358/6000], Train Loss: 0.4133, Test Loss: 0.2508\n",
            "Epoch [359/6000], Train Loss: 0.4128, Test Loss: 0.2502\n",
            "Epoch [360/6000], Train Loss: 0.4122, Test Loss: 0.2497\n",
            "Epoch [361/6000], Train Loss: 0.4116, Test Loss: 0.2492\n",
            "Epoch [362/6000], Train Loss: 0.4112, Test Loss: 0.2489\n",
            "Epoch [363/6000], Train Loss: 0.4105, Test Loss: 0.2485\n",
            "Epoch [364/6000], Train Loss: 0.4101, Test Loss: 0.2478\n",
            "Epoch [365/6000], Train Loss: 0.4096, Test Loss: 0.2475\n",
            "Epoch [366/6000], Train Loss: 0.4091, Test Loss: 0.2472\n",
            "Epoch [367/6000], Train Loss: 0.4084, Test Loss: 0.2467\n",
            "Epoch [368/6000], Train Loss: 0.4079, Test Loss: 0.2463\n",
            "Epoch [369/6000], Train Loss: 0.4075, Test Loss: 0.2459\n",
            "Epoch [370/6000], Train Loss: 0.4068, Test Loss: 0.2454\n",
            "Epoch [371/6000], Train Loss: 0.4064, Test Loss: 0.2450\n",
            "Epoch [372/6000], Train Loss: 0.4059, Test Loss: 0.2445\n",
            "Epoch [373/6000], Train Loss: 0.4053, Test Loss: 0.2440\n",
            "Epoch [374/6000], Train Loss: 0.4047, Test Loss: 0.2435\n",
            "Epoch [375/6000], Train Loss: 0.4042, Test Loss: 0.2431\n",
            "Epoch [376/6000], Train Loss: 0.4037, Test Loss: 0.2426\n",
            "Epoch [377/6000], Train Loss: 0.4033, Test Loss: 0.2423\n",
            "Epoch [378/6000], Train Loss: 0.4027, Test Loss: 0.2418\n",
            "Epoch [379/6000], Train Loss: 0.4022, Test Loss: 0.2415\n",
            "Epoch [380/6000], Train Loss: 0.4016, Test Loss: 0.2410\n",
            "Epoch [381/6000], Train Loss: 0.4011, Test Loss: 0.2405\n",
            "Epoch [382/6000], Train Loss: 0.4006, Test Loss: 0.2400\n",
            "Epoch [383/6000], Train Loss: 0.4000, Test Loss: 0.2395\n",
            "Epoch [384/6000], Train Loss: 0.3994, Test Loss: 0.2391\n",
            "Epoch [385/6000], Train Loss: 0.3989, Test Loss: 0.2387\n",
            "Epoch [386/6000], Train Loss: 0.3984, Test Loss: 0.2383\n",
            "Epoch [387/6000], Train Loss: 0.3979, Test Loss: 0.2379\n",
            "Epoch [388/6000], Train Loss: 0.3975, Test Loss: 0.2373\n",
            "Epoch [389/6000], Train Loss: 0.3969, Test Loss: 0.2370\n",
            "Epoch [390/6000], Train Loss: 0.3965, Test Loss: 0.2367\n",
            "Epoch [391/6000], Train Loss: 0.3959, Test Loss: 0.2363\n",
            "Epoch [392/6000], Train Loss: 0.3955, Test Loss: 0.2359\n",
            "Epoch [393/6000], Train Loss: 0.3948, Test Loss: 0.2355\n",
            "Epoch [394/6000], Train Loss: 0.3944, Test Loss: 0.2352\n",
            "Epoch [395/6000], Train Loss: 0.3938, Test Loss: 0.2347\n",
            "Epoch [396/6000], Train Loss: 0.3934, Test Loss: 0.2344\n",
            "Epoch [397/6000], Train Loss: 0.3928, Test Loss: 0.2340\n",
            "Epoch [398/6000], Train Loss: 0.3923, Test Loss: 0.2336\n",
            "Epoch [399/6000], Train Loss: 0.3918, Test Loss: 0.2332\n",
            "Epoch [400/6000], Train Loss: 0.3913, Test Loss: 0.2327\n",
            "Epoch [401/6000], Train Loss: 0.3908, Test Loss: 0.2322\n",
            "Epoch [402/6000], Train Loss: 0.3902, Test Loss: 0.2316\n",
            "Epoch [403/6000], Train Loss: 0.3898, Test Loss: 0.2311\n",
            "Epoch [404/6000], Train Loss: 0.3892, Test Loss: 0.2308\n",
            "Epoch [405/6000], Train Loss: 0.3887, Test Loss: 0.2305\n",
            "Epoch [406/6000], Train Loss: 0.3882, Test Loss: 0.2302\n",
            "Epoch [407/6000], Train Loss: 0.3877, Test Loss: 0.2299\n",
            "Epoch [408/6000], Train Loss: 0.3873, Test Loss: 0.2295\n",
            "Epoch [409/6000], Train Loss: 0.3868, Test Loss: 0.2292\n",
            "Epoch [410/6000], Train Loss: 0.3862, Test Loss: 0.2288\n",
            "Epoch [411/6000], Train Loss: 0.3858, Test Loss: 0.2285\n",
            "Epoch [412/6000], Train Loss: 0.3853, Test Loss: 0.2283\n",
            "Epoch [413/6000], Train Loss: 0.3847, Test Loss: 0.2277\n",
            "Epoch [414/6000], Train Loss: 0.3843, Test Loss: 0.2275\n",
            "Epoch [415/6000], Train Loss: 0.3837, Test Loss: 0.2271\n",
            "Epoch [416/6000], Train Loss: 0.3831, Test Loss: 0.2267\n",
            "Epoch [417/6000], Train Loss: 0.3826, Test Loss: 0.2263\n",
            "Epoch [418/6000], Train Loss: 0.3822, Test Loss: 0.2259\n",
            "Epoch [419/6000], Train Loss: 0.3818, Test Loss: 0.2257\n",
            "Epoch [420/6000], Train Loss: 0.3811, Test Loss: 0.2253\n",
            "Epoch [421/6000], Train Loss: 0.3808, Test Loss: 0.2251\n",
            "Epoch [422/6000], Train Loss: 0.3802, Test Loss: 0.2245\n",
            "Epoch [423/6000], Train Loss: 0.3797, Test Loss: 0.2241\n",
            "Epoch [424/6000], Train Loss: 0.3792, Test Loss: 0.2237\n",
            "Epoch [425/6000], Train Loss: 0.3787, Test Loss: 0.2234\n",
            "Epoch [426/6000], Train Loss: 0.3783, Test Loss: 0.2228\n",
            "Epoch [427/6000], Train Loss: 0.3777, Test Loss: 0.2225\n",
            "Epoch [428/6000], Train Loss: 0.3773, Test Loss: 0.2222\n",
            "Epoch [429/6000], Train Loss: 0.3768, Test Loss: 0.2218\n",
            "Epoch [430/6000], Train Loss: 0.3763, Test Loss: 0.2215\n",
            "Epoch [431/6000], Train Loss: 0.3757, Test Loss: 0.2211\n",
            "Epoch [432/6000], Train Loss: 0.3753, Test Loss: 0.2208\n",
            "Epoch [433/6000], Train Loss: 0.3748, Test Loss: 0.2203\n",
            "Epoch [434/6000], Train Loss: 0.3743, Test Loss: 0.2200\n",
            "Epoch [435/6000], Train Loss: 0.3738, Test Loss: 0.2196\n",
            "Epoch [436/6000], Train Loss: 0.3734, Test Loss: 0.2194\n",
            "Epoch [437/6000], Train Loss: 0.3729, Test Loss: 0.2191\n",
            "Epoch [438/6000], Train Loss: 0.3724, Test Loss: 0.2188\n",
            "Epoch [439/6000], Train Loss: 0.3719, Test Loss: 0.2184\n",
            "Epoch [440/6000], Train Loss: 0.3714, Test Loss: 0.2181\n",
            "Epoch [441/6000], Train Loss: 0.3709, Test Loss: 0.2179\n",
            "Epoch [442/6000], Train Loss: 0.3704, Test Loss: 0.2174\n",
            "Epoch [443/6000], Train Loss: 0.3700, Test Loss: 0.2171\n",
            "Epoch [444/6000], Train Loss: 0.3695, Test Loss: 0.2168\n",
            "Epoch [445/6000], Train Loss: 0.3690, Test Loss: 0.2167\n",
            "Epoch [446/6000], Train Loss: 0.3684, Test Loss: 0.2163\n",
            "Epoch [447/6000], Train Loss: 0.3680, Test Loss: 0.2160\n",
            "Epoch [448/6000], Train Loss: 0.3675, Test Loss: 0.2157\n",
            "Epoch [449/6000], Train Loss: 0.3671, Test Loss: 0.2154\n",
            "Epoch [450/6000], Train Loss: 0.3666, Test Loss: 0.2152\n",
            "Epoch [451/6000], Train Loss: 0.3660, Test Loss: 0.2148\n",
            "Epoch [452/6000], Train Loss: 0.3656, Test Loss: 0.2143\n",
            "Epoch [453/6000], Train Loss: 0.3651, Test Loss: 0.2139\n",
            "Epoch [454/6000], Train Loss: 0.3646, Test Loss: 0.2136\n",
            "Epoch [455/6000], Train Loss: 0.3642, Test Loss: 0.2133\n",
            "Epoch [456/6000], Train Loss: 0.3638, Test Loss: 0.2130\n",
            "Epoch [457/6000], Train Loss: 0.3632, Test Loss: 0.2127\n",
            "Epoch [458/6000], Train Loss: 0.3628, Test Loss: 0.2122\n",
            "Epoch [459/6000], Train Loss: 0.3624, Test Loss: 0.2120\n",
            "Epoch [460/6000], Train Loss: 0.3619, Test Loss: 0.2117\n",
            "Epoch [461/6000], Train Loss: 0.3614, Test Loss: 0.2114\n",
            "Epoch [462/6000], Train Loss: 0.3609, Test Loss: 0.2111\n",
            "Epoch [463/6000], Train Loss: 0.3604, Test Loss: 0.2107\n",
            "Epoch [464/6000], Train Loss: 0.3599, Test Loss: 0.2103\n",
            "Epoch [465/6000], Train Loss: 0.3595, Test Loss: 0.2101\n",
            "Epoch [466/6000], Train Loss: 0.3591, Test Loss: 0.2098\n",
            "Epoch [467/6000], Train Loss: 0.3588, Test Loss: 0.2093\n",
            "Epoch [468/6000], Train Loss: 0.3584, Test Loss: 0.2090\n",
            "Epoch [469/6000], Train Loss: 0.3581, Test Loss: 0.2087\n",
            "Epoch [470/6000], Train Loss: 0.3577, Test Loss: 0.2084\n",
            "Epoch [471/6000], Train Loss: 0.3573, Test Loss: 0.2080\n",
            "Epoch [472/6000], Train Loss: 0.3570, Test Loss: 0.2076\n",
            "Epoch [473/6000], Train Loss: 0.3567, Test Loss: 0.2071\n",
            "Epoch [474/6000], Train Loss: 0.3563, Test Loss: 0.2067\n",
            "Epoch [475/6000], Train Loss: 0.3559, Test Loss: 0.2062\n",
            "Epoch [476/6000], Train Loss: 0.3555, Test Loss: 0.2058\n",
            "Epoch [477/6000], Train Loss: 0.3552, Test Loss: 0.2054\n",
            "Epoch [478/6000], Train Loss: 0.3549, Test Loss: 0.2049\n",
            "Epoch [479/6000], Train Loss: 0.3546, Test Loss: 0.2044\n",
            "Epoch [480/6000], Train Loss: 0.3542, Test Loss: 0.2040\n",
            "Epoch [481/6000], Train Loss: 0.3539, Test Loss: 0.2036\n",
            "Epoch [482/6000], Train Loss: 0.3535, Test Loss: 0.2033\n",
            "Epoch [483/6000], Train Loss: 0.3532, Test Loss: 0.2027\n",
            "Epoch [484/6000], Train Loss: 0.3528, Test Loss: 0.2024\n",
            "Epoch [485/6000], Train Loss: 0.3525, Test Loss: 0.2020\n",
            "Epoch [486/6000], Train Loss: 0.3521, Test Loss: 0.2016\n",
            "Epoch [487/6000], Train Loss: 0.3518, Test Loss: 0.2012\n",
            "Epoch [488/6000], Train Loss: 0.3514, Test Loss: 0.2009\n",
            "Epoch [489/6000], Train Loss: 0.3511, Test Loss: 0.2005\n",
            "Epoch [490/6000], Train Loss: 0.3508, Test Loss: 0.2000\n",
            "Epoch [491/6000], Train Loss: 0.3504, Test Loss: 0.1996\n",
            "Epoch [492/6000], Train Loss: 0.3502, Test Loss: 0.1990\n",
            "Epoch [493/6000], Train Loss: 0.3498, Test Loss: 0.1987\n",
            "Epoch [494/6000], Train Loss: 0.3495, Test Loss: 0.1983\n",
            "Epoch [495/6000], Train Loss: 0.3491, Test Loss: 0.1979\n",
            "Epoch [496/6000], Train Loss: 0.3488, Test Loss: 0.1976\n",
            "Epoch [497/6000], Train Loss: 0.3485, Test Loss: 0.1972\n",
            "Epoch [498/6000], Train Loss: 0.3481, Test Loss: 0.1969\n",
            "Epoch [499/6000], Train Loss: 0.3478, Test Loss: 0.1966\n",
            "Epoch [500/6000], Train Loss: 0.3475, Test Loss: 0.1963\n",
            "Epoch [501/6000], Train Loss: 0.3472, Test Loss: 0.1959\n",
            "Epoch [502/6000], Train Loss: 0.3469, Test Loss: 0.1955\n",
            "Epoch [503/6000], Train Loss: 0.3466, Test Loss: 0.1950\n",
            "Epoch [504/6000], Train Loss: 0.3462, Test Loss: 0.1947\n",
            "Epoch [505/6000], Train Loss: 0.3459, Test Loss: 0.1943\n",
            "Epoch [506/6000], Train Loss: 0.3456, Test Loss: 0.1939\n",
            "Epoch [507/6000], Train Loss: 0.3452, Test Loss: 0.1935\n",
            "Epoch [508/6000], Train Loss: 0.3449, Test Loss: 0.1932\n",
            "Epoch [509/6000], Train Loss: 0.3445, Test Loss: 0.1928\n",
            "Epoch [510/6000], Train Loss: 0.3442, Test Loss: 0.1924\n",
            "Epoch [511/6000], Train Loss: 0.3439, Test Loss: 0.1921\n",
            "Epoch [512/6000], Train Loss: 0.3436, Test Loss: 0.1917\n",
            "Epoch [513/6000], Train Loss: 0.3433, Test Loss: 0.1912\n",
            "Epoch [514/6000], Train Loss: 0.3429, Test Loss: 0.1909\n",
            "Epoch [515/6000], Train Loss: 0.3426, Test Loss: 0.1907\n",
            "Epoch [516/6000], Train Loss: 0.3423, Test Loss: 0.1904\n",
            "Epoch [517/6000], Train Loss: 0.3420, Test Loss: 0.1900\n",
            "Epoch [518/6000], Train Loss: 0.3417, Test Loss: 0.1896\n",
            "Epoch [519/6000], Train Loss: 0.3414, Test Loss: 0.1894\n",
            "Epoch [520/6000], Train Loss: 0.3411, Test Loss: 0.1891\n",
            "Epoch [521/6000], Train Loss: 0.3407, Test Loss: 0.1887\n",
            "Epoch [522/6000], Train Loss: 0.3404, Test Loss: 0.1884\n",
            "Epoch [523/6000], Train Loss: 0.3401, Test Loss: 0.1881\n",
            "Epoch [524/6000], Train Loss: 0.3398, Test Loss: 0.1879\n",
            "Epoch [525/6000], Train Loss: 0.3394, Test Loss: 0.1876\n",
            "Epoch [526/6000], Train Loss: 0.3392, Test Loss: 0.1872\n",
            "Epoch [527/6000], Train Loss: 0.3388, Test Loss: 0.1868\n",
            "Epoch [528/6000], Train Loss: 0.3385, Test Loss: 0.1865\n",
            "Epoch [529/6000], Train Loss: 0.3383, Test Loss: 0.1864\n",
            "Epoch [530/6000], Train Loss: 0.3380, Test Loss: 0.1862\n",
            "Epoch [531/6000], Train Loss: 0.3377, Test Loss: 0.1860\n",
            "Epoch [532/6000], Train Loss: 0.3373, Test Loss: 0.1857\n",
            "Epoch [533/6000], Train Loss: 0.3370, Test Loss: 0.1854\n",
            "Epoch [534/6000], Train Loss: 0.3367, Test Loss: 0.1850\n",
            "Epoch [535/6000], Train Loss: 0.3365, Test Loss: 0.1846\n",
            "Epoch [536/6000], Train Loss: 0.3362, Test Loss: 0.1844\n",
            "Epoch [537/6000], Train Loss: 0.3358, Test Loss: 0.1840\n",
            "Epoch [538/6000], Train Loss: 0.3356, Test Loss: 0.1837\n",
            "Epoch [539/6000], Train Loss: 0.3352, Test Loss: 0.1834\n",
            "Epoch [540/6000], Train Loss: 0.3349, Test Loss: 0.1830\n",
            "Epoch [541/6000], Train Loss: 0.3346, Test Loss: 0.1828\n",
            "Epoch [542/6000], Train Loss: 0.3343, Test Loss: 0.1825\n",
            "Epoch [543/6000], Train Loss: 0.3340, Test Loss: 0.1821\n",
            "Epoch [544/6000], Train Loss: 0.3337, Test Loss: 0.1818\n",
            "Epoch [545/6000], Train Loss: 0.3334, Test Loss: 0.1815\n",
            "Epoch [546/6000], Train Loss: 0.3331, Test Loss: 0.1812\n",
            "Epoch [547/6000], Train Loss: 0.3328, Test Loss: 0.1809\n",
            "Epoch [548/6000], Train Loss: 0.3325, Test Loss: 0.1806\n",
            "Epoch [549/6000], Train Loss: 0.3322, Test Loss: 0.1803\n",
            "Epoch [550/6000], Train Loss: 0.3320, Test Loss: 0.1802\n",
            "Epoch [551/6000], Train Loss: 0.3316, Test Loss: 0.1800\n",
            "Epoch [552/6000], Train Loss: 0.3314, Test Loss: 0.1796\n",
            "Epoch [553/6000], Train Loss: 0.3311, Test Loss: 0.1795\n",
            "Epoch [554/6000], Train Loss: 0.3307, Test Loss: 0.1791\n",
            "Epoch [555/6000], Train Loss: 0.3305, Test Loss: 0.1787\n",
            "Epoch [556/6000], Train Loss: 0.3302, Test Loss: 0.1785\n",
            "Epoch [557/6000], Train Loss: 0.3299, Test Loss: 0.1783\n",
            "Epoch [558/6000], Train Loss: 0.3296, Test Loss: 0.1780\n",
            "Epoch [559/6000], Train Loss: 0.3293, Test Loss: 0.1777\n",
            "Epoch [560/6000], Train Loss: 0.3290, Test Loss: 0.1774\n",
            "Epoch [561/6000], Train Loss: 0.3287, Test Loss: 0.1772\n",
            "Epoch [562/6000], Train Loss: 0.3284, Test Loss: 0.1770\n",
            "Epoch [563/6000], Train Loss: 0.3281, Test Loss: 0.1768\n",
            "Epoch [564/6000], Train Loss: 0.3278, Test Loss: 0.1766\n",
            "Epoch [565/6000], Train Loss: 0.3275, Test Loss: 0.1764\n",
            "Epoch [566/6000], Train Loss: 0.3273, Test Loss: 0.1763\n",
            "Epoch [567/6000], Train Loss: 0.3270, Test Loss: 0.1760\n",
            "Epoch [568/6000], Train Loss: 0.3267, Test Loss: 0.1756\n",
            "Epoch [569/6000], Train Loss: 0.3264, Test Loss: 0.1753\n",
            "Epoch [570/6000], Train Loss: 0.3261, Test Loss: 0.1751\n",
            "Epoch [571/6000], Train Loss: 0.3259, Test Loss: 0.1749\n",
            "Epoch [572/6000], Train Loss: 0.3256, Test Loss: 0.1747\n",
            "Epoch [573/6000], Train Loss: 0.3253, Test Loss: 0.1745\n",
            "Epoch [574/6000], Train Loss: 0.3250, Test Loss: 0.1744\n",
            "Epoch [575/6000], Train Loss: 0.3247, Test Loss: 0.1741\n",
            "Epoch [576/6000], Train Loss: 0.3244, Test Loss: 0.1739\n",
            "Epoch [577/6000], Train Loss: 0.3242, Test Loss: 0.1737\n",
            "Epoch [578/6000], Train Loss: 0.3239, Test Loss: 0.1734\n",
            "Epoch [579/6000], Train Loss: 0.3236, Test Loss: 0.1731\n",
            "Epoch [580/6000], Train Loss: 0.3233, Test Loss: 0.1728\n",
            "Epoch [581/6000], Train Loss: 0.3230, Test Loss: 0.1727\n",
            "Epoch [582/6000], Train Loss: 0.3228, Test Loss: 0.1724\n",
            "Epoch [583/6000], Train Loss: 0.3225, Test Loss: 0.1723\n",
            "Epoch [584/6000], Train Loss: 0.3222, Test Loss: 0.1720\n",
            "Epoch [585/6000], Train Loss: 0.3220, Test Loss: 0.1719\n",
            "Epoch [586/6000], Train Loss: 0.3217, Test Loss: 0.1718\n",
            "Epoch [587/6000], Train Loss: 0.3214, Test Loss: 0.1717\n",
            "Epoch [588/6000], Train Loss: 0.3211, Test Loss: 0.1715\n",
            "Epoch [589/6000], Train Loss: 0.3208, Test Loss: 0.1712\n",
            "Epoch [590/6000], Train Loss: 0.3205, Test Loss: 0.1710\n",
            "Epoch [591/6000], Train Loss: 0.3203, Test Loss: 0.1708\n",
            "Epoch [592/6000], Train Loss: 0.3200, Test Loss: 0.1706\n",
            "Epoch [593/6000], Train Loss: 0.3197, Test Loss: 0.1704\n",
            "Epoch [594/6000], Train Loss: 0.3194, Test Loss: 0.1702\n",
            "Epoch [595/6000], Train Loss: 0.3192, Test Loss: 0.1699\n",
            "Epoch [596/6000], Train Loss: 0.3189, Test Loss: 0.1697\n",
            "Epoch [597/6000], Train Loss: 0.3186, Test Loss: 0.1694\n",
            "Epoch [598/6000], Train Loss: 0.3184, Test Loss: 0.1693\n",
            "Epoch [599/6000], Train Loss: 0.3181, Test Loss: 0.1691\n",
            "Epoch [600/6000], Train Loss: 0.3179, Test Loss: 0.1689\n",
            "Epoch [601/6000], Train Loss: 0.3176, Test Loss: 0.1687\n",
            "Epoch [602/6000], Train Loss: 0.3173, Test Loss: 0.1685\n",
            "Epoch [603/6000], Train Loss: 0.3171, Test Loss: 0.1684\n",
            "Epoch [604/6000], Train Loss: 0.3167, Test Loss: 0.1682\n",
            "Epoch [605/6000], Train Loss: 0.3165, Test Loss: 0.1680\n",
            "Epoch [606/6000], Train Loss: 0.3162, Test Loss: 0.1678\n",
            "Epoch [607/6000], Train Loss: 0.3159, Test Loss: 0.1676\n",
            "Epoch [608/6000], Train Loss: 0.3157, Test Loss: 0.1674\n",
            "Epoch [609/6000], Train Loss: 0.3154, Test Loss: 0.1673\n",
            "Epoch [610/6000], Train Loss: 0.3152, Test Loss: 0.1672\n",
            "Epoch [611/6000], Train Loss: 0.3149, Test Loss: 0.1669\n",
            "Epoch [612/6000], Train Loss: 0.3146, Test Loss: 0.1667\n",
            "Epoch [613/6000], Train Loss: 0.3143, Test Loss: 0.1665\n",
            "Epoch [614/6000], Train Loss: 0.3141, Test Loss: 0.1663\n",
            "Epoch [615/6000], Train Loss: 0.3138, Test Loss: 0.1662\n",
            "Epoch [616/6000], Train Loss: 0.3135, Test Loss: 0.1660\n",
            "Epoch [617/6000], Train Loss: 0.3133, Test Loss: 0.1659\n",
            "Epoch [618/6000], Train Loss: 0.3130, Test Loss: 0.1657\n",
            "Epoch [619/6000], Train Loss: 0.3128, Test Loss: 0.1654\n",
            "Epoch [620/6000], Train Loss: 0.3125, Test Loss: 0.1652\n",
            "Epoch [621/6000], Train Loss: 0.3122, Test Loss: 0.1650\n",
            "Epoch [622/6000], Train Loss: 0.3119, Test Loss: 0.1649\n",
            "Epoch [623/6000], Train Loss: 0.3117, Test Loss: 0.1647\n",
            "Epoch [624/6000], Train Loss: 0.3114, Test Loss: 0.1645\n",
            "Epoch [625/6000], Train Loss: 0.3112, Test Loss: 0.1643\n",
            "Epoch [626/6000], Train Loss: 0.3110, Test Loss: 0.1643\n",
            "Epoch [627/6000], Train Loss: 0.3106, Test Loss: 0.1641\n",
            "Epoch [628/6000], Train Loss: 0.3104, Test Loss: 0.1639\n",
            "Epoch [629/6000], Train Loss: 0.3101, Test Loss: 0.1638\n",
            "Epoch [630/6000], Train Loss: 0.3099, Test Loss: 0.1637\n",
            "Epoch [631/6000], Train Loss: 0.3097, Test Loss: 0.1636\n",
            "Epoch [632/6000], Train Loss: 0.3094, Test Loss: 0.1633\n",
            "Epoch [633/6000], Train Loss: 0.3091, Test Loss: 0.1632\n",
            "Epoch [634/6000], Train Loss: 0.3088, Test Loss: 0.1630\n",
            "Epoch [635/6000], Train Loss: 0.3085, Test Loss: 0.1628\n",
            "Epoch [636/6000], Train Loss: 0.3084, Test Loss: 0.1628\n",
            "Epoch [637/6000], Train Loss: 0.3081, Test Loss: 0.1625\n",
            "Epoch [638/6000], Train Loss: 0.3079, Test Loss: 0.1624\n",
            "Epoch [639/6000], Train Loss: 0.3075, Test Loss: 0.1622\n",
            "Epoch [640/6000], Train Loss: 0.3073, Test Loss: 0.1621\n",
            "Epoch [641/6000], Train Loss: 0.3070, Test Loss: 0.1619\n",
            "Epoch [642/6000], Train Loss: 0.3068, Test Loss: 0.1617\n",
            "Epoch [643/6000], Train Loss: 0.3065, Test Loss: 0.1615\n",
            "Epoch [644/6000], Train Loss: 0.3062, Test Loss: 0.1614\n",
            "Epoch [645/6000], Train Loss: 0.3060, Test Loss: 0.1613\n",
            "Epoch [646/6000], Train Loss: 0.3058, Test Loss: 0.1611\n",
            "Epoch [647/6000], Train Loss: 0.3056, Test Loss: 0.1610\n",
            "Epoch [648/6000], Train Loss: 0.3052, Test Loss: 0.1608\n",
            "Epoch [649/6000], Train Loss: 0.3050, Test Loss: 0.1606\n",
            "Epoch [650/6000], Train Loss: 0.3048, Test Loss: 0.1604\n",
            "Epoch [651/6000], Train Loss: 0.3045, Test Loss: 0.1602\n",
            "Epoch [652/6000], Train Loss: 0.3043, Test Loss: 0.1602\n",
            "Epoch [653/6000], Train Loss: 0.3039, Test Loss: 0.1600\n",
            "Epoch [654/6000], Train Loss: 0.3037, Test Loss: 0.1598\n",
            "Epoch [655/6000], Train Loss: 0.3035, Test Loss: 0.1596\n",
            "Epoch [656/6000], Train Loss: 0.3032, Test Loss: 0.1595\n",
            "Epoch [657/6000], Train Loss: 0.3030, Test Loss: 0.1595\n",
            "Epoch [658/6000], Train Loss: 0.3027, Test Loss: 0.1593\n",
            "Epoch [659/6000], Train Loss: 0.3025, Test Loss: 0.1591\n",
            "Epoch [660/6000], Train Loss: 0.3022, Test Loss: 0.1589\n",
            "Epoch [661/6000], Train Loss: 0.3020, Test Loss: 0.1587\n",
            "Epoch [662/6000], Train Loss: 0.3018, Test Loss: 0.1587\n",
            "Epoch [663/6000], Train Loss: 0.3015, Test Loss: 0.1585\n",
            "Epoch [664/6000], Train Loss: 0.3012, Test Loss: 0.1583\n",
            "Epoch [665/6000], Train Loss: 0.3010, Test Loss: 0.1581\n",
            "Epoch [666/6000], Train Loss: 0.3007, Test Loss: 0.1580\n",
            "Epoch [667/6000], Train Loss: 0.3005, Test Loss: 0.1579\n",
            "Epoch [668/6000], Train Loss: 0.3002, Test Loss: 0.1577\n",
            "Epoch [669/6000], Train Loss: 0.3000, Test Loss: 0.1576\n",
            "Epoch [670/6000], Train Loss: 0.2997, Test Loss: 0.1575\n",
            "Epoch [671/6000], Train Loss: 0.2995, Test Loss: 0.1574\n",
            "Epoch [672/6000], Train Loss: 0.2992, Test Loss: 0.1573\n",
            "Epoch [673/6000], Train Loss: 0.2990, Test Loss: 0.1570\n",
            "Epoch [674/6000], Train Loss: 0.2987, Test Loss: 0.1570\n",
            "Epoch [675/6000], Train Loss: 0.2985, Test Loss: 0.1568\n",
            "Epoch [676/6000], Train Loss: 0.2983, Test Loss: 0.1566\n",
            "Epoch [677/6000], Train Loss: 0.2980, Test Loss: 0.1565\n",
            "Epoch [678/6000], Train Loss: 0.2978, Test Loss: 0.1563\n",
            "Epoch [679/6000], Train Loss: 0.2975, Test Loss: 0.1563\n",
            "Epoch [680/6000], Train Loss: 0.2973, Test Loss: 0.1562\n",
            "Epoch [681/6000], Train Loss: 0.2970, Test Loss: 0.1560\n",
            "Epoch [682/6000], Train Loss: 0.2968, Test Loss: 0.1559\n",
            "Epoch [683/6000], Train Loss: 0.2966, Test Loss: 0.1558\n",
            "Epoch [684/6000], Train Loss: 0.2963, Test Loss: 0.1556\n",
            "Epoch [685/6000], Train Loss: 0.2961, Test Loss: 0.1554\n",
            "Epoch [686/6000], Train Loss: 0.2958, Test Loss: 0.1552\n",
            "Epoch [687/6000], Train Loss: 0.2957, Test Loss: 0.1552\n",
            "Epoch [688/6000], Train Loss: 0.2953, Test Loss: 0.1551\n",
            "Epoch [689/6000], Train Loss: 0.2951, Test Loss: 0.1550\n",
            "Epoch [690/6000], Train Loss: 0.2949, Test Loss: 0.1548\n",
            "Epoch [691/6000], Train Loss: 0.2946, Test Loss: 0.1547\n",
            "Epoch [692/6000], Train Loss: 0.2944, Test Loss: 0.1546\n",
            "Epoch [693/6000], Train Loss: 0.2941, Test Loss: 0.1545\n",
            "Epoch [694/6000], Train Loss: 0.2939, Test Loss: 0.1544\n",
            "Epoch [695/6000], Train Loss: 0.2936, Test Loss: 0.1544\n",
            "Epoch [696/6000], Train Loss: 0.2935, Test Loss: 0.1543\n",
            "Epoch [697/6000], Train Loss: 0.2931, Test Loss: 0.1542\n",
            "Epoch [698/6000], Train Loss: 0.2929, Test Loss: 0.1541\n",
            "Epoch [699/6000], Train Loss: 0.2927, Test Loss: 0.1540\n",
            "Epoch [700/6000], Train Loss: 0.2924, Test Loss: 0.1539\n",
            "Epoch [701/6000], Train Loss: 0.2922, Test Loss: 0.1537\n",
            "Epoch [702/6000], Train Loss: 0.2920, Test Loss: 0.1536\n",
            "Epoch [703/6000], Train Loss: 0.2917, Test Loss: 0.1534\n",
            "Epoch [704/6000], Train Loss: 0.2915, Test Loss: 0.1534\n",
            "Epoch [705/6000], Train Loss: 0.2913, Test Loss: 0.1532\n",
            "Epoch [706/6000], Train Loss: 0.2910, Test Loss: 0.1530\n",
            "Epoch [707/6000], Train Loss: 0.2908, Test Loss: 0.1530\n",
            "Epoch [708/6000], Train Loss: 0.2905, Test Loss: 0.1530\n",
            "Epoch [709/6000], Train Loss: 0.2903, Test Loss: 0.1528\n",
            "Epoch [710/6000], Train Loss: 0.2901, Test Loss: 0.1527\n",
            "Epoch [711/6000], Train Loss: 0.2898, Test Loss: 0.1525\n",
            "Epoch [712/6000], Train Loss: 0.2897, Test Loss: 0.1525\n",
            "Epoch [713/6000], Train Loss: 0.2894, Test Loss: 0.1523\n",
            "Epoch [714/6000], Train Loss: 0.2891, Test Loss: 0.1522\n",
            "Epoch [715/6000], Train Loss: 0.2889, Test Loss: 0.1520\n",
            "Epoch [716/6000], Train Loss: 0.2886, Test Loss: 0.1518\n",
            "Epoch [717/6000], Train Loss: 0.2884, Test Loss: 0.1516\n",
            "Epoch [718/6000], Train Loss: 0.2882, Test Loss: 0.1515\n",
            "Epoch [719/6000], Train Loss: 0.2879, Test Loss: 0.1514\n",
            "Epoch [720/6000], Train Loss: 0.2877, Test Loss: 0.1513\n",
            "Epoch [721/6000], Train Loss: 0.2876, Test Loss: 0.1513\n",
            "Epoch [722/6000], Train Loss: 0.2872, Test Loss: 0.1512\n",
            "Epoch [723/6000], Train Loss: 0.2870, Test Loss: 0.1511\n",
            "Epoch [724/6000], Train Loss: 0.2868, Test Loss: 0.1511\n",
            "Epoch [725/6000], Train Loss: 0.2866, Test Loss: 0.1509\n",
            "Epoch [726/6000], Train Loss: 0.2863, Test Loss: 0.1509\n",
            "Epoch [727/6000], Train Loss: 0.2861, Test Loss: 0.1508\n",
            "Epoch [728/6000], Train Loss: 0.2859, Test Loss: 0.1506\n",
            "Epoch [729/6000], Train Loss: 0.2856, Test Loss: 0.1505\n",
            "Epoch [730/6000], Train Loss: 0.2855, Test Loss: 0.1504\n",
            "Epoch [731/6000], Train Loss: 0.2851, Test Loss: 0.1504\n",
            "Epoch [732/6000], Train Loss: 0.2849, Test Loss: 0.1502\n",
            "Epoch [733/6000], Train Loss: 0.2847, Test Loss: 0.1500\n",
            "Epoch [734/6000], Train Loss: 0.2845, Test Loss: 0.1499\n",
            "Epoch [735/6000], Train Loss: 0.2843, Test Loss: 0.1500\n",
            "Epoch [736/6000], Train Loss: 0.2840, Test Loss: 0.1498\n",
            "Epoch [737/6000], Train Loss: 0.2838, Test Loss: 0.1498\n",
            "Epoch [738/6000], Train Loss: 0.2836, Test Loss: 0.1497\n",
            "Epoch [739/6000], Train Loss: 0.2833, Test Loss: 0.1495\n",
            "Epoch [740/6000], Train Loss: 0.2831, Test Loss: 0.1495\n",
            "Epoch [741/6000], Train Loss: 0.2828, Test Loss: 0.1493\n",
            "Epoch [742/6000], Train Loss: 0.2826, Test Loss: 0.1492\n",
            "Epoch [743/6000], Train Loss: 0.2824, Test Loss: 0.1490\n",
            "Epoch [744/6000], Train Loss: 0.2822, Test Loss: 0.1489\n",
            "Epoch [745/6000], Train Loss: 0.2820, Test Loss: 0.1488\n",
            "Epoch [746/6000], Train Loss: 0.2817, Test Loss: 0.1487\n",
            "Epoch [747/6000], Train Loss: 0.2815, Test Loss: 0.1486\n",
            "Epoch [748/6000], Train Loss: 0.2813, Test Loss: 0.1486\n",
            "Epoch [749/6000], Train Loss: 0.2811, Test Loss: 0.1486\n",
            "Epoch [750/6000], Train Loss: 0.2808, Test Loss: 0.1484\n",
            "Epoch [751/6000], Train Loss: 0.2806, Test Loss: 0.1484\n",
            "Epoch [752/6000], Train Loss: 0.2804, Test Loss: 0.1484\n",
            "Epoch [753/6000], Train Loss: 0.2802, Test Loss: 0.1484\n",
            "Epoch [754/6000], Train Loss: 0.2799, Test Loss: 0.1482\n",
            "Epoch [755/6000], Train Loss: 0.2797, Test Loss: 0.1481\n",
            "Epoch [756/6000], Train Loss: 0.2795, Test Loss: 0.1480\n",
            "Epoch [757/6000], Train Loss: 0.2793, Test Loss: 0.1480\n",
            "Epoch [758/6000], Train Loss: 0.2790, Test Loss: 0.1478\n",
            "Epoch [759/6000], Train Loss: 0.2788, Test Loss: 0.1478\n",
            "Epoch [760/6000], Train Loss: 0.2785, Test Loss: 0.1476\n",
            "Epoch [761/6000], Train Loss: 0.2783, Test Loss: 0.1475\n",
            "Epoch [762/6000], Train Loss: 0.2781, Test Loss: 0.1474\n",
            "Epoch [763/6000], Train Loss: 0.2779, Test Loss: 0.1473\n",
            "Epoch [764/6000], Train Loss: 0.2776, Test Loss: 0.1470\n",
            "Epoch [765/6000], Train Loss: 0.2774, Test Loss: 0.1468\n",
            "Epoch [766/6000], Train Loss: 0.2772, Test Loss: 0.1467\n",
            "Epoch [767/6000], Train Loss: 0.2770, Test Loss: 0.1468\n",
            "Epoch [768/6000], Train Loss: 0.2767, Test Loss: 0.1466\n",
            "Epoch [769/6000], Train Loss: 0.2765, Test Loss: 0.1466\n",
            "Epoch [770/6000], Train Loss: 0.2763, Test Loss: 0.1466\n",
            "Epoch [771/6000], Train Loss: 0.2761, Test Loss: 0.1465\n",
            "Epoch [772/6000], Train Loss: 0.2759, Test Loss: 0.1465\n",
            "Epoch [773/6000], Train Loss: 0.2757, Test Loss: 0.1463\n",
            "Epoch [774/6000], Train Loss: 0.2754, Test Loss: 0.1462\n",
            "Epoch [775/6000], Train Loss: 0.2752, Test Loss: 0.1462\n",
            "Epoch [776/6000], Train Loss: 0.2750, Test Loss: 0.1460\n",
            "Epoch [777/6000], Train Loss: 0.2748, Test Loss: 0.1460\n",
            "Epoch [778/6000], Train Loss: 0.2746, Test Loss: 0.1459\n",
            "Epoch [779/6000], Train Loss: 0.2743, Test Loss: 0.1458\n",
            "Epoch [780/6000], Train Loss: 0.2741, Test Loss: 0.1456\n",
            "Epoch [781/6000], Train Loss: 0.2739, Test Loss: 0.1456\n",
            "Epoch [782/6000], Train Loss: 0.2737, Test Loss: 0.1454\n",
            "Epoch [783/6000], Train Loss: 0.2735, Test Loss: 0.1453\n",
            "Epoch [784/6000], Train Loss: 0.2732, Test Loss: 0.1452\n",
            "Epoch [785/6000], Train Loss: 0.2730, Test Loss: 0.1451\n",
            "Epoch [786/6000], Train Loss: 0.2729, Test Loss: 0.1451\n",
            "Epoch [787/6000], Train Loss: 0.2726, Test Loss: 0.1450\n",
            "Epoch [788/6000], Train Loss: 0.2724, Test Loss: 0.1450\n",
            "Epoch [789/6000], Train Loss: 0.2721, Test Loss: 0.1449\n",
            "Epoch [790/6000], Train Loss: 0.2719, Test Loss: 0.1448\n",
            "Epoch [791/6000], Train Loss: 0.2718, Test Loss: 0.1448\n",
            "Epoch [792/6000], Train Loss: 0.2715, Test Loss: 0.1446\n",
            "Epoch [793/6000], Train Loss: 0.2713, Test Loss: 0.1446\n",
            "Epoch [794/6000], Train Loss: 0.2710, Test Loss: 0.1445\n",
            "Epoch [795/6000], Train Loss: 0.2709, Test Loss: 0.1444\n",
            "Epoch [796/6000], Train Loss: 0.2706, Test Loss: 0.1443\n",
            "Epoch [797/6000], Train Loss: 0.2704, Test Loss: 0.1442\n",
            "Epoch [798/6000], Train Loss: 0.2702, Test Loss: 0.1441\n",
            "Epoch [799/6000], Train Loss: 0.2700, Test Loss: 0.1439\n",
            "Epoch [800/6000], Train Loss: 0.2697, Test Loss: 0.1438\n",
            "Epoch [801/6000], Train Loss: 0.2696, Test Loss: 0.1436\n",
            "Epoch [802/6000], Train Loss: 0.2693, Test Loss: 0.1436\n",
            "Epoch [803/6000], Train Loss: 0.2691, Test Loss: 0.1436\n",
            "Epoch [804/6000], Train Loss: 0.2689, Test Loss: 0.1435\n",
            "Epoch [805/6000], Train Loss: 0.2687, Test Loss: 0.1435\n",
            "Epoch [806/6000], Train Loss: 0.2685, Test Loss: 0.1434\n",
            "Epoch [807/6000], Train Loss: 0.2682, Test Loss: 0.1434\n",
            "Epoch [808/6000], Train Loss: 0.2680, Test Loss: 0.1432\n",
            "Epoch [809/6000], Train Loss: 0.2678, Test Loss: 0.1431\n",
            "Epoch [810/6000], Train Loss: 0.2676, Test Loss: 0.1431\n",
            "Epoch [811/6000], Train Loss: 0.2674, Test Loss: 0.1429\n",
            "Epoch [812/6000], Train Loss: 0.2672, Test Loss: 0.1428\n",
            "Epoch [813/6000], Train Loss: 0.2670, Test Loss: 0.1428\n",
            "Epoch [814/6000], Train Loss: 0.2667, Test Loss: 0.1427\n",
            "Epoch [815/6000], Train Loss: 0.2666, Test Loss: 0.1427\n",
            "Epoch [816/6000], Train Loss: 0.2663, Test Loss: 0.1426\n",
            "Epoch [817/6000], Train Loss: 0.2661, Test Loss: 0.1426\n",
            "Epoch [818/6000], Train Loss: 0.2659, Test Loss: 0.1424\n",
            "Epoch [819/6000], Train Loss: 0.2657, Test Loss: 0.1424\n",
            "Epoch [820/6000], Train Loss: 0.2655, Test Loss: 0.1424\n",
            "Epoch [821/6000], Train Loss: 0.2653, Test Loss: 0.1422\n",
            "Epoch [822/6000], Train Loss: 0.2650, Test Loss: 0.1422\n",
            "Epoch [823/6000], Train Loss: 0.2648, Test Loss: 0.1420\n",
            "Epoch [824/6000], Train Loss: 0.2646, Test Loss: 0.1419\n",
            "Epoch [825/6000], Train Loss: 0.2644, Test Loss: 0.1418\n",
            "Epoch [826/6000], Train Loss: 0.2642, Test Loss: 0.1417\n",
            "Epoch [827/6000], Train Loss: 0.2640, Test Loss: 0.1416\n",
            "Epoch [828/6000], Train Loss: 0.2638, Test Loss: 0.1416\n",
            "Epoch [829/6000], Train Loss: 0.2636, Test Loss: 0.1416\n",
            "Epoch [830/6000], Train Loss: 0.2634, Test Loss: 0.1415\n",
            "Epoch [831/6000], Train Loss: 0.2631, Test Loss: 0.1415\n",
            "Epoch [832/6000], Train Loss: 0.2629, Test Loss: 0.1413\n",
            "Epoch [833/6000], Train Loss: 0.2627, Test Loss: 0.1413\n",
            "Epoch [834/6000], Train Loss: 0.2626, Test Loss: 0.1413\n",
            "Epoch [835/6000], Train Loss: 0.2623, Test Loss: 0.1412\n",
            "Epoch [836/6000], Train Loss: 0.2621, Test Loss: 0.1412\n",
            "Epoch [837/6000], Train Loss: 0.2619, Test Loss: 0.1411\n",
            "Epoch [838/6000], Train Loss: 0.2617, Test Loss: 0.1409\n",
            "Epoch [839/6000], Train Loss: 0.2615, Test Loss: 0.1409\n",
            "Epoch [840/6000], Train Loss: 0.2613, Test Loss: 0.1409\n",
            "Epoch [841/6000], Train Loss: 0.2610, Test Loss: 0.1407\n",
            "Epoch [842/6000], Train Loss: 0.2609, Test Loss: 0.1407\n",
            "Epoch [843/6000], Train Loss: 0.2607, Test Loss: 0.1407\n",
            "Epoch [844/6000], Train Loss: 0.2605, Test Loss: 0.1407\n",
            "Epoch [845/6000], Train Loss: 0.2602, Test Loss: 0.1405\n",
            "Epoch [846/6000], Train Loss: 0.2600, Test Loss: 0.1403\n",
            "Epoch [847/6000], Train Loss: 0.2598, Test Loss: 0.1403\n",
            "Epoch [848/6000], Train Loss: 0.2596, Test Loss: 0.1402\n",
            "Epoch [849/6000], Train Loss: 0.2594, Test Loss: 0.1402\n",
            "Epoch [850/6000], Train Loss: 0.2592, Test Loss: 0.1401\n",
            "Epoch [851/6000], Train Loss: 0.2590, Test Loss: 0.1400\n",
            "Epoch [852/6000], Train Loss: 0.2587, Test Loss: 0.1400\n",
            "Epoch [853/6000], Train Loss: 0.2585, Test Loss: 0.1398\n",
            "Epoch [854/6000], Train Loss: 0.2584, Test Loss: 0.1399\n",
            "Epoch [855/6000], Train Loss: 0.2582, Test Loss: 0.1399\n",
            "Epoch [856/6000], Train Loss: 0.2579, Test Loss: 0.1398\n",
            "Epoch [857/6000], Train Loss: 0.2577, Test Loss: 0.1398\n",
            "Epoch [858/6000], Train Loss: 0.2576, Test Loss: 0.1398\n",
            "Epoch [859/6000], Train Loss: 0.2573, Test Loss: 0.1397\n",
            "Epoch [860/6000], Train Loss: 0.2571, Test Loss: 0.1396\n",
            "Epoch [861/6000], Train Loss: 0.2569, Test Loss: 0.1395\n",
            "Epoch [862/6000], Train Loss: 0.2567, Test Loss: 0.1394\n",
            "Epoch [863/6000], Train Loss: 0.2565, Test Loss: 0.1393\n",
            "Epoch [864/6000], Train Loss: 0.2563, Test Loss: 0.1391\n",
            "Epoch [865/6000], Train Loss: 0.2561, Test Loss: 0.1390\n",
            "Epoch [866/6000], Train Loss: 0.2559, Test Loss: 0.1390\n",
            "Epoch [867/6000], Train Loss: 0.2556, Test Loss: 0.1388\n",
            "Epoch [868/6000], Train Loss: 0.2554, Test Loss: 0.1388\n",
            "Epoch [869/6000], Train Loss: 0.2553, Test Loss: 0.1387\n",
            "Epoch [870/6000], Train Loss: 0.2550, Test Loss: 0.1386\n",
            "Epoch [871/6000], Train Loss: 0.2548, Test Loss: 0.1384\n",
            "Epoch [872/6000], Train Loss: 0.2546, Test Loss: 0.1384\n",
            "Epoch [873/6000], Train Loss: 0.2544, Test Loss: 0.1384\n",
            "Epoch [874/6000], Train Loss: 0.2542, Test Loss: 0.1383\n",
            "Epoch [875/6000], Train Loss: 0.2540, Test Loss: 0.1382\n",
            "Epoch [876/6000], Train Loss: 0.2538, Test Loss: 0.1381\n",
            "Epoch [877/6000], Train Loss: 0.2536, Test Loss: 0.1381\n",
            "Epoch [878/6000], Train Loss: 0.2534, Test Loss: 0.1381\n",
            "Epoch [879/6000], Train Loss: 0.2533, Test Loss: 0.1381\n",
            "Epoch [880/6000], Train Loss: 0.2530, Test Loss: 0.1380\n",
            "Epoch [881/6000], Train Loss: 0.2528, Test Loss: 0.1379\n",
            "Epoch [882/6000], Train Loss: 0.2526, Test Loss: 0.1379\n",
            "Epoch [883/6000], Train Loss: 0.2524, Test Loss: 0.1378\n",
            "Epoch [884/6000], Train Loss: 0.2522, Test Loss: 0.1377\n",
            "Epoch [885/6000], Train Loss: 0.2520, Test Loss: 0.1377\n",
            "Epoch [886/6000], Train Loss: 0.2518, Test Loss: 0.1376\n",
            "Epoch [887/6000], Train Loss: 0.2516, Test Loss: 0.1376\n",
            "Epoch [888/6000], Train Loss: 0.2514, Test Loss: 0.1376\n",
            "Epoch [889/6000], Train Loss: 0.2512, Test Loss: 0.1375\n",
            "Epoch [890/6000], Train Loss: 0.2510, Test Loss: 0.1374\n",
            "Epoch [891/6000], Train Loss: 0.2507, Test Loss: 0.1372\n",
            "Epoch [892/6000], Train Loss: 0.2506, Test Loss: 0.1371\n",
            "Epoch [893/6000], Train Loss: 0.2503, Test Loss: 0.1370\n",
            "Epoch [894/6000], Train Loss: 0.2501, Test Loss: 0.1368\n",
            "Epoch [895/6000], Train Loss: 0.2500, Test Loss: 0.1368\n",
            "Epoch [896/6000], Train Loss: 0.2497, Test Loss: 0.1367\n",
            "Epoch [897/6000], Train Loss: 0.2495, Test Loss: 0.1367\n",
            "Epoch [898/6000], Train Loss: 0.2493, Test Loss: 0.1366\n",
            "Epoch [899/6000], Train Loss: 0.2491, Test Loss: 0.1366\n",
            "Epoch [900/6000], Train Loss: 0.2490, Test Loss: 0.1365\n",
            "Epoch [901/6000], Train Loss: 0.2487, Test Loss: 0.1364\n",
            "Epoch [902/6000], Train Loss: 0.2485, Test Loss: 0.1364\n",
            "Epoch [903/6000], Train Loss: 0.2483, Test Loss: 0.1364\n",
            "Epoch [904/6000], Train Loss: 0.2481, Test Loss: 0.1365\n",
            "Epoch [905/6000], Train Loss: 0.2479, Test Loss: 0.1364\n",
            "Epoch [906/6000], Train Loss: 0.2478, Test Loss: 0.1364\n",
            "Epoch [907/6000], Train Loss: 0.2475, Test Loss: 0.1364\n",
            "Epoch [908/6000], Train Loss: 0.2473, Test Loss: 0.1363\n",
            "Epoch [909/6000], Train Loss: 0.2472, Test Loss: 0.1362\n",
            "Epoch [910/6000], Train Loss: 0.2469, Test Loss: 0.1362\n",
            "Epoch [911/6000], Train Loss: 0.2467, Test Loss: 0.1362\n",
            "Epoch [912/6000], Train Loss: 0.2465, Test Loss: 0.1361\n",
            "Epoch [913/6000], Train Loss: 0.2464, Test Loss: 0.1360\n",
            "Epoch [914/6000], Train Loss: 0.2462, Test Loss: 0.1360\n",
            "Epoch [915/6000], Train Loss: 0.2460, Test Loss: 0.1360\n",
            "Epoch [916/6000], Train Loss: 0.2457, Test Loss: 0.1359\n",
            "Epoch [917/6000], Train Loss: 0.2455, Test Loss: 0.1359\n",
            "Epoch [918/6000], Train Loss: 0.2453, Test Loss: 0.1357\n",
            "Epoch [919/6000], Train Loss: 0.2452, Test Loss: 0.1357\n",
            "Epoch [920/6000], Train Loss: 0.2449, Test Loss: 0.1355\n",
            "Epoch [921/6000], Train Loss: 0.2448, Test Loss: 0.1355\n",
            "Epoch [922/6000], Train Loss: 0.2445, Test Loss: 0.1353\n",
            "Epoch [923/6000], Train Loss: 0.2443, Test Loss: 0.1353\n",
            "Epoch [924/6000], Train Loss: 0.2441, Test Loss: 0.1352\n",
            "Epoch [925/6000], Train Loss: 0.2439, Test Loss: 0.1350\n",
            "Epoch [926/6000], Train Loss: 0.2438, Test Loss: 0.1349\n",
            "Epoch [927/6000], Train Loss: 0.2435, Test Loss: 0.1349\n",
            "Epoch [928/6000], Train Loss: 0.2434, Test Loss: 0.1348\n",
            "Epoch [929/6000], Train Loss: 0.2431, Test Loss: 0.1348\n",
            "Epoch [930/6000], Train Loss: 0.2429, Test Loss: 0.1348\n",
            "Epoch [931/6000], Train Loss: 0.2427, Test Loss: 0.1346\n",
            "Epoch [932/6000], Train Loss: 0.2425, Test Loss: 0.1346\n",
            "Epoch [933/6000], Train Loss: 0.2424, Test Loss: 0.1346\n",
            "Epoch [934/6000], Train Loss: 0.2422, Test Loss: 0.1346\n",
            "Epoch [935/6000], Train Loss: 0.2420, Test Loss: 0.1346\n",
            "Epoch [936/6000], Train Loss: 0.2418, Test Loss: 0.1346\n",
            "Epoch [937/6000], Train Loss: 0.2416, Test Loss: 0.1346\n",
            "Epoch [938/6000], Train Loss: 0.2414, Test Loss: 0.1346\n",
            "Epoch [939/6000], Train Loss: 0.2412, Test Loss: 0.1344\n",
            "Epoch [940/6000], Train Loss: 0.2410, Test Loss: 0.1343\n",
            "Epoch [941/6000], Train Loss: 0.2408, Test Loss: 0.1342\n",
            "Epoch [942/6000], Train Loss: 0.2406, Test Loss: 0.1341\n",
            "Epoch [943/6000], Train Loss: 0.2404, Test Loss: 0.1340\n",
            "Epoch [944/6000], Train Loss: 0.2402, Test Loss: 0.1340\n",
            "Epoch [945/6000], Train Loss: 0.2400, Test Loss: 0.1339\n",
            "Epoch [946/6000], Train Loss: 0.2398, Test Loss: 0.1339\n",
            "Epoch [947/6000], Train Loss: 0.2397, Test Loss: 0.1340\n",
            "Epoch [948/6000], Train Loss: 0.2394, Test Loss: 0.1338\n",
            "Epoch [949/6000], Train Loss: 0.2392, Test Loss: 0.1337\n",
            "Epoch [950/6000], Train Loss: 0.2390, Test Loss: 0.1337\n",
            "Epoch [951/6000], Train Loss: 0.2389, Test Loss: 0.1337\n",
            "Epoch [952/6000], Train Loss: 0.2386, Test Loss: 0.1336\n",
            "Epoch [953/6000], Train Loss: 0.2384, Test Loss: 0.1335\n",
            "Epoch [954/6000], Train Loss: 0.2382, Test Loss: 0.1334\n",
            "Epoch [955/6000], Train Loss: 0.2380, Test Loss: 0.1333\n",
            "Epoch [956/6000], Train Loss: 0.2378, Test Loss: 0.1333\n",
            "Epoch [957/6000], Train Loss: 0.2377, Test Loss: 0.1333\n",
            "Epoch [958/6000], Train Loss: 0.2375, Test Loss: 0.1332\n",
            "Epoch [959/6000], Train Loss: 0.2373, Test Loss: 0.1332\n",
            "Epoch [960/6000], Train Loss: 0.2371, Test Loss: 0.1332\n",
            "Epoch [961/6000], Train Loss: 0.2369, Test Loss: 0.1331\n",
            "Epoch [962/6000], Train Loss: 0.2367, Test Loss: 0.1329\n",
            "Epoch [963/6000], Train Loss: 0.2366, Test Loss: 0.1329\n",
            "Epoch [964/6000], Train Loss: 0.2363, Test Loss: 0.1328\n",
            "Epoch [965/6000], Train Loss: 0.2362, Test Loss: 0.1327\n",
            "Epoch [966/6000], Train Loss: 0.2360, Test Loss: 0.1326\n",
            "Epoch [967/6000], Train Loss: 0.2358, Test Loss: 0.1325\n",
            "Epoch [968/6000], Train Loss: 0.2356, Test Loss: 0.1325\n",
            "Epoch [969/6000], Train Loss: 0.2354, Test Loss: 0.1325\n",
            "Epoch [970/6000], Train Loss: 0.2352, Test Loss: 0.1325\n",
            "Epoch [971/6000], Train Loss: 0.2350, Test Loss: 0.1325\n",
            "Epoch [972/6000], Train Loss: 0.2349, Test Loss: 0.1325\n",
            "Epoch [973/6000], Train Loss: 0.2347, Test Loss: 0.1324\n",
            "Epoch [974/6000], Train Loss: 0.2345, Test Loss: 0.1324\n",
            "Epoch [975/6000], Train Loss: 0.2343, Test Loss: 0.1324\n",
            "Epoch [976/6000], Train Loss: 0.2342, Test Loss: 0.1324\n",
            "Epoch [977/6000], Train Loss: 0.2340, Test Loss: 0.1324\n",
            "Epoch [978/6000], Train Loss: 0.2337, Test Loss: 0.1323\n",
            "Epoch [979/6000], Train Loss: 0.2335, Test Loss: 0.1322\n",
            "Epoch [980/6000], Train Loss: 0.2334, Test Loss: 0.1321\n",
            "Epoch [981/6000], Train Loss: 0.2332, Test Loss: 0.1321\n",
            "Epoch [982/6000], Train Loss: 0.2330, Test Loss: 0.1321\n",
            "Epoch [983/6000], Train Loss: 0.2328, Test Loss: 0.1320\n",
            "Epoch [984/6000], Train Loss: 0.2326, Test Loss: 0.1319\n",
            "Epoch [985/6000], Train Loss: 0.2324, Test Loss: 0.1319\n",
            "Epoch [986/6000], Train Loss: 0.2323, Test Loss: 0.1318\n",
            "Epoch [987/6000], Train Loss: 0.2321, Test Loss: 0.1317\n",
            "Epoch [988/6000], Train Loss: 0.2319, Test Loss: 0.1315\n",
            "Epoch [989/6000], Train Loss: 0.2318, Test Loss: 0.1316\n",
            "Epoch [990/6000], Train Loss: 0.2315, Test Loss: 0.1316\n",
            "Epoch [991/6000], Train Loss: 0.2314, Test Loss: 0.1315\n",
            "Epoch [992/6000], Train Loss: 0.2312, Test Loss: 0.1314\n",
            "Epoch [993/6000], Train Loss: 0.2310, Test Loss: 0.1313\n",
            "Epoch [994/6000], Train Loss: 0.2309, Test Loss: 0.1314\n",
            "Epoch [995/6000], Train Loss: 0.2306, Test Loss: 0.1312\n",
            "Epoch [996/6000], Train Loss: 0.2304, Test Loss: 0.1311\n",
            "Epoch [997/6000], Train Loss: 0.2303, Test Loss: 0.1312\n",
            "Epoch [998/6000], Train Loss: 0.2301, Test Loss: 0.1311\n",
            "Epoch [999/6000], Train Loss: 0.2299, Test Loss: 0.1310\n",
            "Epoch [1000/6000], Train Loss: 0.2297, Test Loss: 0.1310\n",
            "Epoch [1001/6000], Train Loss: 0.2295, Test Loss: 0.1309\n",
            "Epoch [1002/6000], Train Loss: 0.2294, Test Loss: 0.1309\n",
            "Epoch [1003/6000], Train Loss: 0.2291, Test Loss: 0.1308\n",
            "Epoch [1004/6000], Train Loss: 0.2290, Test Loss: 0.1308\n",
            "Epoch [1005/6000], Train Loss: 0.2288, Test Loss: 0.1307\n",
            "Epoch [1006/6000], Train Loss: 0.2286, Test Loss: 0.1308\n",
            "Epoch [1007/6000], Train Loss: 0.2284, Test Loss: 0.1306\n",
            "Epoch [1008/6000], Train Loss: 0.2283, Test Loss: 0.1306\n",
            "Epoch [1009/6000], Train Loss: 0.2280, Test Loss: 0.1306\n",
            "Epoch [1010/6000], Train Loss: 0.2279, Test Loss: 0.1306\n",
            "Epoch [1011/6000], Train Loss: 0.2277, Test Loss: 0.1306\n",
            "Epoch [1012/6000], Train Loss: 0.2275, Test Loss: 0.1305\n",
            "Epoch [1013/6000], Train Loss: 0.2273, Test Loss: 0.1304\n",
            "Epoch [1014/6000], Train Loss: 0.2272, Test Loss: 0.1304\n",
            "Epoch [1015/6000], Train Loss: 0.2270, Test Loss: 0.1303\n",
            "Epoch [1016/6000], Train Loss: 0.2269, Test Loss: 0.1304\n",
            "Epoch [1017/6000], Train Loss: 0.2267, Test Loss: 0.1304\n",
            "Epoch [1018/6000], Train Loss: 0.2265, Test Loss: 0.1305\n",
            "Epoch [1019/6000], Train Loss: 0.2263, Test Loss: 0.1305\n",
            "Epoch [1020/6000], Train Loss: 0.2261, Test Loss: 0.1304\n",
            "Epoch [1021/6000], Train Loss: 0.2259, Test Loss: 0.1303\n",
            "Epoch [1022/6000], Train Loss: 0.2257, Test Loss: 0.1302\n",
            "Epoch [1023/6000], Train Loss: 0.2255, Test Loss: 0.1302\n",
            "Epoch [1024/6000], Train Loss: 0.2254, Test Loss: 0.1302\n",
            "Epoch [1025/6000], Train Loss: 0.2252, Test Loss: 0.1300\n",
            "Epoch [1026/6000], Train Loss: 0.2251, Test Loss: 0.1300\n",
            "Epoch [1027/6000], Train Loss: 0.2248, Test Loss: 0.1300\n",
            "Epoch [1028/6000], Train Loss: 0.2246, Test Loss: 0.1299\n",
            "Epoch [1029/6000], Train Loss: 0.2245, Test Loss: 0.1298\n",
            "Epoch [1030/6000], Train Loss: 0.2243, Test Loss: 0.1297\n",
            "Epoch [1031/6000], Train Loss: 0.2242, Test Loss: 0.1298\n",
            "Epoch [1032/6000], Train Loss: 0.2239, Test Loss: 0.1296\n",
            "Epoch [1033/6000], Train Loss: 0.2238, Test Loss: 0.1296\n",
            "Epoch [1034/6000], Train Loss: 0.2236, Test Loss: 0.1297\n",
            "Epoch [1035/6000], Train Loss: 0.2234, Test Loss: 0.1296\n",
            "Epoch [1036/6000], Train Loss: 0.2232, Test Loss: 0.1294\n",
            "Epoch [1037/6000], Train Loss: 0.2230, Test Loss: 0.1293\n",
            "Epoch [1038/6000], Train Loss: 0.2229, Test Loss: 0.1294\n",
            "Epoch [1039/6000], Train Loss: 0.2227, Test Loss: 0.1293\n",
            "Epoch [1040/6000], Train Loss: 0.2226, Test Loss: 0.1293\n",
            "Epoch [1041/6000], Train Loss: 0.2224, Test Loss: 0.1293\n",
            "Epoch [1042/6000], Train Loss: 0.2221, Test Loss: 0.1293\n",
            "Epoch [1043/6000], Train Loss: 0.2220, Test Loss: 0.1293\n",
            "Epoch [1044/6000], Train Loss: 0.2218, Test Loss: 0.1291\n",
            "Epoch [1045/6000], Train Loss: 0.2216, Test Loss: 0.1290\n",
            "Epoch [1046/6000], Train Loss: 0.2214, Test Loss: 0.1289\n",
            "Epoch [1047/6000], Train Loss: 0.2213, Test Loss: 0.1288\n",
            "Epoch [1048/6000], Train Loss: 0.2212, Test Loss: 0.1288\n",
            "Epoch [1049/6000], Train Loss: 0.2209, Test Loss: 0.1287\n",
            "Epoch [1050/6000], Train Loss: 0.2208, Test Loss: 0.1288\n",
            "Epoch [1051/6000], Train Loss: 0.2205, Test Loss: 0.1287\n",
            "Epoch [1052/6000], Train Loss: 0.2204, Test Loss: 0.1286\n",
            "Epoch [1053/6000], Train Loss: 0.2202, Test Loss: 0.1286\n",
            "Epoch [1054/6000], Train Loss: 0.2200, Test Loss: 0.1285\n",
            "Epoch [1055/6000], Train Loss: 0.2199, Test Loss: 0.1285\n",
            "Epoch [1056/6000], Train Loss: 0.2197, Test Loss: 0.1284\n",
            "Epoch [1057/6000], Train Loss: 0.2195, Test Loss: 0.1284\n",
            "Epoch [1058/6000], Train Loss: 0.2193, Test Loss: 0.1283\n",
            "Epoch [1059/6000], Train Loss: 0.2192, Test Loss: 0.1282\n",
            "Epoch [1060/6000], Train Loss: 0.2190, Test Loss: 0.1282\n",
            "Epoch [1061/6000], Train Loss: 0.2188, Test Loss: 0.1282\n",
            "Epoch [1062/6000], Train Loss: 0.2186, Test Loss: 0.1282\n",
            "Epoch [1063/6000], Train Loss: 0.2185, Test Loss: 0.1282\n",
            "Epoch [1064/6000], Train Loss: 0.2183, Test Loss: 0.1282\n",
            "Epoch [1065/6000], Train Loss: 0.2181, Test Loss: 0.1282\n",
            "Epoch [1066/6000], Train Loss: 0.2180, Test Loss: 0.1283\n",
            "Epoch [1067/6000], Train Loss: 0.2178, Test Loss: 0.1283\n",
            "Epoch [1068/6000], Train Loss: 0.2176, Test Loss: 0.1283\n",
            "Epoch [1069/6000], Train Loss: 0.2175, Test Loss: 0.1284\n",
            "Epoch [1070/6000], Train Loss: 0.2172, Test Loss: 0.1283\n",
            "Epoch [1071/6000], Train Loss: 0.2171, Test Loss: 0.1283\n",
            "Epoch [1072/6000], Train Loss: 0.2169, Test Loss: 0.1282\n",
            "Epoch [1073/6000], Train Loss: 0.2167, Test Loss: 0.1282\n",
            "Epoch [1074/6000], Train Loss: 0.2166, Test Loss: 0.1282\n",
            "Epoch [1075/6000], Train Loss: 0.2164, Test Loss: 0.1280\n",
            "Epoch [1076/6000], Train Loss: 0.2162, Test Loss: 0.1279\n",
            "Epoch [1077/6000], Train Loss: 0.2160, Test Loss: 0.1279\n",
            "Epoch [1078/6000], Train Loss: 0.2159, Test Loss: 0.1278\n",
            "Epoch [1079/6000], Train Loss: 0.2157, Test Loss: 0.1277\n",
            "Epoch [1080/6000], Train Loss: 0.2155, Test Loss: 0.1277\n",
            "Epoch [1081/6000], Train Loss: 0.2153, Test Loss: 0.1276\n",
            "Epoch [1082/6000], Train Loss: 0.2152, Test Loss: 0.1275\n",
            "Epoch [1083/6000], Train Loss: 0.2150, Test Loss: 0.1275\n",
            "Epoch [1084/6000], Train Loss: 0.2148, Test Loss: 0.1275\n",
            "Epoch [1085/6000], Train Loss: 0.2147, Test Loss: 0.1274\n",
            "Epoch [1086/6000], Train Loss: 0.2145, Test Loss: 0.1273\n",
            "Epoch [1087/6000], Train Loss: 0.2143, Test Loss: 0.1273\n",
            "Epoch [1088/6000], Train Loss: 0.2141, Test Loss: 0.1273\n",
            "Epoch [1089/6000], Train Loss: 0.2140, Test Loss: 0.1272\n",
            "Epoch [1090/6000], Train Loss: 0.2138, Test Loss: 0.1272\n",
            "Epoch [1091/6000], Train Loss: 0.2136, Test Loss: 0.1272\n",
            "Epoch [1092/6000], Train Loss: 0.2135, Test Loss: 0.1272\n",
            "Epoch [1093/6000], Train Loss: 0.2133, Test Loss: 0.1271\n",
            "Epoch [1094/6000], Train Loss: 0.2131, Test Loss: 0.1272\n",
            "Epoch [1095/6000], Train Loss: 0.2129, Test Loss: 0.1271\n",
            "Epoch [1096/6000], Train Loss: 0.2128, Test Loss: 0.1271\n",
            "Epoch [1097/6000], Train Loss: 0.2126, Test Loss: 0.1271\n",
            "Epoch [1098/6000], Train Loss: 0.2124, Test Loss: 0.1271\n",
            "Epoch [1099/6000], Train Loss: 0.2122, Test Loss: 0.1270\n",
            "Epoch [1100/6000], Train Loss: 0.2121, Test Loss: 0.1270\n",
            "Epoch [1101/6000], Train Loss: 0.2119, Test Loss: 0.1268\n",
            "Epoch [1102/6000], Train Loss: 0.2117, Test Loss: 0.1267\n",
            "Epoch [1103/6000], Train Loss: 0.2116, Test Loss: 0.1266\n",
            "Epoch [1104/6000], Train Loss: 0.2114, Test Loss: 0.1267\n",
            "Epoch [1105/6000], Train Loss: 0.2112, Test Loss: 0.1266\n",
            "Epoch [1106/6000], Train Loss: 0.2111, Test Loss: 0.1265\n",
            "Epoch [1107/6000], Train Loss: 0.2109, Test Loss: 0.1264\n",
            "Epoch [1108/6000], Train Loss: 0.2107, Test Loss: 0.1263\n",
            "Epoch [1109/6000], Train Loss: 0.2105, Test Loss: 0.1263\n",
            "Epoch [1110/6000], Train Loss: 0.2104, Test Loss: 0.1264\n",
            "Epoch [1111/6000], Train Loss: 0.2102, Test Loss: 0.1264\n",
            "Epoch [1112/6000], Train Loss: 0.2100, Test Loss: 0.1263\n",
            "Epoch [1113/6000], Train Loss: 0.2099, Test Loss: 0.1263\n",
            "Epoch [1114/6000], Train Loss: 0.2097, Test Loss: 0.1263\n",
            "Epoch [1115/6000], Train Loss: 0.2095, Test Loss: 0.1262\n",
            "Epoch [1116/6000], Train Loss: 0.2093, Test Loss: 0.1262\n",
            "Epoch [1117/6000], Train Loss: 0.2092, Test Loss: 0.1261\n",
            "Epoch [1118/6000], Train Loss: 0.2090, Test Loss: 0.1260\n",
            "Epoch [1119/6000], Train Loss: 0.2088, Test Loss: 0.1259\n",
            "Epoch [1120/6000], Train Loss: 0.2087, Test Loss: 0.1259\n",
            "Epoch [1121/6000], Train Loss: 0.2085, Test Loss: 0.1259\n",
            "Epoch [1122/6000], Train Loss: 0.2083, Test Loss: 0.1258\n",
            "Epoch [1123/6000], Train Loss: 0.2082, Test Loss: 0.1259\n",
            "Epoch [1124/6000], Train Loss: 0.2080, Test Loss: 0.1259\n",
            "Epoch [1125/6000], Train Loss: 0.2078, Test Loss: 0.1258\n",
            "Epoch [1126/6000], Train Loss: 0.2077, Test Loss: 0.1257\n",
            "Epoch [1127/6000], Train Loss: 0.2075, Test Loss: 0.1256\n",
            "Epoch [1128/6000], Train Loss: 0.2073, Test Loss: 0.1256\n",
            "Epoch [1129/6000], Train Loss: 0.2072, Test Loss: 0.1256\n",
            "Epoch [1130/6000], Train Loss: 0.2070, Test Loss: 0.1256\n",
            "Epoch [1131/6000], Train Loss: 0.2068, Test Loss: 0.1255\n",
            "Epoch [1132/6000], Train Loss: 0.2067, Test Loss: 0.1255\n",
            "Epoch [1133/6000], Train Loss: 0.2065, Test Loss: 0.1256\n",
            "Epoch [1134/6000], Train Loss: 0.2064, Test Loss: 0.1257\n",
            "Epoch [1135/6000], Train Loss: 0.2062, Test Loss: 0.1258\n",
            "Epoch [1136/6000], Train Loss: 0.2060, Test Loss: 0.1258\n",
            "Epoch [1137/6000], Train Loss: 0.2058, Test Loss: 0.1258\n",
            "Epoch [1138/6000], Train Loss: 0.2056, Test Loss: 0.1257\n",
            "Epoch [1139/6000], Train Loss: 0.2055, Test Loss: 0.1258\n",
            "Epoch [1140/6000], Train Loss: 0.2053, Test Loss: 0.1257\n",
            "Epoch [1141/6000], Train Loss: 0.2051, Test Loss: 0.1256\n",
            "Epoch [1142/6000], Train Loss: 0.2050, Test Loss: 0.1256\n",
            "Epoch [1143/6000], Train Loss: 0.2049, Test Loss: 0.1256\n",
            "Epoch [1144/6000], Train Loss: 0.2046, Test Loss: 0.1256\n",
            "Epoch [1145/6000], Train Loss: 0.2045, Test Loss: 0.1255\n",
            "Epoch [1146/6000], Train Loss: 0.2043, Test Loss: 0.1255\n",
            "Epoch [1147/6000], Train Loss: 0.2042, Test Loss: 0.1253\n",
            "Epoch [1148/6000], Train Loss: 0.2040, Test Loss: 0.1253\n",
            "Epoch [1149/6000], Train Loss: 0.2038, Test Loss: 0.1252\n",
            "Epoch [1150/6000], Train Loss: 0.2037, Test Loss: 0.1251\n",
            "Epoch [1151/6000], Train Loss: 0.2035, Test Loss: 0.1251\n",
            "Epoch [1152/6000], Train Loss: 0.2034, Test Loss: 0.1252\n",
            "Epoch [1153/6000], Train Loss: 0.2031, Test Loss: 0.1250\n",
            "Epoch [1154/6000], Train Loss: 0.2030, Test Loss: 0.1250\n",
            "Epoch [1155/6000], Train Loss: 0.2028, Test Loss: 0.1249\n",
            "Epoch [1156/6000], Train Loss: 0.2027, Test Loss: 0.1249\n",
            "Epoch [1157/6000], Train Loss: 0.2025, Test Loss: 0.1248\n",
            "Epoch [1158/6000], Train Loss: 0.2023, Test Loss: 0.1247\n",
            "Epoch [1159/6000], Train Loss: 0.2022, Test Loss: 0.1246\n",
            "Epoch [1160/6000], Train Loss: 0.2020, Test Loss: 0.1246\n",
            "Epoch [1161/6000], Train Loss: 0.2018, Test Loss: 0.1246\n",
            "Epoch [1162/6000], Train Loss: 0.2017, Test Loss: 0.1245\n",
            "Epoch [1163/6000], Train Loss: 0.2016, Test Loss: 0.1246\n",
            "Epoch [1164/6000], Train Loss: 0.2014, Test Loss: 0.1246\n",
            "Epoch [1165/6000], Train Loss: 0.2012, Test Loss: 0.1246\n",
            "Epoch [1166/6000], Train Loss: 0.2010, Test Loss: 0.1245\n",
            "Epoch [1167/6000], Train Loss: 0.2009, Test Loss: 0.1245\n",
            "Epoch [1168/6000], Train Loss: 0.2007, Test Loss: 0.1244\n",
            "Epoch [1169/6000], Train Loss: 0.2005, Test Loss: 0.1244\n",
            "Epoch [1170/6000], Train Loss: 0.2003, Test Loss: 0.1243\n",
            "Epoch [1171/6000], Train Loss: 0.2002, Test Loss: 0.1243\n",
            "Epoch [1172/6000], Train Loss: 0.2000, Test Loss: 0.1243\n",
            "Epoch [1173/6000], Train Loss: 0.1999, Test Loss: 0.1243\n",
            "Epoch [1174/6000], Train Loss: 0.1997, Test Loss: 0.1242\n",
            "Epoch [1175/6000], Train Loss: 0.1995, Test Loss: 0.1241\n",
            "Epoch [1176/6000], Train Loss: 0.1994, Test Loss: 0.1240\n",
            "Epoch [1177/6000], Train Loss: 0.1992, Test Loss: 0.1239\n",
            "Epoch [1178/6000], Train Loss: 0.1990, Test Loss: 0.1239\n",
            "Epoch [1179/6000], Train Loss: 0.1989, Test Loss: 0.1239\n",
            "Epoch [1180/6000], Train Loss: 0.1987, Test Loss: 0.1239\n",
            "Epoch [1181/6000], Train Loss: 0.1986, Test Loss: 0.1239\n",
            "Epoch [1182/6000], Train Loss: 0.1984, Test Loss: 0.1239\n",
            "Epoch [1183/6000], Train Loss: 0.1982, Test Loss: 0.1239\n",
            "Epoch [1184/6000], Train Loss: 0.1981, Test Loss: 0.1238\n",
            "Epoch [1185/6000], Train Loss: 0.1979, Test Loss: 0.1237\n",
            "Epoch [1186/6000], Train Loss: 0.1977, Test Loss: 0.1237\n",
            "Epoch [1187/6000], Train Loss: 0.1976, Test Loss: 0.1236\n",
            "Epoch [1188/6000], Train Loss: 0.1974, Test Loss: 0.1237\n",
            "Epoch [1189/6000], Train Loss: 0.1972, Test Loss: 0.1237\n",
            "Epoch [1190/6000], Train Loss: 0.1971, Test Loss: 0.1236\n",
            "Epoch [1191/6000], Train Loss: 0.1969, Test Loss: 0.1235\n",
            "Epoch [1192/6000], Train Loss: 0.1967, Test Loss: 0.1235\n",
            "Epoch [1193/6000], Train Loss: 0.1966, Test Loss: 0.1235\n",
            "Epoch [1194/6000], Train Loss: 0.1964, Test Loss: 0.1235\n",
            "Epoch [1195/6000], Train Loss: 0.1963, Test Loss: 0.1235\n",
            "Epoch [1196/6000], Train Loss: 0.1961, Test Loss: 0.1235\n",
            "Epoch [1197/6000], Train Loss: 0.1959, Test Loss: 0.1234\n",
            "Epoch [1198/6000], Train Loss: 0.1958, Test Loss: 0.1234\n",
            "Epoch [1199/6000], Train Loss: 0.1956, Test Loss: 0.1233\n",
            "Epoch [1200/6000], Train Loss: 0.1955, Test Loss: 0.1234\n",
            "Epoch [1201/6000], Train Loss: 0.1953, Test Loss: 0.1233\n",
            "Epoch [1202/6000], Train Loss: 0.1951, Test Loss: 0.1233\n",
            "Epoch [1203/6000], Train Loss: 0.1950, Test Loss: 0.1232\n",
            "Epoch [1204/6000], Train Loss: 0.1948, Test Loss: 0.1231\n",
            "Epoch [1205/6000], Train Loss: 0.1946, Test Loss: 0.1230\n",
            "Epoch [1206/6000], Train Loss: 0.1945, Test Loss: 0.1229\n",
            "Epoch [1207/6000], Train Loss: 0.1943, Test Loss: 0.1228\n",
            "Epoch [1208/6000], Train Loss: 0.1942, Test Loss: 0.1228\n",
            "Epoch [1209/6000], Train Loss: 0.1940, Test Loss: 0.1229\n",
            "Epoch [1210/6000], Train Loss: 0.1939, Test Loss: 0.1229\n",
            "Epoch [1211/6000], Train Loss: 0.1937, Test Loss: 0.1228\n",
            "Epoch [1212/6000], Train Loss: 0.1935, Test Loss: 0.1227\n",
            "Epoch [1213/6000], Train Loss: 0.1934, Test Loss: 0.1228\n",
            "Epoch [1214/6000], Train Loss: 0.1932, Test Loss: 0.1228\n",
            "Epoch [1215/6000], Train Loss: 0.1930, Test Loss: 0.1227\n",
            "Epoch [1216/6000], Train Loss: 0.1929, Test Loss: 0.1228\n",
            "Epoch [1217/6000], Train Loss: 0.1927, Test Loss: 0.1227\n",
            "Epoch [1218/6000], Train Loss: 0.1925, Test Loss: 0.1226\n",
            "Epoch [1219/6000], Train Loss: 0.1924, Test Loss: 0.1226\n",
            "Epoch [1220/6000], Train Loss: 0.1923, Test Loss: 0.1226\n",
            "Epoch [1221/6000], Train Loss: 0.1921, Test Loss: 0.1225\n",
            "Epoch [1222/6000], Train Loss: 0.1919, Test Loss: 0.1225\n",
            "Epoch [1223/6000], Train Loss: 0.1918, Test Loss: 0.1225\n",
            "Epoch [1224/6000], Train Loss: 0.1916, Test Loss: 0.1224\n",
            "Epoch [1225/6000], Train Loss: 0.1914, Test Loss: 0.1223\n",
            "Epoch [1226/6000], Train Loss: 0.1913, Test Loss: 0.1223\n",
            "Epoch [1227/6000], Train Loss: 0.1911, Test Loss: 0.1222\n",
            "Epoch [1228/6000], Train Loss: 0.1910, Test Loss: 0.1222\n",
            "Epoch [1229/6000], Train Loss: 0.1908, Test Loss: 0.1221\n",
            "Epoch [1230/6000], Train Loss: 0.1906, Test Loss: 0.1220\n",
            "Epoch [1231/6000], Train Loss: 0.1905, Test Loss: 0.1219\n",
            "Epoch [1232/6000], Train Loss: 0.1903, Test Loss: 0.1219\n",
            "Epoch [1233/6000], Train Loss: 0.1902, Test Loss: 0.1218\n",
            "Epoch [1234/6000], Train Loss: 0.1900, Test Loss: 0.1219\n",
            "Epoch [1235/6000], Train Loss: 0.1899, Test Loss: 0.1218\n",
            "Epoch [1236/6000], Train Loss: 0.1897, Test Loss: 0.1218\n",
            "Epoch [1237/6000], Train Loss: 0.1895, Test Loss: 0.1217\n",
            "Epoch [1238/6000], Train Loss: 0.1894, Test Loss: 0.1217\n",
            "Epoch [1239/6000], Train Loss: 0.1892, Test Loss: 0.1217\n",
            "Epoch [1240/6000], Train Loss: 0.1891, Test Loss: 0.1217\n",
            "Epoch [1241/6000], Train Loss: 0.1889, Test Loss: 0.1218\n",
            "Epoch [1242/6000], Train Loss: 0.1887, Test Loss: 0.1217\n",
            "Epoch [1243/6000], Train Loss: 0.1886, Test Loss: 0.1218\n",
            "Epoch [1244/6000], Train Loss: 0.1884, Test Loss: 0.1217\n",
            "Epoch [1245/6000], Train Loss: 0.1883, Test Loss: 0.1217\n",
            "Epoch [1246/6000], Train Loss: 0.1881, Test Loss: 0.1216\n",
            "Epoch [1247/6000], Train Loss: 0.1880, Test Loss: 0.1217\n",
            "Epoch [1248/6000], Train Loss: 0.1879, Test Loss: 0.1218\n",
            "Epoch [1249/6000], Train Loss: 0.1876, Test Loss: 0.1218\n",
            "Epoch [1250/6000], Train Loss: 0.1875, Test Loss: 0.1218\n",
            "Epoch [1251/6000], Train Loss: 0.1873, Test Loss: 0.1218\n",
            "Epoch [1252/6000], Train Loss: 0.1872, Test Loss: 0.1218\n",
            "Epoch [1253/6000], Train Loss: 0.1870, Test Loss: 0.1217\n",
            "Epoch [1254/6000], Train Loss: 0.1868, Test Loss: 0.1216\n",
            "Epoch [1255/6000], Train Loss: 0.1867, Test Loss: 0.1215\n",
            "Epoch [1256/6000], Train Loss: 0.1865, Test Loss: 0.1215\n",
            "Epoch [1257/6000], Train Loss: 0.1864, Test Loss: 0.1215\n",
            "Epoch [1258/6000], Train Loss: 0.1862, Test Loss: 0.1214\n",
            "Epoch [1259/6000], Train Loss: 0.1861, Test Loss: 0.1215\n",
            "Epoch [1260/6000], Train Loss: 0.1859, Test Loss: 0.1214\n",
            "Epoch [1261/6000], Train Loss: 0.1858, Test Loss: 0.1214\n",
            "Epoch [1262/6000], Train Loss: 0.1856, Test Loss: 0.1213\n",
            "Epoch [1263/6000], Train Loss: 0.1854, Test Loss: 0.1213\n",
            "Epoch [1264/6000], Train Loss: 0.1853, Test Loss: 0.1213\n",
            "Epoch [1265/6000], Train Loss: 0.1851, Test Loss: 0.1212\n",
            "Epoch [1266/6000], Train Loss: 0.1850, Test Loss: 0.1211\n",
            "Epoch [1267/6000], Train Loss: 0.1848, Test Loss: 0.1212\n",
            "Epoch [1268/6000], Train Loss: 0.1847, Test Loss: 0.1212\n",
            "Epoch [1269/6000], Train Loss: 0.1845, Test Loss: 0.1210\n",
            "Epoch [1270/6000], Train Loss: 0.1843, Test Loss: 0.1209\n",
            "Epoch [1271/6000], Train Loss: 0.1842, Test Loss: 0.1209\n",
            "Epoch [1272/6000], Train Loss: 0.1840, Test Loss: 0.1209\n",
            "Epoch [1273/6000], Train Loss: 0.1839, Test Loss: 0.1209\n",
            "Epoch [1274/6000], Train Loss: 0.1837, Test Loss: 0.1208\n",
            "Epoch [1275/6000], Train Loss: 0.1836, Test Loss: 0.1207\n",
            "Epoch [1276/6000], Train Loss: 0.1834, Test Loss: 0.1207\n",
            "Epoch [1277/6000], Train Loss: 0.1833, Test Loss: 0.1207\n",
            "Epoch [1278/6000], Train Loss: 0.1831, Test Loss: 0.1207\n",
            "Epoch [1279/6000], Train Loss: 0.1830, Test Loss: 0.1207\n",
            "Epoch [1280/6000], Train Loss: 0.1828, Test Loss: 0.1207\n",
            "Epoch [1281/6000], Train Loss: 0.1827, Test Loss: 0.1207\n",
            "Epoch [1282/6000], Train Loss: 0.1825, Test Loss: 0.1207\n",
            "Epoch [1283/6000], Train Loss: 0.1823, Test Loss: 0.1206\n",
            "Epoch [1284/6000], Train Loss: 0.1822, Test Loss: 0.1205\n",
            "Epoch [1285/6000], Train Loss: 0.1820, Test Loss: 0.1204\n",
            "Epoch [1286/6000], Train Loss: 0.1819, Test Loss: 0.1203\n",
            "Epoch [1287/6000], Train Loss: 0.1818, Test Loss: 0.1204\n",
            "Epoch [1288/6000], Train Loss: 0.1816, Test Loss: 0.1204\n",
            "Epoch [1289/6000], Train Loss: 0.1814, Test Loss: 0.1204\n",
            "Epoch [1290/6000], Train Loss: 0.1812, Test Loss: 0.1203\n",
            "Epoch [1291/6000], Train Loss: 0.1811, Test Loss: 0.1204\n",
            "Epoch [1292/6000], Train Loss: 0.1809, Test Loss: 0.1203\n",
            "Epoch [1293/6000], Train Loss: 0.1808, Test Loss: 0.1202\n",
            "Epoch [1294/6000], Train Loss: 0.1806, Test Loss: 0.1202\n",
            "Epoch [1295/6000], Train Loss: 0.1805, Test Loss: 0.1202\n",
            "Epoch [1296/6000], Train Loss: 0.1803, Test Loss: 0.1201\n",
            "Epoch [1297/6000], Train Loss: 0.1802, Test Loss: 0.1201\n",
            "Epoch [1298/6000], Train Loss: 0.1800, Test Loss: 0.1200\n",
            "Epoch [1299/6000], Train Loss: 0.1799, Test Loss: 0.1200\n",
            "Epoch [1300/6000], Train Loss: 0.1797, Test Loss: 0.1199\n",
            "Epoch [1301/6000], Train Loss: 0.1796, Test Loss: 0.1199\n",
            "Epoch [1302/6000], Train Loss: 0.1794, Test Loss: 0.1199\n",
            "Epoch [1303/6000], Train Loss: 0.1793, Test Loss: 0.1199\n",
            "Epoch [1304/6000], Train Loss: 0.1791, Test Loss: 0.1198\n",
            "Epoch [1305/6000], Train Loss: 0.1789, Test Loss: 0.1197\n",
            "Epoch [1306/6000], Train Loss: 0.1788, Test Loss: 0.1196\n",
            "Epoch [1307/6000], Train Loss: 0.1787, Test Loss: 0.1195\n",
            "Epoch [1308/6000], Train Loss: 0.1785, Test Loss: 0.1194\n",
            "Epoch [1309/6000], Train Loss: 0.1783, Test Loss: 0.1193\n",
            "Epoch [1310/6000], Train Loss: 0.1782, Test Loss: 0.1194\n",
            "Epoch [1311/6000], Train Loss: 0.1780, Test Loss: 0.1194\n",
            "Epoch [1312/6000], Train Loss: 0.1779, Test Loss: 0.1194\n",
            "Epoch [1313/6000], Train Loss: 0.1777, Test Loss: 0.1193\n",
            "Epoch [1314/6000], Train Loss: 0.1776, Test Loss: 0.1193\n",
            "Epoch [1315/6000], Train Loss: 0.1774, Test Loss: 0.1193\n",
            "Epoch [1316/6000], Train Loss: 0.1773, Test Loss: 0.1193\n",
            "Epoch [1317/6000], Train Loss: 0.1771, Test Loss: 0.1191\n",
            "Epoch [1318/6000], Train Loss: 0.1770, Test Loss: 0.1192\n",
            "Epoch [1319/6000], Train Loss: 0.1768, Test Loss: 0.1191\n",
            "Epoch [1320/6000], Train Loss: 0.1767, Test Loss: 0.1191\n",
            "Epoch [1321/6000], Train Loss: 0.1765, Test Loss: 0.1191\n",
            "Epoch [1322/6000], Train Loss: 0.1764, Test Loss: 0.1190\n",
            "Epoch [1323/6000], Train Loss: 0.1763, Test Loss: 0.1191\n",
            "Epoch [1324/6000], Train Loss: 0.1761, Test Loss: 0.1191\n",
            "Epoch [1325/6000], Train Loss: 0.1759, Test Loss: 0.1192\n",
            "Epoch [1326/6000], Train Loss: 0.1757, Test Loss: 0.1190\n",
            "Epoch [1327/6000], Train Loss: 0.1756, Test Loss: 0.1189\n",
            "Epoch [1328/6000], Train Loss: 0.1754, Test Loss: 0.1188\n",
            "Epoch [1329/6000], Train Loss: 0.1753, Test Loss: 0.1187\n",
            "Epoch [1330/6000], Train Loss: 0.1752, Test Loss: 0.1188\n",
            "Epoch [1331/6000], Train Loss: 0.1750, Test Loss: 0.1187\n",
            "Epoch [1332/6000], Train Loss: 0.1748, Test Loss: 0.1187\n",
            "Epoch [1333/6000], Train Loss: 0.1747, Test Loss: 0.1187\n",
            "Epoch [1334/6000], Train Loss: 0.1745, Test Loss: 0.1187\n",
            "Epoch [1335/6000], Train Loss: 0.1744, Test Loss: 0.1187\n",
            "Epoch [1336/6000], Train Loss: 0.1742, Test Loss: 0.1186\n",
            "Epoch [1337/6000], Train Loss: 0.1741, Test Loss: 0.1185\n",
            "Epoch [1338/6000], Train Loss: 0.1740, Test Loss: 0.1186\n",
            "Epoch [1339/6000], Train Loss: 0.1738, Test Loss: 0.1185\n",
            "Epoch [1340/6000], Train Loss: 0.1737, Test Loss: 0.1186\n",
            "Epoch [1341/6000], Train Loss: 0.1735, Test Loss: 0.1185\n",
            "Epoch [1342/6000], Train Loss: 0.1733, Test Loss: 0.1184\n",
            "Epoch [1343/6000], Train Loss: 0.1732, Test Loss: 0.1184\n",
            "Epoch [1344/6000], Train Loss: 0.1730, Test Loss: 0.1183\n",
            "Epoch [1345/6000], Train Loss: 0.1729, Test Loss: 0.1182\n",
            "Epoch [1346/6000], Train Loss: 0.1728, Test Loss: 0.1182\n",
            "Epoch [1347/6000], Train Loss: 0.1726, Test Loss: 0.1181\n",
            "Epoch [1348/6000], Train Loss: 0.1725, Test Loss: 0.1181\n",
            "Epoch [1349/6000], Train Loss: 0.1723, Test Loss: 0.1181\n",
            "Epoch [1350/6000], Train Loss: 0.1722, Test Loss: 0.1181\n",
            "Epoch [1351/6000], Train Loss: 0.1720, Test Loss: 0.1180\n",
            "Epoch [1352/6000], Train Loss: 0.1719, Test Loss: 0.1180\n",
            "Epoch [1353/6000], Train Loss: 0.1718, Test Loss: 0.1180\n",
            "Epoch [1354/6000], Train Loss: 0.1715, Test Loss: 0.1179\n",
            "Epoch [1355/6000], Train Loss: 0.1714, Test Loss: 0.1179\n",
            "Epoch [1356/6000], Train Loss: 0.1713, Test Loss: 0.1180\n",
            "Epoch [1357/6000], Train Loss: 0.1711, Test Loss: 0.1179\n",
            "Epoch [1358/6000], Train Loss: 0.1710, Test Loss: 0.1180\n",
            "Epoch [1359/6000], Train Loss: 0.1708, Test Loss: 0.1178\n",
            "Epoch [1360/6000], Train Loss: 0.1707, Test Loss: 0.1179\n",
            "Epoch [1361/6000], Train Loss: 0.1705, Test Loss: 0.1179\n",
            "Epoch [1362/6000], Train Loss: 0.1704, Test Loss: 0.1179\n",
            "Epoch [1363/6000], Train Loss: 0.1702, Test Loss: 0.1178\n",
            "Epoch [1364/6000], Train Loss: 0.1701, Test Loss: 0.1177\n",
            "Epoch [1365/6000], Train Loss: 0.1699, Test Loss: 0.1176\n",
            "Epoch [1366/6000], Train Loss: 0.1698, Test Loss: 0.1176\n",
            "Epoch [1367/6000], Train Loss: 0.1696, Test Loss: 0.1176\n",
            "Epoch [1368/6000], Train Loss: 0.1695, Test Loss: 0.1177\n",
            "Epoch [1369/6000], Train Loss: 0.1694, Test Loss: 0.1177\n",
            "Epoch [1370/6000], Train Loss: 0.1692, Test Loss: 0.1175\n",
            "Epoch [1371/6000], Train Loss: 0.1691, Test Loss: 0.1176\n",
            "Epoch [1372/6000], Train Loss: 0.1689, Test Loss: 0.1175\n",
            "Epoch [1373/6000], Train Loss: 0.1687, Test Loss: 0.1173\n",
            "Epoch [1374/6000], Train Loss: 0.1686, Test Loss: 0.1173\n",
            "Epoch [1375/6000], Train Loss: 0.1685, Test Loss: 0.1172\n",
            "Epoch [1376/6000], Train Loss: 0.1683, Test Loss: 0.1173\n",
            "Epoch [1377/6000], Train Loss: 0.1682, Test Loss: 0.1173\n",
            "Epoch [1378/6000], Train Loss: 0.1680, Test Loss: 0.1173\n",
            "Epoch [1379/6000], Train Loss: 0.1679, Test Loss: 0.1172\n",
            "Epoch [1380/6000], Train Loss: 0.1677, Test Loss: 0.1172\n",
            "Epoch [1381/6000], Train Loss: 0.1676, Test Loss: 0.1172\n",
            "Epoch [1382/6000], Train Loss: 0.1674, Test Loss: 0.1172\n",
            "Epoch [1383/6000], Train Loss: 0.1673, Test Loss: 0.1172\n",
            "Epoch [1384/6000], Train Loss: 0.1671, Test Loss: 0.1171\n",
            "Epoch [1385/6000], Train Loss: 0.1670, Test Loss: 0.1170\n",
            "Epoch [1386/6000], Train Loss: 0.1668, Test Loss: 0.1170\n",
            "Epoch [1387/6000], Train Loss: 0.1667, Test Loss: 0.1170\n",
            "Epoch [1388/6000], Train Loss: 0.1665, Test Loss: 0.1169\n",
            "Epoch [1389/6000], Train Loss: 0.1664, Test Loss: 0.1169\n",
            "Epoch [1390/6000], Train Loss: 0.1663, Test Loss: 0.1169\n",
            "Epoch [1391/6000], Train Loss: 0.1661, Test Loss: 0.1168\n",
            "Epoch [1392/6000], Train Loss: 0.1660, Test Loss: 0.1167\n",
            "Epoch [1393/6000], Train Loss: 0.1658, Test Loss: 0.1166\n",
            "Epoch [1394/6000], Train Loss: 0.1657, Test Loss: 0.1165\n",
            "Epoch [1395/6000], Train Loss: 0.1655, Test Loss: 0.1165\n",
            "Epoch [1396/6000], Train Loss: 0.1654, Test Loss: 0.1165\n",
            "Epoch [1397/6000], Train Loss: 0.1652, Test Loss: 0.1164\n",
            "Epoch [1398/6000], Train Loss: 0.1651, Test Loss: 0.1165\n",
            "Epoch [1399/6000], Train Loss: 0.1650, Test Loss: 0.1165\n",
            "Epoch [1400/6000], Train Loss: 0.1648, Test Loss: 0.1164\n",
            "Epoch [1401/6000], Train Loss: 0.1647, Test Loss: 0.1163\n",
            "Epoch [1402/6000], Train Loss: 0.1645, Test Loss: 0.1164\n",
            "Epoch [1403/6000], Train Loss: 0.1644, Test Loss: 0.1164\n",
            "Epoch [1404/6000], Train Loss: 0.1642, Test Loss: 0.1164\n",
            "Epoch [1405/6000], Train Loss: 0.1641, Test Loss: 0.1163\n",
            "Epoch [1406/6000], Train Loss: 0.1639, Test Loss: 0.1162\n",
            "Epoch [1407/6000], Train Loss: 0.1638, Test Loss: 0.1161\n",
            "Epoch [1408/6000], Train Loss: 0.1637, Test Loss: 0.1161\n",
            "Epoch [1409/6000], Train Loss: 0.1635, Test Loss: 0.1161\n",
            "Epoch [1410/6000], Train Loss: 0.1634, Test Loss: 0.1162\n",
            "Epoch [1411/6000], Train Loss: 0.1632, Test Loss: 0.1161\n",
            "Epoch [1412/6000], Train Loss: 0.1631, Test Loss: 0.1161\n",
            "Epoch [1413/6000], Train Loss: 0.1629, Test Loss: 0.1160\n",
            "Epoch [1414/6000], Train Loss: 0.1628, Test Loss: 0.1159\n",
            "Epoch [1415/6000], Train Loss: 0.1626, Test Loss: 0.1159\n",
            "Epoch [1416/6000], Train Loss: 0.1625, Test Loss: 0.1158\n",
            "Epoch [1417/6000], Train Loss: 0.1624, Test Loss: 0.1157\n",
            "Epoch [1418/6000], Train Loss: 0.1622, Test Loss: 0.1157\n",
            "Epoch [1419/6000], Train Loss: 0.1620, Test Loss: 0.1156\n",
            "Epoch [1420/6000], Train Loss: 0.1619, Test Loss: 0.1156\n",
            "Epoch [1421/6000], Train Loss: 0.1618, Test Loss: 0.1156\n",
            "Epoch [1422/6000], Train Loss: 0.1616, Test Loss: 0.1156\n",
            "Epoch [1423/6000], Train Loss: 0.1615, Test Loss: 0.1156\n",
            "Epoch [1424/6000], Train Loss: 0.1614, Test Loss: 0.1156\n",
            "Epoch [1425/6000], Train Loss: 0.1612, Test Loss: 0.1157\n",
            "Epoch [1426/6000], Train Loss: 0.1610, Test Loss: 0.1156\n",
            "Epoch [1427/6000], Train Loss: 0.1609, Test Loss: 0.1155\n",
            "Epoch [1428/6000], Train Loss: 0.1608, Test Loss: 0.1155\n",
            "Epoch [1429/6000], Train Loss: 0.1606, Test Loss: 0.1155\n",
            "Epoch [1430/6000], Train Loss: 0.1605, Test Loss: 0.1155\n",
            "Epoch [1431/6000], Train Loss: 0.1604, Test Loss: 0.1156\n",
            "Epoch [1432/6000], Train Loss: 0.1602, Test Loss: 0.1156\n",
            "Epoch [1433/6000], Train Loss: 0.1600, Test Loss: 0.1155\n",
            "Epoch [1434/6000], Train Loss: 0.1599, Test Loss: 0.1155\n",
            "Epoch [1435/6000], Train Loss: 0.1598, Test Loss: 0.1154\n",
            "Epoch [1436/6000], Train Loss: 0.1597, Test Loss: 0.1154\n",
            "Epoch [1437/6000], Train Loss: 0.1595, Test Loss: 0.1153\n",
            "Epoch [1438/6000], Train Loss: 0.1593, Test Loss: 0.1151\n",
            "Epoch [1439/6000], Train Loss: 0.1592, Test Loss: 0.1151\n",
            "Epoch [1440/6000], Train Loss: 0.1591, Test Loss: 0.1152\n",
            "Epoch [1441/6000], Train Loss: 0.1589, Test Loss: 0.1150\n",
            "Epoch [1442/6000], Train Loss: 0.1588, Test Loss: 0.1150\n",
            "Epoch [1443/6000], Train Loss: 0.1586, Test Loss: 0.1150\n",
            "Epoch [1444/6000], Train Loss: 0.1585, Test Loss: 0.1151\n",
            "Epoch [1445/6000], Train Loss: 0.1584, Test Loss: 0.1150\n",
            "Epoch [1446/6000], Train Loss: 0.1582, Test Loss: 0.1148\n",
            "Epoch [1447/6000], Train Loss: 0.1581, Test Loss: 0.1149\n",
            "Epoch [1448/6000], Train Loss: 0.1579, Test Loss: 0.1148\n",
            "Epoch [1449/6000], Train Loss: 0.1578, Test Loss: 0.1147\n",
            "Epoch [1450/6000], Train Loss: 0.1576, Test Loss: 0.1147\n",
            "Epoch [1451/6000], Train Loss: 0.1575, Test Loss: 0.1147\n",
            "Epoch [1452/6000], Train Loss: 0.1574, Test Loss: 0.1146\n",
            "Epoch [1453/6000], Train Loss: 0.1572, Test Loss: 0.1145\n",
            "Epoch [1454/6000], Train Loss: 0.1571, Test Loss: 0.1145\n",
            "Epoch [1455/6000], Train Loss: 0.1570, Test Loss: 0.1145\n",
            "Epoch [1456/6000], Train Loss: 0.1568, Test Loss: 0.1144\n",
            "Epoch [1457/6000], Train Loss: 0.1567, Test Loss: 0.1145\n",
            "Epoch [1458/6000], Train Loss: 0.1565, Test Loss: 0.1144\n",
            "Epoch [1459/6000], Train Loss: 0.1564, Test Loss: 0.1144\n",
            "Epoch [1460/6000], Train Loss: 0.1562, Test Loss: 0.1144\n",
            "Epoch [1461/6000], Train Loss: 0.1561, Test Loss: 0.1143\n",
            "Epoch [1462/6000], Train Loss: 0.1560, Test Loss: 0.1143\n",
            "Epoch [1463/6000], Train Loss: 0.1559, Test Loss: 0.1144\n",
            "Epoch [1464/6000], Train Loss: 0.1557, Test Loss: 0.1143\n",
            "Epoch [1465/6000], Train Loss: 0.1555, Test Loss: 0.1142\n",
            "Epoch [1466/6000], Train Loss: 0.1554, Test Loss: 0.1141\n",
            "Epoch [1467/6000], Train Loss: 0.1553, Test Loss: 0.1140\n",
            "Epoch [1468/6000], Train Loss: 0.1551, Test Loss: 0.1140\n",
            "Epoch [1469/6000], Train Loss: 0.1550, Test Loss: 0.1140\n",
            "Epoch [1470/6000], Train Loss: 0.1548, Test Loss: 0.1140\n",
            "Epoch [1471/6000], Train Loss: 0.1547, Test Loss: 0.1141\n",
            "Epoch [1472/6000], Train Loss: 0.1546, Test Loss: 0.1141\n",
            "Epoch [1473/6000], Train Loss: 0.1544, Test Loss: 0.1141\n",
            "Epoch [1474/6000], Train Loss: 0.1543, Test Loss: 0.1140\n",
            "Epoch [1475/6000], Train Loss: 0.1542, Test Loss: 0.1140\n",
            "Epoch [1476/6000], Train Loss: 0.1540, Test Loss: 0.1139\n",
            "Epoch [1477/6000], Train Loss: 0.1539, Test Loss: 0.1139\n",
            "Epoch [1478/6000], Train Loss: 0.1537, Test Loss: 0.1138\n",
            "Epoch [1479/6000], Train Loss: 0.1536, Test Loss: 0.1138\n",
            "Epoch [1480/6000], Train Loss: 0.1535, Test Loss: 0.1138\n",
            "Epoch [1481/6000], Train Loss: 0.1533, Test Loss: 0.1137\n",
            "Epoch [1482/6000], Train Loss: 0.1532, Test Loss: 0.1137\n",
            "Epoch [1483/6000], Train Loss: 0.1530, Test Loss: 0.1136\n",
            "Epoch [1484/6000], Train Loss: 0.1529, Test Loss: 0.1136\n",
            "Epoch [1485/6000], Train Loss: 0.1528, Test Loss: 0.1136\n",
            "Epoch [1486/6000], Train Loss: 0.1526, Test Loss: 0.1136\n",
            "Epoch [1487/6000], Train Loss: 0.1525, Test Loss: 0.1135\n",
            "Epoch [1488/6000], Train Loss: 0.1523, Test Loss: 0.1134\n",
            "Epoch [1489/6000], Train Loss: 0.1522, Test Loss: 0.1134\n",
            "Epoch [1490/6000], Train Loss: 0.1521, Test Loss: 0.1133\n",
            "Epoch [1491/6000], Train Loss: 0.1519, Test Loss: 0.1132\n",
            "Epoch [1492/6000], Train Loss: 0.1518, Test Loss: 0.1132\n",
            "Epoch [1493/6000], Train Loss: 0.1516, Test Loss: 0.1131\n",
            "Epoch [1494/6000], Train Loss: 0.1515, Test Loss: 0.1131\n",
            "Epoch [1495/6000], Train Loss: 0.1514, Test Loss: 0.1131\n",
            "Epoch [1496/6000], Train Loss: 0.1512, Test Loss: 0.1130\n",
            "Epoch [1497/6000], Train Loss: 0.1511, Test Loss: 0.1129\n",
            "Epoch [1498/6000], Train Loss: 0.1510, Test Loss: 0.1129\n",
            "Epoch [1499/6000], Train Loss: 0.1508, Test Loss: 0.1128\n",
            "Epoch [1500/6000], Train Loss: 0.1507, Test Loss: 0.1128\n",
            "Epoch [1501/6000], Train Loss: 0.1506, Test Loss: 0.1127\n",
            "Epoch [1502/6000], Train Loss: 0.1504, Test Loss: 0.1127\n",
            "Epoch [1503/6000], Train Loss: 0.1503, Test Loss: 0.1128\n",
            "Epoch [1504/6000], Train Loss: 0.1501, Test Loss: 0.1128\n",
            "Epoch [1505/6000], Train Loss: 0.1500, Test Loss: 0.1128\n",
            "Epoch [1506/6000], Train Loss: 0.1499, Test Loss: 0.1129\n",
            "Epoch [1507/6000], Train Loss: 0.1497, Test Loss: 0.1127\n",
            "Epoch [1508/6000], Train Loss: 0.1496, Test Loss: 0.1127\n",
            "Epoch [1509/6000], Train Loss: 0.1494, Test Loss: 0.1126\n",
            "Epoch [1510/6000], Train Loss: 0.1493, Test Loss: 0.1126\n",
            "Epoch [1511/6000], Train Loss: 0.1492, Test Loss: 0.1126\n",
            "Epoch [1512/6000], Train Loss: 0.1491, Test Loss: 0.1126\n",
            "Epoch [1513/6000], Train Loss: 0.1489, Test Loss: 0.1126\n",
            "Epoch [1514/6000], Train Loss: 0.1488, Test Loss: 0.1125\n",
            "Epoch [1515/6000], Train Loss: 0.1486, Test Loss: 0.1125\n",
            "Epoch [1516/6000], Train Loss: 0.1485, Test Loss: 0.1125\n",
            "Epoch [1517/6000], Train Loss: 0.1484, Test Loss: 0.1125\n",
            "Epoch [1518/6000], Train Loss: 0.1482, Test Loss: 0.1125\n",
            "Epoch [1519/6000], Train Loss: 0.1481, Test Loss: 0.1125\n",
            "Epoch [1520/6000], Train Loss: 0.1480, Test Loss: 0.1125\n",
            "Epoch [1521/6000], Train Loss: 0.1478, Test Loss: 0.1124\n",
            "Epoch [1522/6000], Train Loss: 0.1477, Test Loss: 0.1124\n",
            "Epoch [1523/6000], Train Loss: 0.1475, Test Loss: 0.1123\n",
            "Epoch [1524/6000], Train Loss: 0.1474, Test Loss: 0.1123\n",
            "Epoch [1525/6000], Train Loss: 0.1473, Test Loss: 0.1122\n",
            "Epoch [1526/6000], Train Loss: 0.1471, Test Loss: 0.1121\n",
            "Epoch [1527/6000], Train Loss: 0.1470, Test Loss: 0.1120\n",
            "Epoch [1528/6000], Train Loss: 0.1469, Test Loss: 0.1121\n",
            "Epoch [1529/6000], Train Loss: 0.1467, Test Loss: 0.1121\n",
            "Epoch [1530/6000], Train Loss: 0.1466, Test Loss: 0.1120\n",
            "Epoch [1531/6000], Train Loss: 0.1465, Test Loss: 0.1121\n",
            "Epoch [1532/6000], Train Loss: 0.1463, Test Loss: 0.1120\n",
            "Epoch [1533/6000], Train Loss: 0.1462, Test Loss: 0.1120\n",
            "Epoch [1534/6000], Train Loss: 0.1461, Test Loss: 0.1119\n",
            "Epoch [1535/6000], Train Loss: 0.1459, Test Loss: 0.1119\n",
            "Epoch [1536/6000], Train Loss: 0.1458, Test Loss: 0.1119\n",
            "Epoch [1537/6000], Train Loss: 0.1457, Test Loss: 0.1117\n",
            "Epoch [1538/6000], Train Loss: 0.1455, Test Loss: 0.1117\n",
            "Epoch [1539/6000], Train Loss: 0.1454, Test Loss: 0.1117\n",
            "Epoch [1540/6000], Train Loss: 0.1453, Test Loss: 0.1117\n",
            "Epoch [1541/6000], Train Loss: 0.1451, Test Loss: 0.1116\n",
            "Epoch [1542/6000], Train Loss: 0.1450, Test Loss: 0.1115\n",
            "Epoch [1543/6000], Train Loss: 0.1449, Test Loss: 0.1115\n",
            "Epoch [1544/6000], Train Loss: 0.1448, Test Loss: 0.1116\n",
            "Epoch [1545/6000], Train Loss: 0.1446, Test Loss: 0.1115\n",
            "Epoch [1546/6000], Train Loss: 0.1445, Test Loss: 0.1115\n",
            "Epoch [1547/6000], Train Loss: 0.1444, Test Loss: 0.1116\n",
            "Epoch [1548/6000], Train Loss: 0.1442, Test Loss: 0.1115\n",
            "Epoch [1549/6000], Train Loss: 0.1441, Test Loss: 0.1116\n",
            "Epoch [1550/6000], Train Loss: 0.1439, Test Loss: 0.1116\n",
            "Epoch [1551/6000], Train Loss: 0.1438, Test Loss: 0.1115\n",
            "Epoch [1552/6000], Train Loss: 0.1437, Test Loss: 0.1113\n",
            "Epoch [1553/6000], Train Loss: 0.1435, Test Loss: 0.1113\n",
            "Epoch [1554/6000], Train Loss: 0.1434, Test Loss: 0.1112\n",
            "Epoch [1555/6000], Train Loss: 0.1432, Test Loss: 0.1111\n",
            "Epoch [1556/6000], Train Loss: 0.1431, Test Loss: 0.1111\n",
            "Epoch [1557/6000], Train Loss: 0.1430, Test Loss: 0.1111\n",
            "Epoch [1558/6000], Train Loss: 0.1429, Test Loss: 0.1112\n",
            "Epoch [1559/6000], Train Loss: 0.1428, Test Loss: 0.1112\n",
            "Epoch [1560/6000], Train Loss: 0.1426, Test Loss: 0.1111\n",
            "Epoch [1561/6000], Train Loss: 0.1425, Test Loss: 0.1111\n",
            "Epoch [1562/6000], Train Loss: 0.1424, Test Loss: 0.1111\n",
            "Epoch [1563/6000], Train Loss: 0.1422, Test Loss: 0.1110\n",
            "Epoch [1564/6000], Train Loss: 0.1421, Test Loss: 0.1109\n",
            "Epoch [1565/6000], Train Loss: 0.1419, Test Loss: 0.1108\n",
            "Epoch [1566/6000], Train Loss: 0.1418, Test Loss: 0.1108\n",
            "Epoch [1567/6000], Train Loss: 0.1417, Test Loss: 0.1108\n",
            "Epoch [1568/6000], Train Loss: 0.1415, Test Loss: 0.1108\n",
            "Epoch [1569/6000], Train Loss: 0.1414, Test Loss: 0.1108\n",
            "Epoch [1570/6000], Train Loss: 0.1413, Test Loss: 0.1106\n",
            "Epoch [1571/6000], Train Loss: 0.1411, Test Loss: 0.1106\n",
            "Epoch [1572/6000], Train Loss: 0.1410, Test Loss: 0.1105\n",
            "Epoch [1573/6000], Train Loss: 0.1409, Test Loss: 0.1105\n",
            "Epoch [1574/6000], Train Loss: 0.1407, Test Loss: 0.1105\n",
            "Epoch [1575/6000], Train Loss: 0.1406, Test Loss: 0.1105\n",
            "Epoch [1576/6000], Train Loss: 0.1405, Test Loss: 0.1104\n",
            "Epoch [1577/6000], Train Loss: 0.1403, Test Loss: 0.1103\n",
            "Epoch [1578/6000], Train Loss: 0.1402, Test Loss: 0.1102\n",
            "Epoch [1579/6000], Train Loss: 0.1401, Test Loss: 0.1102\n",
            "Epoch [1580/6000], Train Loss: 0.1399, Test Loss: 0.1101\n",
            "Epoch [1581/6000], Train Loss: 0.1398, Test Loss: 0.1101\n",
            "Epoch [1582/6000], Train Loss: 0.1397, Test Loss: 0.1101\n",
            "Epoch [1583/6000], Train Loss: 0.1396, Test Loss: 0.1101\n",
            "Epoch [1584/6000], Train Loss: 0.1394, Test Loss: 0.1101\n",
            "Epoch [1585/6000], Train Loss: 0.1393, Test Loss: 0.1101\n",
            "Epoch [1586/6000], Train Loss: 0.1392, Test Loss: 0.1102\n",
            "Epoch [1587/6000], Train Loss: 0.1390, Test Loss: 0.1101\n",
            "Epoch [1588/6000], Train Loss: 0.1389, Test Loss: 0.1101\n",
            "Epoch [1589/6000], Train Loss: 0.1388, Test Loss: 0.1099\n",
            "Epoch [1590/6000], Train Loss: 0.1387, Test Loss: 0.1100\n",
            "Epoch [1591/6000], Train Loss: 0.1385, Test Loss: 0.1099\n",
            "Epoch [1592/6000], Train Loss: 0.1384, Test Loss: 0.1098\n",
            "Epoch [1593/6000], Train Loss: 0.1382, Test Loss: 0.1098\n",
            "Epoch [1594/6000], Train Loss: 0.1382, Test Loss: 0.1098\n",
            "Epoch [1595/6000], Train Loss: 0.1380, Test Loss: 0.1099\n",
            "Epoch [1596/6000], Train Loss: 0.1378, Test Loss: 0.1098\n",
            "Epoch [1597/6000], Train Loss: 0.1378, Test Loss: 0.1098\n",
            "Epoch [1598/6000], Train Loss: 0.1376, Test Loss: 0.1098\n",
            "Epoch [1599/6000], Train Loss: 0.1375, Test Loss: 0.1098\n",
            "Epoch [1600/6000], Train Loss: 0.1373, Test Loss: 0.1097\n",
            "Epoch [1601/6000], Train Loss: 0.1372, Test Loss: 0.1095\n",
            "Epoch [1602/6000], Train Loss: 0.1371, Test Loss: 0.1094\n",
            "Epoch [1603/6000], Train Loss: 0.1370, Test Loss: 0.1094\n",
            "Epoch [1604/6000], Train Loss: 0.1368, Test Loss: 0.1094\n",
            "Epoch [1605/6000], Train Loss: 0.1367, Test Loss: 0.1093\n",
            "Epoch [1606/6000], Train Loss: 0.1366, Test Loss: 0.1093\n",
            "Epoch [1607/6000], Train Loss: 0.1364, Test Loss: 0.1093\n",
            "Epoch [1608/6000], Train Loss: 0.1363, Test Loss: 0.1092\n",
            "Epoch [1609/6000], Train Loss: 0.1362, Test Loss: 0.1091\n",
            "Epoch [1610/6000], Train Loss: 0.1361, Test Loss: 0.1090\n",
            "Epoch [1611/6000], Train Loss: 0.1359, Test Loss: 0.1089\n",
            "Epoch [1612/6000], Train Loss: 0.1358, Test Loss: 0.1090\n",
            "Epoch [1613/6000], Train Loss: 0.1357, Test Loss: 0.1091\n",
            "Epoch [1614/6000], Train Loss: 0.1355, Test Loss: 0.1090\n",
            "Epoch [1615/6000], Train Loss: 0.1354, Test Loss: 0.1090\n",
            "Epoch [1616/6000], Train Loss: 0.1353, Test Loss: 0.1091\n",
            "Epoch [1617/6000], Train Loss: 0.1351, Test Loss: 0.1091\n",
            "Epoch [1618/6000], Train Loss: 0.1350, Test Loss: 0.1091\n",
            "Epoch [1619/6000], Train Loss: 0.1349, Test Loss: 0.1091\n",
            "Epoch [1620/6000], Train Loss: 0.1348, Test Loss: 0.1090\n",
            "Epoch [1621/6000], Train Loss: 0.1346, Test Loss: 0.1090\n",
            "Epoch [1622/6000], Train Loss: 0.1345, Test Loss: 0.1090\n",
            "Epoch [1623/6000], Train Loss: 0.1344, Test Loss: 0.1090\n",
            "Epoch [1624/6000], Train Loss: 0.1343, Test Loss: 0.1090\n",
            "Epoch [1625/6000], Train Loss: 0.1341, Test Loss: 0.1090\n",
            "Epoch [1626/6000], Train Loss: 0.1340, Test Loss: 0.1090\n",
            "Epoch [1627/6000], Train Loss: 0.1339, Test Loss: 0.1088\n",
            "Epoch [1628/6000], Train Loss: 0.1338, Test Loss: 0.1085\n",
            "Epoch [1629/6000], Train Loss: 0.1337, Test Loss: 0.1084\n",
            "Epoch [1630/6000], Train Loss: 0.1335, Test Loss: 0.1081\n",
            "Epoch [1631/6000], Train Loss: 0.1334, Test Loss: 0.1081\n",
            "Epoch [1632/6000], Train Loss: 0.1333, Test Loss: 0.1078\n",
            "Epoch [1633/6000], Train Loss: 0.1332, Test Loss: 0.1076\n",
            "Epoch [1634/6000], Train Loss: 0.1331, Test Loss: 0.1075\n",
            "Epoch [1635/6000], Train Loss: 0.1330, Test Loss: 0.1074\n",
            "Epoch [1636/6000], Train Loss: 0.1328, Test Loss: 0.1072\n",
            "Epoch [1637/6000], Train Loss: 0.1327, Test Loss: 0.1071\n",
            "Epoch [1638/6000], Train Loss: 0.1326, Test Loss: 0.1070\n",
            "Epoch [1639/6000], Train Loss: 0.1325, Test Loss: 0.1069\n",
            "Epoch [1640/6000], Train Loss: 0.1324, Test Loss: 0.1067\n",
            "Epoch [1641/6000], Train Loss: 0.1323, Test Loss: 0.1067\n",
            "Epoch [1642/6000], Train Loss: 0.1322, Test Loss: 0.1065\n",
            "Epoch [1643/6000], Train Loss: 0.1320, Test Loss: 0.1063\n",
            "Epoch [1644/6000], Train Loss: 0.1319, Test Loss: 0.1062\n",
            "Epoch [1645/6000], Train Loss: 0.1318, Test Loss: 0.1062\n",
            "Epoch [1646/6000], Train Loss: 0.1317, Test Loss: 0.1061\n",
            "Epoch [1647/6000], Train Loss: 0.1316, Test Loss: 0.1059\n",
            "Epoch [1648/6000], Train Loss: 0.1315, Test Loss: 0.1058\n",
            "Epoch [1649/6000], Train Loss: 0.1314, Test Loss: 0.1057\n",
            "Epoch [1650/6000], Train Loss: 0.1313, Test Loss: 0.1057\n",
            "Epoch [1651/6000], Train Loss: 0.1311, Test Loss: 0.1055\n",
            "Epoch [1652/6000], Train Loss: 0.1310, Test Loss: 0.1054\n",
            "Epoch [1653/6000], Train Loss: 0.1309, Test Loss: 0.1053\n",
            "Epoch [1654/6000], Train Loss: 0.1308, Test Loss: 0.1051\n",
            "Epoch [1655/6000], Train Loss: 0.1307, Test Loss: 0.1049\n",
            "Epoch [1656/6000], Train Loss: 0.1306, Test Loss: 0.1047\n",
            "Epoch [1657/6000], Train Loss: 0.1305, Test Loss: 0.1046\n",
            "Epoch [1658/6000], Train Loss: 0.1304, Test Loss: 0.1044\n",
            "Epoch [1659/6000], Train Loss: 0.1302, Test Loss: 0.1043\n",
            "Epoch [1660/6000], Train Loss: 0.1301, Test Loss: 0.1043\n",
            "Epoch [1661/6000], Train Loss: 0.1300, Test Loss: 0.1044\n",
            "Epoch [1662/6000], Train Loss: 0.1299, Test Loss: 0.1042\n",
            "Epoch [1663/6000], Train Loss: 0.1298, Test Loss: 0.1041\n",
            "Epoch [1664/6000], Train Loss: 0.1297, Test Loss: 0.1040\n",
            "Epoch [1665/6000], Train Loss: 0.1296, Test Loss: 0.1038\n",
            "Epoch [1666/6000], Train Loss: 0.1295, Test Loss: 0.1036\n",
            "Epoch [1667/6000], Train Loss: 0.1294, Test Loss: 0.1036\n",
            "Epoch [1668/6000], Train Loss: 0.1293, Test Loss: 0.1036\n",
            "Epoch [1669/6000], Train Loss: 0.1291, Test Loss: 0.1035\n",
            "Epoch [1670/6000], Train Loss: 0.1290, Test Loss: 0.1033\n",
            "Epoch [1671/6000], Train Loss: 0.1289, Test Loss: 0.1032\n",
            "Epoch [1672/6000], Train Loss: 0.1288, Test Loss: 0.1031\n",
            "Epoch [1673/6000], Train Loss: 0.1287, Test Loss: 0.1031\n",
            "Epoch [1674/6000], Train Loss: 0.1286, Test Loss: 0.1031\n",
            "Epoch [1675/6000], Train Loss: 0.1285, Test Loss: 0.1030\n",
            "Epoch [1676/6000], Train Loss: 0.1283, Test Loss: 0.1028\n",
            "Epoch [1677/6000], Train Loss: 0.1282, Test Loss: 0.1026\n",
            "Epoch [1678/6000], Train Loss: 0.1281, Test Loss: 0.1025\n",
            "Epoch [1679/6000], Train Loss: 0.1281, Test Loss: 0.1025\n",
            "Epoch [1680/6000], Train Loss: 0.1279, Test Loss: 0.1025\n",
            "Epoch [1681/6000], Train Loss: 0.1278, Test Loss: 0.1024\n",
            "Epoch [1682/6000], Train Loss: 0.1277, Test Loss: 0.1023\n",
            "Epoch [1683/6000], Train Loss: 0.1276, Test Loss: 0.1022\n",
            "Epoch [1684/6000], Train Loss: 0.1275, Test Loss: 0.1021\n",
            "Epoch [1685/6000], Train Loss: 0.1274, Test Loss: 0.1020\n",
            "Epoch [1686/6000], Train Loss: 0.1272, Test Loss: 0.1019\n",
            "Epoch [1687/6000], Train Loss: 0.1271, Test Loss: 0.1018\n",
            "Epoch [1688/6000], Train Loss: 0.1270, Test Loss: 0.1016\n",
            "Epoch [1689/6000], Train Loss: 0.1270, Test Loss: 0.1017\n",
            "Epoch [1690/6000], Train Loss: 0.1269, Test Loss: 0.1017\n",
            "Epoch [1691/6000], Train Loss: 0.1267, Test Loss: 0.1016\n",
            "Epoch [1692/6000], Train Loss: 0.1266, Test Loss: 0.1014\n",
            "Epoch [1693/6000], Train Loss: 0.1265, Test Loss: 0.1013\n",
            "Epoch [1694/6000], Train Loss: 0.1264, Test Loss: 0.1011\n",
            "Epoch [1695/6000], Train Loss: 0.1263, Test Loss: 0.1010\n",
            "Epoch [1696/6000], Train Loss: 0.1262, Test Loss: 0.1009\n",
            "Epoch [1697/6000], Train Loss: 0.1261, Test Loss: 0.1009\n",
            "Epoch [1698/6000], Train Loss: 0.1260, Test Loss: 0.1009\n",
            "Epoch [1699/6000], Train Loss: 0.1258, Test Loss: 0.1008\n",
            "Epoch [1700/6000], Train Loss: 0.1257, Test Loss: 0.1007\n",
            "Epoch [1701/6000], Train Loss: 0.1256, Test Loss: 0.1006\n",
            "Epoch [1702/6000], Train Loss: 0.1255, Test Loss: 0.1004\n",
            "Epoch [1703/6000], Train Loss: 0.1254, Test Loss: 0.1003\n",
            "Epoch [1704/6000], Train Loss: 0.1253, Test Loss: 0.1003\n",
            "Epoch [1705/6000], Train Loss: 0.1252, Test Loss: 0.1002\n",
            "Epoch [1706/6000], Train Loss: 0.1251, Test Loss: 0.1001\n",
            "Epoch [1707/6000], Train Loss: 0.1250, Test Loss: 0.1000\n",
            "Epoch [1708/6000], Train Loss: 0.1249, Test Loss: 0.1000\n",
            "Epoch [1709/6000], Train Loss: 0.1248, Test Loss: 0.0999\n",
            "Epoch [1710/6000], Train Loss: 0.1247, Test Loss: 0.0999\n",
            "Epoch [1711/6000], Train Loss: 0.1246, Test Loss: 0.0999\n",
            "Epoch [1712/6000], Train Loss: 0.1244, Test Loss: 0.0998\n",
            "Epoch [1713/6000], Train Loss: 0.1244, Test Loss: 0.0997\n",
            "Epoch [1714/6000], Train Loss: 0.1243, Test Loss: 0.0997\n",
            "Epoch [1715/6000], Train Loss: 0.1241, Test Loss: 0.0996\n",
            "Epoch [1716/6000], Train Loss: 0.1240, Test Loss: 0.0995\n",
            "Epoch [1717/6000], Train Loss: 0.1239, Test Loss: 0.0993\n",
            "Epoch [1718/6000], Train Loss: 0.1238, Test Loss: 0.0992\n",
            "Epoch [1719/6000], Train Loss: 0.1237, Test Loss: 0.0991\n",
            "Epoch [1720/6000], Train Loss: 0.1236, Test Loss: 0.0991\n",
            "Epoch [1721/6000], Train Loss: 0.1235, Test Loss: 0.0991\n",
            "Epoch [1722/6000], Train Loss: 0.1234, Test Loss: 0.0990\n",
            "Epoch [1723/6000], Train Loss: 0.1233, Test Loss: 0.0989\n",
            "Epoch [1724/6000], Train Loss: 0.1232, Test Loss: 0.0989\n",
            "Epoch [1725/6000], Train Loss: 0.1231, Test Loss: 0.0989\n",
            "Epoch [1726/6000], Train Loss: 0.1230, Test Loss: 0.0988\n",
            "Epoch [1727/6000], Train Loss: 0.1229, Test Loss: 0.0988\n",
            "Epoch [1728/6000], Train Loss: 0.1228, Test Loss: 0.0987\n",
            "Epoch [1729/6000], Train Loss: 0.1227, Test Loss: 0.0986\n",
            "Epoch [1730/6000], Train Loss: 0.1226, Test Loss: 0.0985\n",
            "Epoch [1731/6000], Train Loss: 0.1224, Test Loss: 0.0984\n",
            "Epoch [1732/6000], Train Loss: 0.1224, Test Loss: 0.0985\n",
            "Epoch [1733/6000], Train Loss: 0.1223, Test Loss: 0.0984\n",
            "Epoch [1734/6000], Train Loss: 0.1221, Test Loss: 0.0983\n",
            "Epoch [1735/6000], Train Loss: 0.1221, Test Loss: 0.0983\n",
            "Epoch [1736/6000], Train Loss: 0.1219, Test Loss: 0.0981\n",
            "Epoch [1737/6000], Train Loss: 0.1218, Test Loss: 0.0980\n",
            "Epoch [1738/6000], Train Loss: 0.1217, Test Loss: 0.0978\n",
            "Epoch [1739/6000], Train Loss: 0.1216, Test Loss: 0.0978\n",
            "Epoch [1740/6000], Train Loss: 0.1216, Test Loss: 0.0978\n",
            "Epoch [1741/6000], Train Loss: 0.1214, Test Loss: 0.0979\n",
            "Epoch [1742/6000], Train Loss: 0.1213, Test Loss: 0.0979\n",
            "Epoch [1743/6000], Train Loss: 0.1212, Test Loss: 0.0978\n",
            "Epoch [1744/6000], Train Loss: 0.1211, Test Loss: 0.0977\n",
            "Epoch [1745/6000], Train Loss: 0.1210, Test Loss: 0.0976\n",
            "Epoch [1746/6000], Train Loss: 0.1209, Test Loss: 0.0975\n",
            "Epoch [1747/6000], Train Loss: 0.1208, Test Loss: 0.0975\n",
            "Epoch [1748/6000], Train Loss: 0.1207, Test Loss: 0.0974\n",
            "Epoch [1749/6000], Train Loss: 0.1206, Test Loss: 0.0974\n",
            "Epoch [1750/6000], Train Loss: 0.1205, Test Loss: 0.0974\n",
            "Epoch [1751/6000], Train Loss: 0.1204, Test Loss: 0.0973\n",
            "Epoch [1752/6000], Train Loss: 0.1203, Test Loss: 0.0973\n",
            "Epoch [1753/6000], Train Loss: 0.1202, Test Loss: 0.0971\n",
            "Epoch [1754/6000], Train Loss: 0.1201, Test Loss: 0.0970\n",
            "Epoch [1755/6000], Train Loss: 0.1200, Test Loss: 0.0969\n",
            "Epoch [1756/6000], Train Loss: 0.1199, Test Loss: 0.0970\n",
            "Epoch [1757/6000], Train Loss: 0.1198, Test Loss: 0.0970\n",
            "Epoch [1758/6000], Train Loss: 0.1197, Test Loss: 0.0969\n",
            "Epoch [1759/6000], Train Loss: 0.1196, Test Loss: 0.0969\n",
            "Epoch [1760/6000], Train Loss: 0.1195, Test Loss: 0.0967\n",
            "Epoch [1761/6000], Train Loss: 0.1194, Test Loss: 0.0967\n",
            "Epoch [1762/6000], Train Loss: 0.1193, Test Loss: 0.0966\n",
            "Epoch [1763/6000], Train Loss: 0.1192, Test Loss: 0.0967\n",
            "Epoch [1764/6000], Train Loss: 0.1190, Test Loss: 0.0967\n",
            "Epoch [1765/6000], Train Loss: 0.1190, Test Loss: 0.0966\n",
            "Epoch [1766/6000], Train Loss: 0.1188, Test Loss: 0.0965\n",
            "Epoch [1767/6000], Train Loss: 0.1188, Test Loss: 0.0964\n",
            "Epoch [1768/6000], Train Loss: 0.1187, Test Loss: 0.0962\n",
            "Epoch [1769/6000], Train Loss: 0.1186, Test Loss: 0.0963\n",
            "Epoch [1770/6000], Train Loss: 0.1184, Test Loss: 0.0962\n",
            "Epoch [1771/6000], Train Loss: 0.1183, Test Loss: 0.0962\n",
            "Epoch [1772/6000], Train Loss: 0.1182, Test Loss: 0.0961\n",
            "Epoch [1773/6000], Train Loss: 0.1181, Test Loss: 0.0960\n",
            "Epoch [1774/6000], Train Loss: 0.1180, Test Loss: 0.0960\n",
            "Epoch [1775/6000], Train Loss: 0.1180, Test Loss: 0.0960\n",
            "Epoch [1776/6000], Train Loss: 0.1178, Test Loss: 0.0959\n",
            "Epoch [1777/6000], Train Loss: 0.1177, Test Loss: 0.0958\n",
            "Epoch [1778/6000], Train Loss: 0.1176, Test Loss: 0.0958\n",
            "Epoch [1779/6000], Train Loss: 0.1175, Test Loss: 0.0957\n",
            "Epoch [1780/6000], Train Loss: 0.1174, Test Loss: 0.0956\n",
            "Epoch [1781/6000], Train Loss: 0.1173, Test Loss: 0.0955\n",
            "Epoch [1782/6000], Train Loss: 0.1172, Test Loss: 0.0954\n",
            "Epoch [1783/6000], Train Loss: 0.1171, Test Loss: 0.0954\n",
            "Epoch [1784/6000], Train Loss: 0.1170, Test Loss: 0.0954\n",
            "Epoch [1785/6000], Train Loss: 0.1169, Test Loss: 0.0953\n",
            "Epoch [1786/6000], Train Loss: 0.1169, Test Loss: 0.0954\n",
            "Epoch [1787/6000], Train Loss: 0.1167, Test Loss: 0.0953\n",
            "Epoch [1788/6000], Train Loss: 0.1166, Test Loss: 0.0952\n",
            "Epoch [1789/6000], Train Loss: 0.1165, Test Loss: 0.0951\n",
            "Epoch [1790/6000], Train Loss: 0.1164, Test Loss: 0.0951\n",
            "Epoch [1791/6000], Train Loss: 0.1163, Test Loss: 0.0951\n",
            "Epoch [1792/6000], Train Loss: 0.1163, Test Loss: 0.0951\n",
            "Epoch [1793/6000], Train Loss: 0.1161, Test Loss: 0.0951\n",
            "Epoch [1794/6000], Train Loss: 0.1160, Test Loss: 0.0950\n",
            "Epoch [1795/6000], Train Loss: 0.1159, Test Loss: 0.0950\n",
            "Epoch [1796/6000], Train Loss: 0.1158, Test Loss: 0.0950\n",
            "Epoch [1797/6000], Train Loss: 0.1157, Test Loss: 0.0950\n",
            "Epoch [1798/6000], Train Loss: 0.1156, Test Loss: 0.0948\n",
            "Epoch [1799/6000], Train Loss: 0.1155, Test Loss: 0.0947\n",
            "Epoch [1800/6000], Train Loss: 0.1154, Test Loss: 0.0946\n",
            "Epoch [1801/6000], Train Loss: 0.1153, Test Loss: 0.0946\n",
            "Epoch [1802/6000], Train Loss: 0.1153, Test Loss: 0.0947\n",
            "Epoch [1803/6000], Train Loss: 0.1151, Test Loss: 0.0946\n",
            "Epoch [1804/6000], Train Loss: 0.1150, Test Loss: 0.0946\n",
            "Epoch [1805/6000], Train Loss: 0.1149, Test Loss: 0.0945\n",
            "Epoch [1806/6000], Train Loss: 0.1148, Test Loss: 0.0944\n",
            "Epoch [1807/6000], Train Loss: 0.1148, Test Loss: 0.0944\n",
            "Epoch [1808/6000], Train Loss: 0.1146, Test Loss: 0.0943\n",
            "Epoch [1809/6000], Train Loss: 0.1145, Test Loss: 0.0942\n",
            "Epoch [1810/6000], Train Loss: 0.1145, Test Loss: 0.0943\n",
            "Epoch [1811/6000], Train Loss: 0.1144, Test Loss: 0.0943\n",
            "Epoch [1812/6000], Train Loss: 0.1143, Test Loss: 0.0941\n",
            "Epoch [1813/6000], Train Loss: 0.1141, Test Loss: 0.0940\n",
            "Epoch [1814/6000], Train Loss: 0.1141, Test Loss: 0.0939\n",
            "Epoch [1815/6000], Train Loss: 0.1140, Test Loss: 0.0938\n",
            "Epoch [1816/6000], Train Loss: 0.1139, Test Loss: 0.0939\n",
            "Epoch [1817/6000], Train Loss: 0.1138, Test Loss: 0.0938\n",
            "Epoch [1818/6000], Train Loss: 0.1137, Test Loss: 0.0938\n",
            "Epoch [1819/6000], Train Loss: 0.1136, Test Loss: 0.0937\n",
            "Epoch [1820/6000], Train Loss: 0.1135, Test Loss: 0.0936\n",
            "Epoch [1821/6000], Train Loss: 0.1134, Test Loss: 0.0937\n",
            "Epoch [1822/6000], Train Loss: 0.1133, Test Loss: 0.0936\n",
            "Epoch [1823/6000], Train Loss: 0.1132, Test Loss: 0.0936\n",
            "Epoch [1824/6000], Train Loss: 0.1131, Test Loss: 0.0935\n",
            "Epoch [1825/6000], Train Loss: 0.1130, Test Loss: 0.0934\n",
            "Epoch [1826/6000], Train Loss: 0.1129, Test Loss: 0.0934\n",
            "Epoch [1827/6000], Train Loss: 0.1128, Test Loss: 0.0933\n",
            "Epoch [1828/6000], Train Loss: 0.1127, Test Loss: 0.0934\n",
            "Epoch [1829/6000], Train Loss: 0.1126, Test Loss: 0.0934\n",
            "Epoch [1830/6000], Train Loss: 0.1125, Test Loss: 0.0933\n",
            "Epoch [1831/6000], Train Loss: 0.1124, Test Loss: 0.0932\n",
            "Epoch [1832/6000], Train Loss: 0.1123, Test Loss: 0.0931\n",
            "Epoch [1833/6000], Train Loss: 0.1122, Test Loss: 0.0930\n",
            "Epoch [1834/6000], Train Loss: 0.1121, Test Loss: 0.0930\n",
            "Epoch [1835/6000], Train Loss: 0.1120, Test Loss: 0.0930\n",
            "Epoch [1836/6000], Train Loss: 0.1119, Test Loss: 0.0930\n",
            "Epoch [1837/6000], Train Loss: 0.1118, Test Loss: 0.0929\n",
            "Epoch [1838/6000], Train Loss: 0.1117, Test Loss: 0.0929\n",
            "Epoch [1839/6000], Train Loss: 0.1116, Test Loss: 0.0928\n",
            "Epoch [1840/6000], Train Loss: 0.1115, Test Loss: 0.0927\n",
            "Epoch [1841/6000], Train Loss: 0.1114, Test Loss: 0.0928\n",
            "Epoch [1842/6000], Train Loss: 0.1113, Test Loss: 0.0928\n",
            "Epoch [1843/6000], Train Loss: 0.1112, Test Loss: 0.0927\n",
            "Epoch [1844/6000], Train Loss: 0.1111, Test Loss: 0.0927\n",
            "Epoch [1845/6000], Train Loss: 0.1110, Test Loss: 0.0926\n",
            "Epoch [1846/6000], Train Loss: 0.1110, Test Loss: 0.0924\n",
            "Epoch [1847/6000], Train Loss: 0.1109, Test Loss: 0.0924\n",
            "Epoch [1848/6000], Train Loss: 0.1108, Test Loss: 0.0923\n",
            "Epoch [1849/6000], Train Loss: 0.1107, Test Loss: 0.0923\n",
            "Epoch [1850/6000], Train Loss: 0.1106, Test Loss: 0.0924\n",
            "Epoch [1851/6000], Train Loss: 0.1105, Test Loss: 0.0925\n",
            "Epoch [1852/6000], Train Loss: 0.1104, Test Loss: 0.0924\n",
            "Epoch [1853/6000], Train Loss: 0.1103, Test Loss: 0.0924\n",
            "Epoch [1854/6000], Train Loss: 0.1102, Test Loss: 0.0923\n",
            "Epoch [1855/6000], Train Loss: 0.1101, Test Loss: 0.0922\n",
            "Epoch [1856/6000], Train Loss: 0.1100, Test Loss: 0.0922\n",
            "Epoch [1857/6000], Train Loss: 0.1099, Test Loss: 0.0921\n",
            "Epoch [1858/6000], Train Loss: 0.1098, Test Loss: 0.0921\n",
            "Epoch [1859/6000], Train Loss: 0.1097, Test Loss: 0.0921\n",
            "Epoch [1860/6000], Train Loss: 0.1096, Test Loss: 0.0921\n",
            "Epoch [1861/6000], Train Loss: 0.1095, Test Loss: 0.0920\n",
            "Epoch [1862/6000], Train Loss: 0.1094, Test Loss: 0.0919\n",
            "Epoch [1863/6000], Train Loss: 0.1093, Test Loss: 0.0918\n",
            "Epoch [1864/6000], Train Loss: 0.1092, Test Loss: 0.0917\n",
            "Epoch [1865/6000], Train Loss: 0.1092, Test Loss: 0.0918\n",
            "Epoch [1866/6000], Train Loss: 0.1091, Test Loss: 0.0918\n",
            "Epoch [1867/6000], Train Loss: 0.1090, Test Loss: 0.0917\n",
            "Epoch [1868/6000], Train Loss: 0.1089, Test Loss: 0.0916\n",
            "Epoch [1869/6000], Train Loss: 0.1088, Test Loss: 0.0915\n",
            "Epoch [1870/6000], Train Loss: 0.1087, Test Loss: 0.0914\n",
            "Epoch [1871/6000], Train Loss: 0.1086, Test Loss: 0.0914\n",
            "Epoch [1872/6000], Train Loss: 0.1085, Test Loss: 0.0914\n",
            "Epoch [1873/6000], Train Loss: 0.1084, Test Loss: 0.0914\n",
            "Epoch [1874/6000], Train Loss: 0.1083, Test Loss: 0.0913\n",
            "Epoch [1875/6000], Train Loss: 0.1082, Test Loss: 0.0913\n",
            "Epoch [1876/6000], Train Loss: 0.1081, Test Loss: 0.0912\n",
            "Epoch [1877/6000], Train Loss: 0.1080, Test Loss: 0.0912\n",
            "Epoch [1878/6000], Train Loss: 0.1079, Test Loss: 0.0911\n",
            "Epoch [1879/6000], Train Loss: 0.1079, Test Loss: 0.0912\n",
            "Epoch [1880/6000], Train Loss: 0.1077, Test Loss: 0.0912\n",
            "Epoch [1881/6000], Train Loss: 0.1077, Test Loss: 0.0910\n",
            "Epoch [1882/6000], Train Loss: 0.1076, Test Loss: 0.0910\n",
            "Epoch [1883/6000], Train Loss: 0.1075, Test Loss: 0.0910\n",
            "Epoch [1884/6000], Train Loss: 0.1074, Test Loss: 0.0910\n",
            "Epoch [1885/6000], Train Loss: 0.1073, Test Loss: 0.0910\n",
            "Epoch [1886/6000], Train Loss: 0.1072, Test Loss: 0.0909\n",
            "Epoch [1887/6000], Train Loss: 0.1071, Test Loss: 0.0908\n",
            "Epoch [1888/6000], Train Loss: 0.1070, Test Loss: 0.0906\n",
            "Epoch [1889/6000], Train Loss: 0.1069, Test Loss: 0.0907\n",
            "Epoch [1890/6000], Train Loss: 0.1068, Test Loss: 0.0907\n",
            "Epoch [1891/6000], Train Loss: 0.1067, Test Loss: 0.0907\n",
            "Epoch [1892/6000], Train Loss: 0.1066, Test Loss: 0.0907\n",
            "Epoch [1893/6000], Train Loss: 0.1066, Test Loss: 0.0906\n",
            "Epoch [1894/6000], Train Loss: 0.1065, Test Loss: 0.0906\n",
            "Epoch [1895/6000], Train Loss: 0.1064, Test Loss: 0.0904\n",
            "Epoch [1896/6000], Train Loss: 0.1063, Test Loss: 0.0903\n",
            "Epoch [1897/6000], Train Loss: 0.1062, Test Loss: 0.0903\n",
            "Epoch [1898/6000], Train Loss: 0.1061, Test Loss: 0.0903\n",
            "Epoch [1899/6000], Train Loss: 0.1060, Test Loss: 0.0903\n",
            "Epoch [1900/6000], Train Loss: 0.1059, Test Loss: 0.0903\n",
            "Epoch [1901/6000], Train Loss: 0.1058, Test Loss: 0.0902\n",
            "Epoch [1902/6000], Train Loss: 0.1057, Test Loss: 0.0902\n",
            "Epoch [1903/6000], Train Loss: 0.1056, Test Loss: 0.0901\n",
            "Epoch [1904/6000], Train Loss: 0.1055, Test Loss: 0.0900\n",
            "Epoch [1905/6000], Train Loss: 0.1054, Test Loss: 0.0900\n",
            "Epoch [1906/6000], Train Loss: 0.1054, Test Loss: 0.0900\n",
            "Epoch [1907/6000], Train Loss: 0.1053, Test Loss: 0.0899\n",
            "Epoch [1908/6000], Train Loss: 0.1052, Test Loss: 0.0900\n",
            "Epoch [1909/6000], Train Loss: 0.1051, Test Loss: 0.0899\n",
            "Epoch [1910/6000], Train Loss: 0.1050, Test Loss: 0.0899\n",
            "Epoch [1911/6000], Train Loss: 0.1049, Test Loss: 0.0899\n",
            "Epoch [1912/6000], Train Loss: 0.1048, Test Loss: 0.0897\n",
            "Epoch [1913/6000], Train Loss: 0.1047, Test Loss: 0.0896\n",
            "Epoch [1914/6000], Train Loss: 0.1046, Test Loss: 0.0895\n",
            "Epoch [1915/6000], Train Loss: 0.1046, Test Loss: 0.0896\n",
            "Epoch [1916/6000], Train Loss: 0.1045, Test Loss: 0.0896\n",
            "Epoch [1917/6000], Train Loss: 0.1044, Test Loss: 0.0896\n",
            "Epoch [1918/6000], Train Loss: 0.1043, Test Loss: 0.0895\n",
            "Epoch [1919/6000], Train Loss: 0.1042, Test Loss: 0.0894\n",
            "Epoch [1920/6000], Train Loss: 0.1041, Test Loss: 0.0894\n",
            "Epoch [1921/6000], Train Loss: 0.1040, Test Loss: 0.0894\n",
            "Epoch [1922/6000], Train Loss: 0.1039, Test Loss: 0.0894\n",
            "Epoch [1923/6000], Train Loss: 0.1038, Test Loss: 0.0893\n",
            "Epoch [1924/6000], Train Loss: 0.1037, Test Loss: 0.0892\n",
            "Epoch [1925/6000], Train Loss: 0.1036, Test Loss: 0.0893\n",
            "Epoch [1926/6000], Train Loss: 0.1035, Test Loss: 0.0893\n",
            "Epoch [1927/6000], Train Loss: 0.1035, Test Loss: 0.0892\n",
            "Epoch [1928/6000], Train Loss: 0.1034, Test Loss: 0.0891\n",
            "Epoch [1929/6000], Train Loss: 0.1033, Test Loss: 0.0890\n",
            "Epoch [1930/6000], Train Loss: 0.1032, Test Loss: 0.0889\n",
            "Epoch [1931/6000], Train Loss: 0.1031, Test Loss: 0.0889\n",
            "Epoch [1932/6000], Train Loss: 0.1030, Test Loss: 0.0890\n",
            "Epoch [1933/6000], Train Loss: 0.1029, Test Loss: 0.0889\n",
            "Epoch [1934/6000], Train Loss: 0.1028, Test Loss: 0.0888\n",
            "Epoch [1935/6000], Train Loss: 0.1027, Test Loss: 0.0888\n",
            "Epoch [1936/6000], Train Loss: 0.1027, Test Loss: 0.0888\n",
            "Epoch [1937/6000], Train Loss: 0.1025, Test Loss: 0.0888\n",
            "Epoch [1938/6000], Train Loss: 0.1025, Test Loss: 0.0887\n",
            "Epoch [1939/6000], Train Loss: 0.1024, Test Loss: 0.0886\n",
            "Epoch [1940/6000], Train Loss: 0.1023, Test Loss: 0.0887\n",
            "Epoch [1941/6000], Train Loss: 0.1022, Test Loss: 0.0886\n",
            "Epoch [1942/6000], Train Loss: 0.1021, Test Loss: 0.0886\n",
            "Epoch [1943/6000], Train Loss: 0.1020, Test Loss: 0.0885\n",
            "Epoch [1944/6000], Train Loss: 0.1019, Test Loss: 0.0885\n",
            "Epoch [1945/6000], Train Loss: 0.1018, Test Loss: 0.0884\n",
            "Epoch [1946/6000], Train Loss: 0.1018, Test Loss: 0.0885\n",
            "Epoch [1947/6000], Train Loss: 0.1017, Test Loss: 0.0884\n",
            "Epoch [1948/6000], Train Loss: 0.1016, Test Loss: 0.0884\n",
            "Epoch [1949/6000], Train Loss: 0.1015, Test Loss: 0.0883\n",
            "Epoch [1950/6000], Train Loss: 0.1014, Test Loss: 0.0883\n",
            "Epoch [1951/6000], Train Loss: 0.1013, Test Loss: 0.0882\n",
            "Epoch [1952/6000], Train Loss: 0.1012, Test Loss: 0.0880\n",
            "Epoch [1953/6000], Train Loss: 0.1011, Test Loss: 0.0880\n",
            "Epoch [1954/6000], Train Loss: 0.1011, Test Loss: 0.0881\n",
            "Epoch [1955/6000], Train Loss: 0.1010, Test Loss: 0.0881\n",
            "Epoch [1956/6000], Train Loss: 0.1009, Test Loss: 0.0880\n",
            "Epoch [1957/6000], Train Loss: 0.1008, Test Loss: 0.0878\n",
            "Epoch [1958/6000], Train Loss: 0.1007, Test Loss: 0.0878\n",
            "Epoch [1959/6000], Train Loss: 0.1006, Test Loss: 0.0878\n",
            "Epoch [1960/6000], Train Loss: 0.1005, Test Loss: 0.0877\n",
            "Epoch [1961/6000], Train Loss: 0.1004, Test Loss: 0.0878\n",
            "Epoch [1962/6000], Train Loss: 0.1003, Test Loss: 0.0876\n",
            "Epoch [1963/6000], Train Loss: 0.1003, Test Loss: 0.0876\n",
            "Epoch [1964/6000], Train Loss: 0.1002, Test Loss: 0.0875\n",
            "Epoch [1965/6000], Train Loss: 0.1001, Test Loss: 0.0875\n",
            "Epoch [1966/6000], Train Loss: 0.1000, Test Loss: 0.0876\n",
            "Epoch [1967/6000], Train Loss: 0.0999, Test Loss: 0.0875\n",
            "Epoch [1968/6000], Train Loss: 0.0998, Test Loss: 0.0875\n",
            "Epoch [1969/6000], Train Loss: 0.0997, Test Loss: 0.0874\n",
            "Epoch [1970/6000], Train Loss: 0.0996, Test Loss: 0.0873\n",
            "Epoch [1971/6000], Train Loss: 0.0996, Test Loss: 0.0873\n",
            "Epoch [1972/6000], Train Loss: 0.0995, Test Loss: 0.0874\n",
            "Epoch [1973/6000], Train Loss: 0.0994, Test Loss: 0.0873\n",
            "Epoch [1974/6000], Train Loss: 0.0993, Test Loss: 0.0872\n",
            "Epoch [1975/6000], Train Loss: 0.0992, Test Loss: 0.0871\n",
            "Epoch [1976/6000], Train Loss: 0.0991, Test Loss: 0.0871\n",
            "Epoch [1977/6000], Train Loss: 0.0991, Test Loss: 0.0871\n",
            "Epoch [1978/6000], Train Loss: 0.0990, Test Loss: 0.0871\n",
            "Epoch [1979/6000], Train Loss: 0.0989, Test Loss: 0.0871\n",
            "Epoch [1980/6000], Train Loss: 0.0988, Test Loss: 0.0869\n",
            "Epoch [1981/6000], Train Loss: 0.0987, Test Loss: 0.0868\n",
            "Epoch [1982/6000], Train Loss: 0.0986, Test Loss: 0.0869\n",
            "Epoch [1983/6000], Train Loss: 0.0985, Test Loss: 0.0869\n",
            "Epoch [1984/6000], Train Loss: 0.0984, Test Loss: 0.0869\n",
            "Epoch [1985/6000], Train Loss: 0.0984, Test Loss: 0.0868\n",
            "Epoch [1986/6000], Train Loss: 0.0983, Test Loss: 0.0867\n",
            "Epoch [1987/6000], Train Loss: 0.0982, Test Loss: 0.0866\n",
            "Epoch [1988/6000], Train Loss: 0.0981, Test Loss: 0.0865\n",
            "Epoch [1989/6000], Train Loss: 0.0980, Test Loss: 0.0865\n",
            "Epoch [1990/6000], Train Loss: 0.0979, Test Loss: 0.0865\n",
            "Epoch [1991/6000], Train Loss: 0.0978, Test Loss: 0.0864\n",
            "Epoch [1992/6000], Train Loss: 0.0978, Test Loss: 0.0865\n",
            "Epoch [1993/6000], Train Loss: 0.0977, Test Loss: 0.0864\n",
            "Epoch [1994/6000], Train Loss: 0.0976, Test Loss: 0.0863\n",
            "Epoch [1995/6000], Train Loss: 0.0975, Test Loss: 0.0862\n",
            "Epoch [1996/6000], Train Loss: 0.0974, Test Loss: 0.0863\n",
            "Epoch [1997/6000], Train Loss: 0.0974, Test Loss: 0.0863\n",
            "Epoch [1998/6000], Train Loss: 0.0973, Test Loss: 0.0863\n",
            "Epoch [1999/6000], Train Loss: 0.0972, Test Loss: 0.0862\n",
            "Epoch [2000/6000], Train Loss: 0.0971, Test Loss: 0.0861\n",
            "Epoch [2001/6000], Train Loss: 0.0970, Test Loss: 0.0861\n",
            "Epoch [2002/6000], Train Loss: 0.0969, Test Loss: 0.0861\n",
            "Epoch [2003/6000], Train Loss: 0.0968, Test Loss: 0.0861\n",
            "Epoch [2004/6000], Train Loss: 0.0968, Test Loss: 0.0860\n",
            "Epoch [2005/6000], Train Loss: 0.0967, Test Loss: 0.0861\n",
            "Epoch [2006/6000], Train Loss: 0.0966, Test Loss: 0.0861\n",
            "Epoch [2007/6000], Train Loss: 0.0965, Test Loss: 0.0861\n",
            "Epoch [2008/6000], Train Loss: 0.0964, Test Loss: 0.0860\n",
            "Epoch [2009/6000], Train Loss: 0.0963, Test Loss: 0.0859\n",
            "Epoch [2010/6000], Train Loss: 0.0962, Test Loss: 0.0858\n",
            "Epoch [2011/6000], Train Loss: 0.0962, Test Loss: 0.0857\n",
            "Epoch [2012/6000], Train Loss: 0.0961, Test Loss: 0.0857\n",
            "Epoch [2013/6000], Train Loss: 0.0960, Test Loss: 0.0858\n",
            "Epoch [2014/6000], Train Loss: 0.0959, Test Loss: 0.0858\n",
            "Epoch [2015/6000], Train Loss: 0.0958, Test Loss: 0.0857\n",
            "Epoch [2016/6000], Train Loss: 0.0957, Test Loss: 0.0856\n",
            "Epoch [2017/6000], Train Loss: 0.0957, Test Loss: 0.0855\n",
            "Epoch [2018/6000], Train Loss: 0.0956, Test Loss: 0.0853\n",
            "Epoch [2019/6000], Train Loss: 0.0955, Test Loss: 0.0852\n",
            "Epoch [2020/6000], Train Loss: 0.0954, Test Loss: 0.0852\n",
            "Epoch [2021/6000], Train Loss: 0.0953, Test Loss: 0.0853\n",
            "Epoch [2022/6000], Train Loss: 0.0952, Test Loss: 0.0854\n",
            "Epoch [2023/6000], Train Loss: 0.0952, Test Loss: 0.0854\n",
            "Epoch [2024/6000], Train Loss: 0.0951, Test Loss: 0.0853\n",
            "Epoch [2025/6000], Train Loss: 0.0950, Test Loss: 0.0852\n",
            "Epoch [2026/6000], Train Loss: 0.0949, Test Loss: 0.0851\n",
            "Epoch [2027/6000], Train Loss: 0.0948, Test Loss: 0.0850\n",
            "Epoch [2028/6000], Train Loss: 0.0948, Test Loss: 0.0849\n",
            "Epoch [2029/6000], Train Loss: 0.0947, Test Loss: 0.0849\n",
            "Epoch [2030/6000], Train Loss: 0.0946, Test Loss: 0.0849\n",
            "Epoch [2031/6000], Train Loss: 0.0945, Test Loss: 0.0849\n",
            "Epoch [2032/6000], Train Loss: 0.0944, Test Loss: 0.0850\n",
            "Epoch [2033/6000], Train Loss: 0.0943, Test Loss: 0.0850\n",
            "Epoch [2034/6000], Train Loss: 0.0943, Test Loss: 0.0849\n",
            "Epoch [2035/6000], Train Loss: 0.0942, Test Loss: 0.0849\n",
            "Epoch [2036/6000], Train Loss: 0.0941, Test Loss: 0.0848\n",
            "Epoch [2037/6000], Train Loss: 0.0940, Test Loss: 0.0846\n",
            "Epoch [2038/6000], Train Loss: 0.0939, Test Loss: 0.0846\n",
            "Epoch [2039/6000], Train Loss: 0.0938, Test Loss: 0.0846\n",
            "Epoch [2040/6000], Train Loss: 0.0938, Test Loss: 0.0846\n",
            "Epoch [2041/6000], Train Loss: 0.0937, Test Loss: 0.0847\n",
            "Epoch [2042/6000], Train Loss: 0.0936, Test Loss: 0.0846\n",
            "Epoch [2043/6000], Train Loss: 0.0935, Test Loss: 0.0844\n",
            "Epoch [2044/6000], Train Loss: 0.0934, Test Loss: 0.0843\n",
            "Epoch [2045/6000], Train Loss: 0.0934, Test Loss: 0.0843\n",
            "Epoch [2046/6000], Train Loss: 0.0933, Test Loss: 0.0843\n",
            "Epoch [2047/6000], Train Loss: 0.0932, Test Loss: 0.0842\n",
            "Epoch [2048/6000], Train Loss: 0.0931, Test Loss: 0.0843\n",
            "Epoch [2049/6000], Train Loss: 0.0930, Test Loss: 0.0842\n",
            "Epoch [2050/6000], Train Loss: 0.0929, Test Loss: 0.0842\n",
            "Epoch [2051/6000], Train Loss: 0.0928, Test Loss: 0.0841\n",
            "Epoch [2052/6000], Train Loss: 0.0928, Test Loss: 0.0839\n",
            "Epoch [2053/6000], Train Loss: 0.0927, Test Loss: 0.0839\n",
            "Epoch [2054/6000], Train Loss: 0.0926, Test Loss: 0.0839\n",
            "Epoch [2055/6000], Train Loss: 0.0925, Test Loss: 0.0839\n",
            "Epoch [2056/6000], Train Loss: 0.0924, Test Loss: 0.0839\n",
            "Epoch [2057/6000], Train Loss: 0.0924, Test Loss: 0.0838\n",
            "Epoch [2058/6000], Train Loss: 0.0923, Test Loss: 0.0838\n",
            "Epoch [2059/6000], Train Loss: 0.0922, Test Loss: 0.0838\n",
            "Epoch [2060/6000], Train Loss: 0.0921, Test Loss: 0.0837\n",
            "Epoch [2061/6000], Train Loss: 0.0920, Test Loss: 0.0837\n",
            "Epoch [2062/6000], Train Loss: 0.0920, Test Loss: 0.0836\n",
            "Epoch [2063/6000], Train Loss: 0.0919, Test Loss: 0.0837\n",
            "Epoch [2064/6000], Train Loss: 0.0918, Test Loss: 0.0835\n",
            "Epoch [2065/6000], Train Loss: 0.0917, Test Loss: 0.0834\n",
            "Epoch [2066/6000], Train Loss: 0.0916, Test Loss: 0.0834\n",
            "Epoch [2067/6000], Train Loss: 0.0916, Test Loss: 0.0835\n",
            "Epoch [2068/6000], Train Loss: 0.0915, Test Loss: 0.0835\n",
            "Epoch [2069/6000], Train Loss: 0.0914, Test Loss: 0.0835\n",
            "Epoch [2070/6000], Train Loss: 0.0913, Test Loss: 0.0834\n",
            "Epoch [2071/6000], Train Loss: 0.0912, Test Loss: 0.0832\n",
            "Epoch [2072/6000], Train Loss: 0.0912, Test Loss: 0.0831\n",
            "Epoch [2073/6000], Train Loss: 0.0911, Test Loss: 0.0830\n",
            "Epoch [2074/6000], Train Loss: 0.0910, Test Loss: 0.0831\n",
            "Epoch [2075/6000], Train Loss: 0.0909, Test Loss: 0.0831\n",
            "Epoch [2076/6000], Train Loss: 0.0908, Test Loss: 0.0831\n",
            "Epoch [2077/6000], Train Loss: 0.0908, Test Loss: 0.0831\n",
            "Epoch [2078/6000], Train Loss: 0.0907, Test Loss: 0.0831\n",
            "Epoch [2079/6000], Train Loss: 0.0906, Test Loss: 0.0830\n",
            "Epoch [2080/6000], Train Loss: 0.0905, Test Loss: 0.0829\n",
            "Epoch [2081/6000], Train Loss: 0.0904, Test Loss: 0.0829\n",
            "Epoch [2082/6000], Train Loss: 0.0904, Test Loss: 0.0829\n",
            "Epoch [2083/6000], Train Loss: 0.0903, Test Loss: 0.0829\n",
            "Epoch [2084/6000], Train Loss: 0.0902, Test Loss: 0.0827\n",
            "Epoch [2085/6000], Train Loss: 0.0901, Test Loss: 0.0826\n",
            "Epoch [2086/6000], Train Loss: 0.0901, Test Loss: 0.0826\n",
            "Epoch [2087/6000], Train Loss: 0.0900, Test Loss: 0.0826\n",
            "Epoch [2088/6000], Train Loss: 0.0899, Test Loss: 0.0827\n",
            "Epoch [2089/6000], Train Loss: 0.0898, Test Loss: 0.0826\n",
            "Epoch [2090/6000], Train Loss: 0.0897, Test Loss: 0.0826\n",
            "Epoch [2091/6000], Train Loss: 0.0897, Test Loss: 0.0826\n",
            "Epoch [2092/6000], Train Loss: 0.0896, Test Loss: 0.0825\n",
            "Epoch [2093/6000], Train Loss: 0.0895, Test Loss: 0.0824\n",
            "Epoch [2094/6000], Train Loss: 0.0894, Test Loss: 0.0823\n",
            "Epoch [2095/6000], Train Loss: 0.0893, Test Loss: 0.0823\n",
            "Epoch [2096/6000], Train Loss: 0.0893, Test Loss: 0.0823\n",
            "Epoch [2097/6000], Train Loss: 0.0892, Test Loss: 0.0823\n",
            "Epoch [2098/6000], Train Loss: 0.0891, Test Loss: 0.0823\n",
            "Epoch [2099/6000], Train Loss: 0.0890, Test Loss: 0.0822\n",
            "Epoch [2100/6000], Train Loss: 0.0889, Test Loss: 0.0821\n",
            "Epoch [2101/6000], Train Loss: 0.0889, Test Loss: 0.0819\n",
            "Epoch [2102/6000], Train Loss: 0.0888, Test Loss: 0.0820\n",
            "Epoch [2103/6000], Train Loss: 0.0887, Test Loss: 0.0820\n",
            "Epoch [2104/6000], Train Loss: 0.0886, Test Loss: 0.0820\n",
            "Epoch [2105/6000], Train Loss: 0.0886, Test Loss: 0.0819\n",
            "Epoch [2106/6000], Train Loss: 0.0885, Test Loss: 0.0818\n",
            "Epoch [2107/6000], Train Loss: 0.0884, Test Loss: 0.0817\n",
            "Epoch [2108/6000], Train Loss: 0.0883, Test Loss: 0.0817\n",
            "Epoch [2109/6000], Train Loss: 0.0883, Test Loss: 0.0818\n",
            "Epoch [2110/6000], Train Loss: 0.0882, Test Loss: 0.0817\n",
            "Epoch [2111/6000], Train Loss: 0.0881, Test Loss: 0.0817\n",
            "Epoch [2112/6000], Train Loss: 0.0880, Test Loss: 0.0816\n",
            "Epoch [2113/6000], Train Loss: 0.0879, Test Loss: 0.0816\n",
            "Epoch [2114/6000], Train Loss: 0.0879, Test Loss: 0.0815\n",
            "Epoch [2115/6000], Train Loss: 0.0878, Test Loss: 0.0815\n",
            "Epoch [2116/6000], Train Loss: 0.0877, Test Loss: 0.0814\n",
            "Epoch [2117/6000], Train Loss: 0.0876, Test Loss: 0.0814\n",
            "Epoch [2118/6000], Train Loss: 0.0876, Test Loss: 0.0813\n",
            "Epoch [2119/6000], Train Loss: 0.0875, Test Loss: 0.0813\n",
            "Epoch [2120/6000], Train Loss: 0.0874, Test Loss: 0.0814\n",
            "Epoch [2121/6000], Train Loss: 0.0873, Test Loss: 0.0814\n",
            "Epoch [2122/6000], Train Loss: 0.0873, Test Loss: 0.0814\n",
            "Epoch [2123/6000], Train Loss: 0.0872, Test Loss: 0.0813\n",
            "Epoch [2124/6000], Train Loss: 0.0871, Test Loss: 0.0812\n",
            "Epoch [2125/6000], Train Loss: 0.0870, Test Loss: 0.0811\n",
            "Epoch [2126/6000], Train Loss: 0.0870, Test Loss: 0.0810\n",
            "Epoch [2127/6000], Train Loss: 0.0869, Test Loss: 0.0810\n",
            "Epoch [2128/6000], Train Loss: 0.0868, Test Loss: 0.0811\n",
            "Epoch [2129/6000], Train Loss: 0.0867, Test Loss: 0.0810\n",
            "Epoch [2130/6000], Train Loss: 0.0866, Test Loss: 0.0810\n",
            "Epoch [2131/6000], Train Loss: 0.0866, Test Loss: 0.0809\n",
            "Epoch [2132/6000], Train Loss: 0.0865, Test Loss: 0.0808\n",
            "Epoch [2133/6000], Train Loss: 0.0864, Test Loss: 0.0808\n",
            "Epoch [2134/6000], Train Loss: 0.0863, Test Loss: 0.0809\n",
            "Epoch [2135/6000], Train Loss: 0.0863, Test Loss: 0.0809\n",
            "Epoch [2136/6000], Train Loss: 0.0862, Test Loss: 0.0808\n",
            "Epoch [2137/6000], Train Loss: 0.0861, Test Loss: 0.0807\n",
            "Epoch [2138/6000], Train Loss: 0.0860, Test Loss: 0.0807\n",
            "Epoch [2139/6000], Train Loss: 0.0859, Test Loss: 0.0806\n",
            "Epoch [2140/6000], Train Loss: 0.0859, Test Loss: 0.0806\n",
            "Epoch [2141/6000], Train Loss: 0.0858, Test Loss: 0.0806\n",
            "Epoch [2142/6000], Train Loss: 0.0857, Test Loss: 0.0806\n",
            "Epoch [2143/6000], Train Loss: 0.0857, Test Loss: 0.0806\n",
            "Epoch [2144/6000], Train Loss: 0.0856, Test Loss: 0.0805\n",
            "Epoch [2145/6000], Train Loss: 0.0855, Test Loss: 0.0804\n",
            "Epoch [2146/6000], Train Loss: 0.0854, Test Loss: 0.0803\n",
            "Epoch [2147/6000], Train Loss: 0.0854, Test Loss: 0.0803\n",
            "Epoch [2148/6000], Train Loss: 0.0853, Test Loss: 0.0803\n",
            "Epoch [2149/6000], Train Loss: 0.0852, Test Loss: 0.0802\n",
            "Epoch [2150/6000], Train Loss: 0.0851, Test Loss: 0.0801\n",
            "Epoch [2151/6000], Train Loss: 0.0850, Test Loss: 0.0801\n",
            "Epoch [2152/6000], Train Loss: 0.0850, Test Loss: 0.0801\n",
            "Epoch [2153/6000], Train Loss: 0.0849, Test Loss: 0.0801\n",
            "Epoch [2154/6000], Train Loss: 0.0848, Test Loss: 0.0800\n",
            "Epoch [2155/6000], Train Loss: 0.0848, Test Loss: 0.0799\n",
            "Epoch [2156/6000], Train Loss: 0.0847, Test Loss: 0.0798\n",
            "Epoch [2157/6000], Train Loss: 0.0846, Test Loss: 0.0798\n",
            "Epoch [2158/6000], Train Loss: 0.0845, Test Loss: 0.0798\n",
            "Epoch [2159/6000], Train Loss: 0.0845, Test Loss: 0.0797\n",
            "Epoch [2160/6000], Train Loss: 0.0844, Test Loss: 0.0797\n",
            "Epoch [2161/6000], Train Loss: 0.0843, Test Loss: 0.0796\n",
            "Epoch [2162/6000], Train Loss: 0.0843, Test Loss: 0.0796\n",
            "Epoch [2163/6000], Train Loss: 0.0842, Test Loss: 0.0796\n",
            "Epoch [2164/6000], Train Loss: 0.0841, Test Loss: 0.0796\n",
            "Epoch [2165/6000], Train Loss: 0.0840, Test Loss: 0.0796\n",
            "Epoch [2166/6000], Train Loss: 0.0839, Test Loss: 0.0796\n",
            "Epoch [2167/6000], Train Loss: 0.0839, Test Loss: 0.0795\n",
            "Epoch [2168/6000], Train Loss: 0.0838, Test Loss: 0.0794\n",
            "Epoch [2169/6000], Train Loss: 0.0837, Test Loss: 0.0793\n",
            "Epoch [2170/6000], Train Loss: 0.0837, Test Loss: 0.0793\n",
            "Epoch [2171/6000], Train Loss: 0.0836, Test Loss: 0.0793\n",
            "Epoch [2172/6000], Train Loss: 0.0835, Test Loss: 0.0793\n",
            "Epoch [2173/6000], Train Loss: 0.0834, Test Loss: 0.0792\n",
            "Epoch [2174/6000], Train Loss: 0.0833, Test Loss: 0.0792\n",
            "Epoch [2175/6000], Train Loss: 0.0833, Test Loss: 0.0792\n",
            "Epoch [2176/6000], Train Loss: 0.0832, Test Loss: 0.0791\n",
            "Epoch [2177/6000], Train Loss: 0.0831, Test Loss: 0.0791\n",
            "Epoch [2178/6000], Train Loss: 0.0831, Test Loss: 0.0791\n",
            "Epoch [2179/6000], Train Loss: 0.0830, Test Loss: 0.0790\n",
            "Epoch [2180/6000], Train Loss: 0.0829, Test Loss: 0.0790\n",
            "Epoch [2181/6000], Train Loss: 0.0828, Test Loss: 0.0789\n",
            "Epoch [2182/6000], Train Loss: 0.0828, Test Loss: 0.0789\n",
            "Epoch [2183/6000], Train Loss: 0.0827, Test Loss: 0.0789\n",
            "Epoch [2184/6000], Train Loss: 0.0826, Test Loss: 0.0788\n",
            "Epoch [2185/6000], Train Loss: 0.0825, Test Loss: 0.0789\n",
            "Epoch [2186/6000], Train Loss: 0.0825, Test Loss: 0.0788\n",
            "Epoch [2187/6000], Train Loss: 0.0824, Test Loss: 0.0787\n",
            "Epoch [2188/6000], Train Loss: 0.0823, Test Loss: 0.0787\n",
            "Epoch [2189/6000], Train Loss: 0.0822, Test Loss: 0.0786\n",
            "Epoch [2190/6000], Train Loss: 0.0822, Test Loss: 0.0787\n",
            "Epoch [2191/6000], Train Loss: 0.0821, Test Loss: 0.0786\n",
            "Epoch [2192/6000], Train Loss: 0.0820, Test Loss: 0.0786\n",
            "Epoch [2193/6000], Train Loss: 0.0820, Test Loss: 0.0785\n",
            "Epoch [2194/6000], Train Loss: 0.0819, Test Loss: 0.0784\n",
            "Epoch [2195/6000], Train Loss: 0.0818, Test Loss: 0.0784\n",
            "Epoch [2196/6000], Train Loss: 0.0817, Test Loss: 0.0784\n",
            "Epoch [2197/6000], Train Loss: 0.0817, Test Loss: 0.0783\n",
            "Epoch [2198/6000], Train Loss: 0.0816, Test Loss: 0.0783\n",
            "Epoch [2199/6000], Train Loss: 0.0815, Test Loss: 0.0782\n",
            "Epoch [2200/6000], Train Loss: 0.0815, Test Loss: 0.0782\n",
            "Epoch [2201/6000], Train Loss: 0.0814, Test Loss: 0.0781\n",
            "Epoch [2202/6000], Train Loss: 0.0813, Test Loss: 0.0781\n",
            "Epoch [2203/6000], Train Loss: 0.0813, Test Loss: 0.0780\n",
            "Epoch [2204/6000], Train Loss: 0.0812, Test Loss: 0.0779\n",
            "Epoch [2205/6000], Train Loss: 0.0811, Test Loss: 0.0780\n",
            "Epoch [2206/6000], Train Loss: 0.0810, Test Loss: 0.0780\n",
            "Epoch [2207/6000], Train Loss: 0.0810, Test Loss: 0.0780\n",
            "Epoch [2208/6000], Train Loss: 0.0809, Test Loss: 0.0779\n",
            "Epoch [2209/6000], Train Loss: 0.0808, Test Loss: 0.0779\n",
            "Epoch [2210/6000], Train Loss: 0.0807, Test Loss: 0.0778\n",
            "Epoch [2211/6000], Train Loss: 0.0807, Test Loss: 0.0777\n",
            "Epoch [2212/6000], Train Loss: 0.0806, Test Loss: 0.0776\n",
            "Epoch [2213/6000], Train Loss: 0.0806, Test Loss: 0.0776\n",
            "Epoch [2214/6000], Train Loss: 0.0805, Test Loss: 0.0776\n",
            "Epoch [2215/6000], Train Loss: 0.0804, Test Loss: 0.0776\n",
            "Epoch [2216/6000], Train Loss: 0.0803, Test Loss: 0.0776\n",
            "Epoch [2217/6000], Train Loss: 0.0802, Test Loss: 0.0775\n",
            "Epoch [2218/6000], Train Loss: 0.0802, Test Loss: 0.0774\n",
            "Epoch [2219/6000], Train Loss: 0.0801, Test Loss: 0.0774\n",
            "Epoch [2220/6000], Train Loss: 0.0800, Test Loss: 0.0773\n",
            "Epoch [2221/6000], Train Loss: 0.0800, Test Loss: 0.0774\n",
            "Epoch [2222/6000], Train Loss: 0.0799, Test Loss: 0.0774\n",
            "Epoch [2223/6000], Train Loss: 0.0798, Test Loss: 0.0773\n",
            "Epoch [2224/6000], Train Loss: 0.0797, Test Loss: 0.0773\n",
            "Epoch [2225/6000], Train Loss: 0.0797, Test Loss: 0.0771\n",
            "Epoch [2226/6000], Train Loss: 0.0796, Test Loss: 0.0771\n",
            "Epoch [2227/6000], Train Loss: 0.0795, Test Loss: 0.0770\n",
            "Epoch [2228/6000], Train Loss: 0.0795, Test Loss: 0.0771\n",
            "Epoch [2229/6000], Train Loss: 0.0794, Test Loss: 0.0771\n",
            "Epoch [2230/6000], Train Loss: 0.0793, Test Loss: 0.0770\n",
            "Epoch [2231/6000], Train Loss: 0.0793, Test Loss: 0.0770\n",
            "Epoch [2232/6000], Train Loss: 0.0792, Test Loss: 0.0769\n",
            "Epoch [2233/6000], Train Loss: 0.0791, Test Loss: 0.0768\n",
            "Epoch [2234/6000], Train Loss: 0.0791, Test Loss: 0.0767\n",
            "Epoch [2235/6000], Train Loss: 0.0790, Test Loss: 0.0767\n",
            "Epoch [2236/6000], Train Loss: 0.0789, Test Loss: 0.0766\n",
            "Epoch [2237/6000], Train Loss: 0.0789, Test Loss: 0.0768\n",
            "Epoch [2238/6000], Train Loss: 0.0788, Test Loss: 0.0768\n",
            "Epoch [2239/6000], Train Loss: 0.0787, Test Loss: 0.0768\n",
            "Epoch [2240/6000], Train Loss: 0.0786, Test Loss: 0.0767\n",
            "Epoch [2241/6000], Train Loss: 0.0786, Test Loss: 0.0767\n",
            "Epoch [2242/6000], Train Loss: 0.0785, Test Loss: 0.0766\n",
            "Epoch [2243/6000], Train Loss: 0.0784, Test Loss: 0.0764\n",
            "Epoch [2244/6000], Train Loss: 0.0784, Test Loss: 0.0764\n",
            "Epoch [2245/6000], Train Loss: 0.0783, Test Loss: 0.0764\n",
            "Epoch [2246/6000], Train Loss: 0.0782, Test Loss: 0.0763\n",
            "Epoch [2247/6000], Train Loss: 0.0781, Test Loss: 0.0763\n",
            "Epoch [2248/6000], Train Loss: 0.0781, Test Loss: 0.0763\n",
            "Epoch [2249/6000], Train Loss: 0.0780, Test Loss: 0.0762\n",
            "Epoch [2250/6000], Train Loss: 0.0779, Test Loss: 0.0762\n",
            "Epoch [2251/6000], Train Loss: 0.0779, Test Loss: 0.0761\n",
            "Epoch [2252/6000], Train Loss: 0.0778, Test Loss: 0.0761\n",
            "Epoch [2253/6000], Train Loss: 0.0777, Test Loss: 0.0762\n",
            "Epoch [2254/6000], Train Loss: 0.0777, Test Loss: 0.0761\n",
            "Epoch [2255/6000], Train Loss: 0.0776, Test Loss: 0.0760\n",
            "Epoch [2256/6000], Train Loss: 0.0775, Test Loss: 0.0759\n",
            "Epoch [2257/6000], Train Loss: 0.0775, Test Loss: 0.0758\n",
            "Epoch [2258/6000], Train Loss: 0.0774, Test Loss: 0.0758\n",
            "Epoch [2259/6000], Train Loss: 0.0774, Test Loss: 0.0758\n",
            "Epoch [2260/6000], Train Loss: 0.0772, Test Loss: 0.0759\n",
            "Epoch [2261/6000], Train Loss: 0.0772, Test Loss: 0.0759\n",
            "Epoch [2262/6000], Train Loss: 0.0771, Test Loss: 0.0759\n",
            "Epoch [2263/6000], Train Loss: 0.0770, Test Loss: 0.0758\n",
            "Epoch [2264/6000], Train Loss: 0.0770, Test Loss: 0.0757\n",
            "Epoch [2265/6000], Train Loss: 0.0769, Test Loss: 0.0756\n",
            "Epoch [2266/6000], Train Loss: 0.0769, Test Loss: 0.0756\n",
            "Epoch [2267/6000], Train Loss: 0.0768, Test Loss: 0.0756\n",
            "Epoch [2268/6000], Train Loss: 0.0767, Test Loss: 0.0756\n",
            "Epoch [2269/6000], Train Loss: 0.0767, Test Loss: 0.0756\n",
            "Epoch [2270/6000], Train Loss: 0.0766, Test Loss: 0.0756\n",
            "Epoch [2271/6000], Train Loss: 0.0765, Test Loss: 0.0755\n",
            "Epoch [2272/6000], Train Loss: 0.0764, Test Loss: 0.0754\n",
            "Epoch [2273/6000], Train Loss: 0.0764, Test Loss: 0.0754\n",
            "Epoch [2274/6000], Train Loss: 0.0763, Test Loss: 0.0752\n",
            "Epoch [2275/6000], Train Loss: 0.0763, Test Loss: 0.0753\n",
            "Epoch [2276/6000], Train Loss: 0.0762, Test Loss: 0.0753\n",
            "Epoch [2277/6000], Train Loss: 0.0761, Test Loss: 0.0753\n",
            "Epoch [2278/6000], Train Loss: 0.0760, Test Loss: 0.0752\n",
            "Epoch [2279/6000], Train Loss: 0.0760, Test Loss: 0.0751\n",
            "Epoch [2280/6000], Train Loss: 0.0759, Test Loss: 0.0750\n",
            "Epoch [2281/6000], Train Loss: 0.0758, Test Loss: 0.0749\n",
            "Epoch [2282/6000], Train Loss: 0.0758, Test Loss: 0.0748\n",
            "Epoch [2283/6000], Train Loss: 0.0757, Test Loss: 0.0749\n",
            "Epoch [2284/6000], Train Loss: 0.0756, Test Loss: 0.0749\n",
            "Epoch [2285/6000], Train Loss: 0.0756, Test Loss: 0.0750\n",
            "Epoch [2286/6000], Train Loss: 0.0755, Test Loss: 0.0749\n",
            "Epoch [2287/6000], Train Loss: 0.0754, Test Loss: 0.0748\n",
            "Epoch [2288/6000], Train Loss: 0.0754, Test Loss: 0.0747\n",
            "Epoch [2289/6000], Train Loss: 0.0753, Test Loss: 0.0747\n",
            "Epoch [2290/6000], Train Loss: 0.0752, Test Loss: 0.0747\n",
            "Epoch [2291/6000], Train Loss: 0.0752, Test Loss: 0.0747\n",
            "Epoch [2292/6000], Train Loss: 0.0751, Test Loss: 0.0746\n",
            "Epoch [2293/6000], Train Loss: 0.0750, Test Loss: 0.0745\n",
            "Epoch [2294/6000], Train Loss: 0.0750, Test Loss: 0.0744\n",
            "Epoch [2295/6000], Train Loss: 0.0749, Test Loss: 0.0744\n",
            "Epoch [2296/6000], Train Loss: 0.0748, Test Loss: 0.0743\n",
            "Epoch [2297/6000], Train Loss: 0.0748, Test Loss: 0.0743\n",
            "Epoch [2298/6000], Train Loss: 0.0747, Test Loss: 0.0744\n",
            "Epoch [2299/6000], Train Loss: 0.0746, Test Loss: 0.0744\n",
            "Epoch [2300/6000], Train Loss: 0.0746, Test Loss: 0.0744\n",
            "Epoch [2301/6000], Train Loss: 0.0745, Test Loss: 0.0743\n",
            "Epoch [2302/6000], Train Loss: 0.0744, Test Loss: 0.0741\n",
            "Epoch [2303/6000], Train Loss: 0.0744, Test Loss: 0.0741\n",
            "Epoch [2304/6000], Train Loss: 0.0743, Test Loss: 0.0741\n",
            "Epoch [2305/6000], Train Loss: 0.0742, Test Loss: 0.0741\n",
            "Epoch [2306/6000], Train Loss: 0.0742, Test Loss: 0.0741\n",
            "Epoch [2307/6000], Train Loss: 0.0741, Test Loss: 0.0740\n",
            "Epoch [2308/6000], Train Loss: 0.0740, Test Loss: 0.0739\n",
            "Epoch [2309/6000], Train Loss: 0.0740, Test Loss: 0.0739\n",
            "Epoch [2310/6000], Train Loss: 0.0739, Test Loss: 0.0739\n",
            "Epoch [2311/6000], Train Loss: 0.0738, Test Loss: 0.0738\n",
            "Epoch [2312/6000], Train Loss: 0.0738, Test Loss: 0.0737\n",
            "Epoch [2313/6000], Train Loss: 0.0737, Test Loss: 0.0738\n",
            "Epoch [2314/6000], Train Loss: 0.0737, Test Loss: 0.0738\n",
            "Epoch [2315/6000], Train Loss: 0.0736, Test Loss: 0.0737\n",
            "Epoch [2316/6000], Train Loss: 0.0735, Test Loss: 0.0737\n",
            "Epoch [2317/6000], Train Loss: 0.0734, Test Loss: 0.0736\n",
            "Epoch [2318/6000], Train Loss: 0.0734, Test Loss: 0.0736\n",
            "Epoch [2319/6000], Train Loss: 0.0733, Test Loss: 0.0736\n",
            "Epoch [2320/6000], Train Loss: 0.0733, Test Loss: 0.0736\n",
            "Epoch [2321/6000], Train Loss: 0.0732, Test Loss: 0.0734\n",
            "Epoch [2322/6000], Train Loss: 0.0731, Test Loss: 0.0733\n",
            "Epoch [2323/6000], Train Loss: 0.0731, Test Loss: 0.0733\n",
            "Epoch [2324/6000], Train Loss: 0.0730, Test Loss: 0.0732\n",
            "Epoch [2325/6000], Train Loss: 0.0730, Test Loss: 0.0733\n",
            "Epoch [2326/6000], Train Loss: 0.0729, Test Loss: 0.0734\n",
            "Epoch [2327/6000], Train Loss: 0.0728, Test Loss: 0.0733\n",
            "Epoch [2328/6000], Train Loss: 0.0727, Test Loss: 0.0732\n",
            "Epoch [2329/6000], Train Loss: 0.0727, Test Loss: 0.0731\n",
            "Epoch [2330/6000], Train Loss: 0.0726, Test Loss: 0.0730\n",
            "Epoch [2331/6000], Train Loss: 0.0725, Test Loss: 0.0730\n",
            "Epoch [2332/6000], Train Loss: 0.0725, Test Loss: 0.0730\n",
            "Epoch [2333/6000], Train Loss: 0.0724, Test Loss: 0.0729\n",
            "Epoch [2334/6000], Train Loss: 0.0724, Test Loss: 0.0730\n",
            "Epoch [2335/6000], Train Loss: 0.0723, Test Loss: 0.0729\n",
            "Epoch [2336/6000], Train Loss: 0.0722, Test Loss: 0.0729\n",
            "Epoch [2337/6000], Train Loss: 0.0722, Test Loss: 0.0728\n",
            "Epoch [2338/6000], Train Loss: 0.0721, Test Loss: 0.0727\n",
            "Epoch [2339/6000], Train Loss: 0.0720, Test Loss: 0.0726\n",
            "Epoch [2340/6000], Train Loss: 0.0720, Test Loss: 0.0725\n",
            "Epoch [2341/6000], Train Loss: 0.0719, Test Loss: 0.0725\n",
            "Epoch [2342/6000], Train Loss: 0.0719, Test Loss: 0.0726\n",
            "Epoch [2343/6000], Train Loss: 0.0718, Test Loss: 0.0727\n",
            "Epoch [2344/6000], Train Loss: 0.0717, Test Loss: 0.0726\n",
            "Epoch [2345/6000], Train Loss: 0.0717, Test Loss: 0.0726\n",
            "Epoch [2346/6000], Train Loss: 0.0716, Test Loss: 0.0724\n",
            "Epoch [2347/6000], Train Loss: 0.0715, Test Loss: 0.0724\n",
            "Epoch [2348/6000], Train Loss: 0.0714, Test Loss: 0.0723\n",
            "Epoch [2349/6000], Train Loss: 0.0714, Test Loss: 0.0724\n",
            "Epoch [2350/6000], Train Loss: 0.0713, Test Loss: 0.0723\n",
            "Epoch [2351/6000], Train Loss: 0.0713, Test Loss: 0.0722\n",
            "Epoch [2352/6000], Train Loss: 0.0712, Test Loss: 0.0722\n",
            "Epoch [2353/6000], Train Loss: 0.0711, Test Loss: 0.0721\n",
            "Epoch [2354/6000], Train Loss: 0.0711, Test Loss: 0.0721\n",
            "Epoch [2355/6000], Train Loss: 0.0710, Test Loss: 0.0722\n",
            "Epoch [2356/6000], Train Loss: 0.0710, Test Loss: 0.0722\n",
            "Epoch [2357/6000], Train Loss: 0.0709, Test Loss: 0.0721\n",
            "Epoch [2358/6000], Train Loss: 0.0708, Test Loss: 0.0719\n",
            "Epoch [2359/6000], Train Loss: 0.0708, Test Loss: 0.0718\n",
            "Epoch [2360/6000], Train Loss: 0.0707, Test Loss: 0.0718\n",
            "Epoch [2361/6000], Train Loss: 0.0706, Test Loss: 0.0717\n",
            "Epoch [2362/6000], Train Loss: 0.0706, Test Loss: 0.0718\n",
            "Epoch [2363/6000], Train Loss: 0.0705, Test Loss: 0.0718\n",
            "Epoch [2364/6000], Train Loss: 0.0705, Test Loss: 0.0718\n",
            "Epoch [2365/6000], Train Loss: 0.0704, Test Loss: 0.0717\n",
            "Epoch [2366/6000], Train Loss: 0.0703, Test Loss: 0.0716\n",
            "Epoch [2367/6000], Train Loss: 0.0703, Test Loss: 0.0717\n",
            "Epoch [2368/6000], Train Loss: 0.0702, Test Loss: 0.0716\n",
            "Epoch [2369/6000], Train Loss: 0.0701, Test Loss: 0.0715\n",
            "Epoch [2370/6000], Train Loss: 0.0701, Test Loss: 0.0715\n",
            "Epoch [2371/6000], Train Loss: 0.0700, Test Loss: 0.0714\n",
            "Epoch [2372/6000], Train Loss: 0.0699, Test Loss: 0.0715\n",
            "Epoch [2373/6000], Train Loss: 0.0699, Test Loss: 0.0714\n",
            "Epoch [2374/6000], Train Loss: 0.0698, Test Loss: 0.0714\n",
            "Epoch [2375/6000], Train Loss: 0.0698, Test Loss: 0.0713\n",
            "Epoch [2376/6000], Train Loss: 0.0697, Test Loss: 0.0712\n",
            "Epoch [2377/6000], Train Loss: 0.0696, Test Loss: 0.0712\n",
            "Epoch [2378/6000], Train Loss: 0.0696, Test Loss: 0.0713\n",
            "Epoch [2379/6000], Train Loss: 0.0695, Test Loss: 0.0713\n",
            "Epoch [2380/6000], Train Loss: 0.0695, Test Loss: 0.0712\n",
            "Epoch [2381/6000], Train Loss: 0.0694, Test Loss: 0.0712\n",
            "Epoch [2382/6000], Train Loss: 0.0693, Test Loss: 0.0711\n",
            "Epoch [2383/6000], Train Loss: 0.0693, Test Loss: 0.0709\n",
            "Epoch [2384/6000], Train Loss: 0.0692, Test Loss: 0.0709\n",
            "Epoch [2385/6000], Train Loss: 0.0692, Test Loss: 0.0710\n",
            "Epoch [2386/6000], Train Loss: 0.0691, Test Loss: 0.0709\n",
            "Epoch [2387/6000], Train Loss: 0.0690, Test Loss: 0.0708\n",
            "Epoch [2388/6000], Train Loss: 0.0690, Test Loss: 0.0707\n",
            "Epoch [2389/6000], Train Loss: 0.0689, Test Loss: 0.0708\n",
            "Epoch [2390/6000], Train Loss: 0.0688, Test Loss: 0.0707\n",
            "Epoch [2391/6000], Train Loss: 0.0688, Test Loss: 0.0706\n",
            "Epoch [2392/6000], Train Loss: 0.0687, Test Loss: 0.0705\n",
            "Epoch [2393/6000], Train Loss: 0.0687, Test Loss: 0.0706\n",
            "Epoch [2394/6000], Train Loss: 0.0686, Test Loss: 0.0706\n",
            "Epoch [2395/6000], Train Loss: 0.0685, Test Loss: 0.0705\n",
            "Epoch [2396/6000], Train Loss: 0.0685, Test Loss: 0.0705\n",
            "Epoch [2397/6000], Train Loss: 0.0684, Test Loss: 0.0703\n",
            "Epoch [2398/6000], Train Loss: 0.0684, Test Loss: 0.0704\n",
            "Epoch [2399/6000], Train Loss: 0.0683, Test Loss: 0.0704\n",
            "Epoch [2400/6000], Train Loss: 0.0682, Test Loss: 0.0703\n",
            "Epoch [2401/6000], Train Loss: 0.0682, Test Loss: 0.0703\n",
            "Epoch [2402/6000], Train Loss: 0.0681, Test Loss: 0.0703\n",
            "Epoch [2403/6000], Train Loss: 0.0681, Test Loss: 0.0703\n",
            "Epoch [2404/6000], Train Loss: 0.0680, Test Loss: 0.0703\n",
            "Epoch [2405/6000], Train Loss: 0.0679, Test Loss: 0.0703\n",
            "Epoch [2406/6000], Train Loss: 0.0679, Test Loss: 0.0703\n",
            "Epoch [2407/6000], Train Loss: 0.0678, Test Loss: 0.0702\n",
            "Epoch [2408/6000], Train Loss: 0.0677, Test Loss: 0.0701\n",
            "Epoch [2409/6000], Train Loss: 0.0677, Test Loss: 0.0699\n",
            "Epoch [2410/6000], Train Loss: 0.0676, Test Loss: 0.0699\n",
            "Epoch [2411/6000], Train Loss: 0.0676, Test Loss: 0.0698\n",
            "Epoch [2412/6000], Train Loss: 0.0675, Test Loss: 0.0698\n",
            "Epoch [2413/6000], Train Loss: 0.0675, Test Loss: 0.0699\n",
            "Epoch [2414/6000], Train Loss: 0.0674, Test Loss: 0.0699\n",
            "Epoch [2415/6000], Train Loss: 0.0673, Test Loss: 0.0698\n",
            "Epoch [2416/6000], Train Loss: 0.0673, Test Loss: 0.0697\n",
            "Epoch [2417/6000], Train Loss: 0.0672, Test Loss: 0.0697\n",
            "Epoch [2418/6000], Train Loss: 0.0671, Test Loss: 0.0696\n",
            "Epoch [2419/6000], Train Loss: 0.0671, Test Loss: 0.0696\n",
            "Epoch [2420/6000], Train Loss: 0.0670, Test Loss: 0.0696\n",
            "Epoch [2421/6000], Train Loss: 0.0670, Test Loss: 0.0696\n",
            "Epoch [2422/6000], Train Loss: 0.0669, Test Loss: 0.0695\n",
            "Epoch [2423/6000], Train Loss: 0.0668, Test Loss: 0.0695\n",
            "Epoch [2424/6000], Train Loss: 0.0668, Test Loss: 0.0694\n",
            "Epoch [2425/6000], Train Loss: 0.0668, Test Loss: 0.0695\n",
            "Epoch [2426/6000], Train Loss: 0.0667, Test Loss: 0.0694\n",
            "Epoch [2427/6000], Train Loss: 0.0666, Test Loss: 0.0693\n",
            "Epoch [2428/6000], Train Loss: 0.0666, Test Loss: 0.0693\n",
            "Epoch [2429/6000], Train Loss: 0.0665, Test Loss: 0.0693\n",
            "Epoch [2430/6000], Train Loss: 0.0664, Test Loss: 0.0692\n",
            "Epoch [2431/6000], Train Loss: 0.0664, Test Loss: 0.0691\n",
            "Epoch [2432/6000], Train Loss: 0.0663, Test Loss: 0.0690\n",
            "Epoch [2433/6000], Train Loss: 0.0663, Test Loss: 0.0690\n",
            "Epoch [2434/6000], Train Loss: 0.0662, Test Loss: 0.0691\n",
            "Epoch [2435/6000], Train Loss: 0.0661, Test Loss: 0.0691\n",
            "Epoch [2436/6000], Train Loss: 0.0661, Test Loss: 0.0691\n",
            "Epoch [2437/6000], Train Loss: 0.0660, Test Loss: 0.0690\n",
            "Epoch [2438/6000], Train Loss: 0.0660, Test Loss: 0.0689\n",
            "Epoch [2439/6000], Train Loss: 0.0659, Test Loss: 0.0689\n",
            "Epoch [2440/6000], Train Loss: 0.0659, Test Loss: 0.0687\n",
            "Epoch [2441/6000], Train Loss: 0.0658, Test Loss: 0.0687\n",
            "Epoch [2442/6000], Train Loss: 0.0657, Test Loss: 0.0687\n",
            "Epoch [2443/6000], Train Loss: 0.0657, Test Loss: 0.0687\n",
            "Epoch [2444/6000], Train Loss: 0.0656, Test Loss: 0.0687\n",
            "Epoch [2445/6000], Train Loss: 0.0656, Test Loss: 0.0686\n",
            "Epoch [2446/6000], Train Loss: 0.0655, Test Loss: 0.0685\n",
            "Epoch [2447/6000], Train Loss: 0.0655, Test Loss: 0.0684\n",
            "Epoch [2448/6000], Train Loss: 0.0654, Test Loss: 0.0685\n",
            "Epoch [2449/6000], Train Loss: 0.0653, Test Loss: 0.0686\n",
            "Epoch [2450/6000], Train Loss: 0.0653, Test Loss: 0.0685\n",
            "Epoch [2451/6000], Train Loss: 0.0652, Test Loss: 0.0685\n",
            "Epoch [2452/6000], Train Loss: 0.0652, Test Loss: 0.0684\n",
            "Epoch [2453/6000], Train Loss: 0.0651, Test Loss: 0.0683\n",
            "Epoch [2454/6000], Train Loss: 0.0650, Test Loss: 0.0683\n",
            "Epoch [2455/6000], Train Loss: 0.0650, Test Loss: 0.0683\n",
            "Epoch [2456/6000], Train Loss: 0.0649, Test Loss: 0.0682\n",
            "Epoch [2457/6000], Train Loss: 0.0649, Test Loss: 0.0682\n",
            "Epoch [2458/6000], Train Loss: 0.0648, Test Loss: 0.0681\n",
            "Epoch [2459/6000], Train Loss: 0.0648, Test Loss: 0.0681\n",
            "Epoch [2460/6000], Train Loss: 0.0647, Test Loss: 0.0681\n",
            "Epoch [2461/6000], Train Loss: 0.0646, Test Loss: 0.0680\n",
            "Epoch [2462/6000], Train Loss: 0.0646, Test Loss: 0.0679\n",
            "Epoch [2463/6000], Train Loss: 0.0645, Test Loss: 0.0679\n",
            "Epoch [2464/6000], Train Loss: 0.0645, Test Loss: 0.0679\n",
            "Epoch [2465/6000], Train Loss: 0.0644, Test Loss: 0.0678\n",
            "Epoch [2466/6000], Train Loss: 0.0643, Test Loss: 0.0678\n",
            "Epoch [2467/6000], Train Loss: 0.0643, Test Loss: 0.0678\n",
            "Epoch [2468/6000], Train Loss: 0.0642, Test Loss: 0.0677\n",
            "Epoch [2469/6000], Train Loss: 0.0642, Test Loss: 0.0677\n",
            "Epoch [2470/6000], Train Loss: 0.0641, Test Loss: 0.0677\n",
            "Epoch [2471/6000], Train Loss: 0.0641, Test Loss: 0.0677\n",
            "Epoch [2472/6000], Train Loss: 0.0640, Test Loss: 0.0677\n",
            "Epoch [2473/6000], Train Loss: 0.0639, Test Loss: 0.0676\n",
            "Epoch [2474/6000], Train Loss: 0.0639, Test Loss: 0.0675\n",
            "Epoch [2475/6000], Train Loss: 0.0638, Test Loss: 0.0675\n",
            "Epoch [2476/6000], Train Loss: 0.0638, Test Loss: 0.0675\n",
            "Epoch [2477/6000], Train Loss: 0.0637, Test Loss: 0.0676\n",
            "Epoch [2478/6000], Train Loss: 0.0637, Test Loss: 0.0676\n",
            "Epoch [2479/6000], Train Loss: 0.0636, Test Loss: 0.0675\n",
            "Epoch [2480/6000], Train Loss: 0.0636, Test Loss: 0.0674\n",
            "Epoch [2481/6000], Train Loss: 0.0635, Test Loss: 0.0673\n",
            "Epoch [2482/6000], Train Loss: 0.0634, Test Loss: 0.0671\n",
            "Epoch [2483/6000], Train Loss: 0.0634, Test Loss: 0.0671\n",
            "Epoch [2484/6000], Train Loss: 0.0633, Test Loss: 0.0671\n",
            "Epoch [2485/6000], Train Loss: 0.0633, Test Loss: 0.0671\n",
            "Epoch [2486/6000], Train Loss: 0.0632, Test Loss: 0.0671\n",
            "Epoch [2487/6000], Train Loss: 0.0632, Test Loss: 0.0670\n",
            "Epoch [2488/6000], Train Loss: 0.0631, Test Loss: 0.0670\n",
            "Epoch [2489/6000], Train Loss: 0.0630, Test Loss: 0.0670\n",
            "Epoch [2490/6000], Train Loss: 0.0630, Test Loss: 0.0670\n",
            "Epoch [2491/6000], Train Loss: 0.0629, Test Loss: 0.0671\n",
            "Epoch [2492/6000], Train Loss: 0.0629, Test Loss: 0.0669\n",
            "Epoch [2493/6000], Train Loss: 0.0628, Test Loss: 0.0668\n",
            "Epoch [2494/6000], Train Loss: 0.0628, Test Loss: 0.0667\n",
            "Epoch [2495/6000], Train Loss: 0.0627, Test Loss: 0.0668\n",
            "Epoch [2496/6000], Train Loss: 0.0626, Test Loss: 0.0667\n",
            "Epoch [2497/6000], Train Loss: 0.0626, Test Loss: 0.0667\n",
            "Epoch [2498/6000], Train Loss: 0.0625, Test Loss: 0.0667\n",
            "Epoch [2499/6000], Train Loss: 0.0625, Test Loss: 0.0666\n",
            "Epoch [2500/6000], Train Loss: 0.0624, Test Loss: 0.0666\n",
            "Epoch [2501/6000], Train Loss: 0.0624, Test Loss: 0.0665\n",
            "Epoch [2502/6000], Train Loss: 0.0623, Test Loss: 0.0665\n",
            "Epoch [2503/6000], Train Loss: 0.0623, Test Loss: 0.0665\n",
            "Epoch [2504/6000], Train Loss: 0.0622, Test Loss: 0.0665\n",
            "Epoch [2505/6000], Train Loss: 0.0621, Test Loss: 0.0664\n",
            "Epoch [2506/6000], Train Loss: 0.0621, Test Loss: 0.0664\n",
            "Epoch [2507/6000], Train Loss: 0.0620, Test Loss: 0.0664\n",
            "Epoch [2508/6000], Train Loss: 0.0620, Test Loss: 0.0662\n",
            "Epoch [2509/6000], Train Loss: 0.0619, Test Loss: 0.0662\n",
            "Epoch [2510/6000], Train Loss: 0.0619, Test Loss: 0.0662\n",
            "Epoch [2511/6000], Train Loss: 0.0618, Test Loss: 0.0662\n",
            "Epoch [2512/6000], Train Loss: 0.0618, Test Loss: 0.0662\n",
            "Epoch [2513/6000], Train Loss: 0.0617, Test Loss: 0.0661\n",
            "Epoch [2514/6000], Train Loss: 0.0617, Test Loss: 0.0659\n",
            "Epoch [2515/6000], Train Loss: 0.0616, Test Loss: 0.0658\n",
            "Epoch [2516/6000], Train Loss: 0.0616, Test Loss: 0.0657\n",
            "Epoch [2517/6000], Train Loss: 0.0615, Test Loss: 0.0658\n",
            "Epoch [2518/6000], Train Loss: 0.0615, Test Loss: 0.0659\n",
            "Epoch [2519/6000], Train Loss: 0.0614, Test Loss: 0.0659\n",
            "Epoch [2520/6000], Train Loss: 0.0613, Test Loss: 0.0658\n",
            "Epoch [2521/6000], Train Loss: 0.0613, Test Loss: 0.0657\n",
            "Epoch [2522/6000], Train Loss: 0.0612, Test Loss: 0.0656\n",
            "Epoch [2523/6000], Train Loss: 0.0612, Test Loss: 0.0655\n",
            "Epoch [2524/6000], Train Loss: 0.0611, Test Loss: 0.0655\n",
            "Epoch [2525/6000], Train Loss: 0.0611, Test Loss: 0.0656\n",
            "Epoch [2526/6000], Train Loss: 0.0610, Test Loss: 0.0655\n",
            "Epoch [2527/6000], Train Loss: 0.0610, Test Loss: 0.0655\n",
            "Epoch [2528/6000], Train Loss: 0.0609, Test Loss: 0.0655\n",
            "Epoch [2529/6000], Train Loss: 0.0609, Test Loss: 0.0655\n",
            "Epoch [2530/6000], Train Loss: 0.0608, Test Loss: 0.0655\n",
            "Epoch [2531/6000], Train Loss: 0.0608, Test Loss: 0.0655\n",
            "Epoch [2532/6000], Train Loss: 0.0607, Test Loss: 0.0654\n",
            "Epoch [2533/6000], Train Loss: 0.0606, Test Loss: 0.0654\n",
            "Epoch [2534/6000], Train Loss: 0.0606, Test Loss: 0.0653\n",
            "Epoch [2535/6000], Train Loss: 0.0605, Test Loss: 0.0652\n",
            "Epoch [2536/6000], Train Loss: 0.0605, Test Loss: 0.0652\n",
            "Epoch [2537/6000], Train Loss: 0.0604, Test Loss: 0.0652\n",
            "Epoch [2538/6000], Train Loss: 0.0604, Test Loss: 0.0651\n",
            "Epoch [2539/6000], Train Loss: 0.0603, Test Loss: 0.0651\n",
            "Epoch [2540/6000], Train Loss: 0.0603, Test Loss: 0.0651\n",
            "Epoch [2541/6000], Train Loss: 0.0602, Test Loss: 0.0650\n",
            "Epoch [2542/6000], Train Loss: 0.0601, Test Loss: 0.0650\n",
            "Epoch [2543/6000], Train Loss: 0.0601, Test Loss: 0.0649\n",
            "Epoch [2544/6000], Train Loss: 0.0600, Test Loss: 0.0648\n",
            "Epoch [2545/6000], Train Loss: 0.0600, Test Loss: 0.0647\n",
            "Epoch [2546/6000], Train Loss: 0.0599, Test Loss: 0.0648\n",
            "Epoch [2547/6000], Train Loss: 0.0599, Test Loss: 0.0649\n",
            "Epoch [2548/6000], Train Loss: 0.0598, Test Loss: 0.0648\n",
            "Epoch [2549/6000], Train Loss: 0.0598, Test Loss: 0.0648\n",
            "Epoch [2550/6000], Train Loss: 0.0597, Test Loss: 0.0647\n",
            "Epoch [2551/6000], Train Loss: 0.0597, Test Loss: 0.0645\n",
            "Epoch [2552/6000], Train Loss: 0.0596, Test Loss: 0.0645\n",
            "Epoch [2553/6000], Train Loss: 0.0596, Test Loss: 0.0646\n",
            "Epoch [2554/6000], Train Loss: 0.0595, Test Loss: 0.0646\n",
            "Epoch [2555/6000], Train Loss: 0.0595, Test Loss: 0.0645\n",
            "Epoch [2556/6000], Train Loss: 0.0594, Test Loss: 0.0644\n",
            "Epoch [2557/6000], Train Loss: 0.0594, Test Loss: 0.0643\n",
            "Epoch [2558/6000], Train Loss: 0.0593, Test Loss: 0.0643\n",
            "Epoch [2559/6000], Train Loss: 0.0593, Test Loss: 0.0642\n",
            "Epoch [2560/6000], Train Loss: 0.0592, Test Loss: 0.0642\n",
            "Epoch [2561/6000], Train Loss: 0.0592, Test Loss: 0.0643\n",
            "Epoch [2562/6000], Train Loss: 0.0591, Test Loss: 0.0643\n",
            "Epoch [2563/6000], Train Loss: 0.0590, Test Loss: 0.0642\n",
            "Epoch [2564/6000], Train Loss: 0.0590, Test Loss: 0.0641\n",
            "Epoch [2565/6000], Train Loss: 0.0589, Test Loss: 0.0641\n",
            "Epoch [2566/6000], Train Loss: 0.0589, Test Loss: 0.0640\n",
            "Epoch [2567/6000], Train Loss: 0.0588, Test Loss: 0.0639\n",
            "Epoch [2568/6000], Train Loss: 0.0588, Test Loss: 0.0639\n",
            "Epoch [2569/6000], Train Loss: 0.0587, Test Loss: 0.0639\n",
            "Epoch [2570/6000], Train Loss: 0.0587, Test Loss: 0.0639\n",
            "Epoch [2571/6000], Train Loss: 0.0586, Test Loss: 0.0639\n",
            "Epoch [2572/6000], Train Loss: 0.0586, Test Loss: 0.0639\n",
            "Epoch [2573/6000], Train Loss: 0.0585, Test Loss: 0.0638\n",
            "Epoch [2574/6000], Train Loss: 0.0585, Test Loss: 0.0637\n",
            "Epoch [2575/6000], Train Loss: 0.0584, Test Loss: 0.0637\n",
            "Epoch [2576/6000], Train Loss: 0.0584, Test Loss: 0.0637\n",
            "Epoch [2577/6000], Train Loss: 0.0583, Test Loss: 0.0637\n",
            "Epoch [2578/6000], Train Loss: 0.0583, Test Loss: 0.0637\n",
            "Epoch [2579/6000], Train Loss: 0.0582, Test Loss: 0.0636\n",
            "Epoch [2580/6000], Train Loss: 0.0582, Test Loss: 0.0635\n",
            "Epoch [2581/6000], Train Loss: 0.0581, Test Loss: 0.0634\n",
            "Epoch [2582/6000], Train Loss: 0.0581, Test Loss: 0.0635\n",
            "Epoch [2583/6000], Train Loss: 0.0580, Test Loss: 0.0635\n",
            "Epoch [2584/6000], Train Loss: 0.0580, Test Loss: 0.0634\n",
            "Epoch [2585/6000], Train Loss: 0.0579, Test Loss: 0.0633\n",
            "Epoch [2586/6000], Train Loss: 0.0579, Test Loss: 0.0633\n",
            "Epoch [2587/6000], Train Loss: 0.0578, Test Loss: 0.0633\n",
            "Epoch [2588/6000], Train Loss: 0.0577, Test Loss: 0.0632\n",
            "Epoch [2589/6000], Train Loss: 0.0577, Test Loss: 0.0632\n",
            "Epoch [2590/6000], Train Loss: 0.0576, Test Loss: 0.0631\n",
            "Epoch [2591/6000], Train Loss: 0.0576, Test Loss: 0.0631\n",
            "Epoch [2592/6000], Train Loss: 0.0575, Test Loss: 0.0631\n",
            "Epoch [2593/6000], Train Loss: 0.0575, Test Loss: 0.0631\n",
            "Epoch [2594/6000], Train Loss: 0.0575, Test Loss: 0.0631\n",
            "Epoch [2595/6000], Train Loss: 0.0574, Test Loss: 0.0630\n",
            "Epoch [2596/6000], Train Loss: 0.0573, Test Loss: 0.0630\n",
            "Epoch [2597/6000], Train Loss: 0.0573, Test Loss: 0.0630\n",
            "Epoch [2598/6000], Train Loss: 0.0572, Test Loss: 0.0629\n",
            "Epoch [2599/6000], Train Loss: 0.0572, Test Loss: 0.0629\n",
            "Epoch [2600/6000], Train Loss: 0.0571, Test Loss: 0.0628\n",
            "Epoch [2601/6000], Train Loss: 0.0571, Test Loss: 0.0627\n",
            "Epoch [2602/6000], Train Loss: 0.0570, Test Loss: 0.0627\n",
            "Epoch [2603/6000], Train Loss: 0.0570, Test Loss: 0.0627\n",
            "Epoch [2604/6000], Train Loss: 0.0569, Test Loss: 0.0627\n",
            "Epoch [2605/6000], Train Loss: 0.0569, Test Loss: 0.0627\n",
            "Epoch [2606/6000], Train Loss: 0.0568, Test Loss: 0.0626\n",
            "Epoch [2607/6000], Train Loss: 0.0568, Test Loss: 0.0626\n",
            "Epoch [2608/6000], Train Loss: 0.0567, Test Loss: 0.0625\n",
            "Epoch [2609/6000], Train Loss: 0.0567, Test Loss: 0.0625\n",
            "Epoch [2610/6000], Train Loss: 0.0566, Test Loss: 0.0625\n",
            "Epoch [2611/6000], Train Loss: 0.0566, Test Loss: 0.0624\n",
            "Epoch [2612/6000], Train Loss: 0.0565, Test Loss: 0.0625\n",
            "Epoch [2613/6000], Train Loss: 0.0565, Test Loss: 0.0624\n",
            "Epoch [2614/6000], Train Loss: 0.0564, Test Loss: 0.0623\n",
            "Epoch [2615/6000], Train Loss: 0.0564, Test Loss: 0.0623\n",
            "Epoch [2616/6000], Train Loss: 0.0563, Test Loss: 0.0622\n",
            "Epoch [2617/6000], Train Loss: 0.0563, Test Loss: 0.0622\n",
            "Epoch [2618/6000], Train Loss: 0.0562, Test Loss: 0.0622\n",
            "Epoch [2619/6000], Train Loss: 0.0562, Test Loss: 0.0621\n",
            "Epoch [2620/6000], Train Loss: 0.0561, Test Loss: 0.0620\n",
            "Epoch [2621/6000], Train Loss: 0.0561, Test Loss: 0.0620\n",
            "Epoch [2622/6000], Train Loss: 0.0560, Test Loss: 0.0620\n",
            "Epoch [2623/6000], Train Loss: 0.0560, Test Loss: 0.0620\n",
            "Epoch [2624/6000], Train Loss: 0.0559, Test Loss: 0.0619\n",
            "Epoch [2625/6000], Train Loss: 0.0559, Test Loss: 0.0619\n",
            "Epoch [2626/6000], Train Loss: 0.0558, Test Loss: 0.0619\n",
            "Epoch [2627/6000], Train Loss: 0.0558, Test Loss: 0.0619\n",
            "Epoch [2628/6000], Train Loss: 0.0557, Test Loss: 0.0618\n",
            "Epoch [2629/6000], Train Loss: 0.0557, Test Loss: 0.0618\n",
            "Epoch [2630/6000], Train Loss: 0.0556, Test Loss: 0.0618\n",
            "Epoch [2631/6000], Train Loss: 0.0556, Test Loss: 0.0617\n",
            "Epoch [2632/6000], Train Loss: 0.0555, Test Loss: 0.0618\n",
            "Epoch [2633/6000], Train Loss: 0.0555, Test Loss: 0.0617\n",
            "Epoch [2634/6000], Train Loss: 0.0554, Test Loss: 0.0616\n",
            "Epoch [2635/6000], Train Loss: 0.0554, Test Loss: 0.0615\n",
            "Epoch [2636/6000], Train Loss: 0.0553, Test Loss: 0.0615\n",
            "Epoch [2637/6000], Train Loss: 0.0553, Test Loss: 0.0615\n",
            "Epoch [2638/6000], Train Loss: 0.0552, Test Loss: 0.0614\n",
            "Epoch [2639/6000], Train Loss: 0.0552, Test Loss: 0.0614\n",
            "Epoch [2640/6000], Train Loss: 0.0552, Test Loss: 0.0614\n",
            "Epoch [2641/6000], Train Loss: 0.0551, Test Loss: 0.0614\n",
            "Epoch [2642/6000], Train Loss: 0.0550, Test Loss: 0.0613\n",
            "Epoch [2643/6000], Train Loss: 0.0550, Test Loss: 0.0612\n",
            "Epoch [2644/6000], Train Loss: 0.0550, Test Loss: 0.0613\n",
            "Epoch [2645/6000], Train Loss: 0.0549, Test Loss: 0.0613\n",
            "Epoch [2646/6000], Train Loss: 0.0549, Test Loss: 0.0612\n",
            "Epoch [2647/6000], Train Loss: 0.0548, Test Loss: 0.0610\n",
            "Epoch [2648/6000], Train Loss: 0.0548, Test Loss: 0.0610\n",
            "Epoch [2649/6000], Train Loss: 0.0547, Test Loss: 0.0610\n",
            "Epoch [2650/6000], Train Loss: 0.0547, Test Loss: 0.0611\n",
            "Epoch [2651/6000], Train Loss: 0.0546, Test Loss: 0.0610\n",
            "Epoch [2652/6000], Train Loss: 0.0546, Test Loss: 0.0610\n",
            "Epoch [2653/6000], Train Loss: 0.0545, Test Loss: 0.0609\n",
            "Epoch [2654/6000], Train Loss: 0.0545, Test Loss: 0.0608\n",
            "Epoch [2655/6000], Train Loss: 0.0544, Test Loss: 0.0608\n",
            "Epoch [2656/6000], Train Loss: 0.0544, Test Loss: 0.0608\n",
            "Epoch [2657/6000], Train Loss: 0.0543, Test Loss: 0.0608\n",
            "Epoch [2658/6000], Train Loss: 0.0543, Test Loss: 0.0608\n",
            "Epoch [2659/6000], Train Loss: 0.0542, Test Loss: 0.0608\n",
            "Epoch [2660/6000], Train Loss: 0.0542, Test Loss: 0.0607\n",
            "Epoch [2661/6000], Train Loss: 0.0541, Test Loss: 0.0607\n",
            "Epoch [2662/6000], Train Loss: 0.0541, Test Loss: 0.0606\n",
            "Epoch [2663/6000], Train Loss: 0.0540, Test Loss: 0.0604\n",
            "Epoch [2664/6000], Train Loss: 0.0540, Test Loss: 0.0604\n",
            "Epoch [2665/6000], Train Loss: 0.0539, Test Loss: 0.0604\n",
            "Epoch [2666/6000], Train Loss: 0.0539, Test Loss: 0.0604\n",
            "Epoch [2667/6000], Train Loss: 0.0538, Test Loss: 0.0605\n",
            "Epoch [2668/6000], Train Loss: 0.0538, Test Loss: 0.0605\n",
            "Epoch [2669/6000], Train Loss: 0.0538, Test Loss: 0.0604\n",
            "Epoch [2670/6000], Train Loss: 0.0537, Test Loss: 0.0603\n",
            "Epoch [2671/6000], Train Loss: 0.0537, Test Loss: 0.0602\n",
            "Epoch [2672/6000], Train Loss: 0.0536, Test Loss: 0.0601\n",
            "Epoch [2673/6000], Train Loss: 0.0536, Test Loss: 0.0600\n",
            "Epoch [2674/6000], Train Loss: 0.0535, Test Loss: 0.0599\n",
            "Epoch [2675/6000], Train Loss: 0.0535, Test Loss: 0.0599\n",
            "Epoch [2676/6000], Train Loss: 0.0534, Test Loss: 0.0599\n",
            "Epoch [2677/6000], Train Loss: 0.0534, Test Loss: 0.0599\n",
            "Epoch [2678/6000], Train Loss: 0.0533, Test Loss: 0.0599\n",
            "Epoch [2679/6000], Train Loss: 0.0533, Test Loss: 0.0600\n",
            "Epoch [2680/6000], Train Loss: 0.0532, Test Loss: 0.0599\n",
            "Epoch [2681/6000], Train Loss: 0.0532, Test Loss: 0.0598\n",
            "Epoch [2682/6000], Train Loss: 0.0531, Test Loss: 0.0598\n",
            "Epoch [2683/6000], Train Loss: 0.0531, Test Loss: 0.0598\n",
            "Epoch [2684/6000], Train Loss: 0.0530, Test Loss: 0.0598\n",
            "Epoch [2685/6000], Train Loss: 0.0530, Test Loss: 0.0598\n",
            "Epoch [2686/6000], Train Loss: 0.0529, Test Loss: 0.0597\n",
            "Epoch [2687/6000], Train Loss: 0.0529, Test Loss: 0.0596\n",
            "Epoch [2688/6000], Train Loss: 0.0529, Test Loss: 0.0597\n",
            "Epoch [2689/6000], Train Loss: 0.0528, Test Loss: 0.0596\n",
            "Epoch [2690/6000], Train Loss: 0.0527, Test Loss: 0.0595\n",
            "Epoch [2691/6000], Train Loss: 0.0527, Test Loss: 0.0595\n",
            "Epoch [2692/6000], Train Loss: 0.0527, Test Loss: 0.0594\n",
            "Epoch [2693/6000], Train Loss: 0.0526, Test Loss: 0.0594\n",
            "Epoch [2694/6000], Train Loss: 0.0526, Test Loss: 0.0594\n",
            "Epoch [2695/6000], Train Loss: 0.0525, Test Loss: 0.0593\n",
            "Epoch [2696/6000], Train Loss: 0.0525, Test Loss: 0.0593\n",
            "Epoch [2697/6000], Train Loss: 0.0524, Test Loss: 0.0592\n",
            "Epoch [2698/6000], Train Loss: 0.0524, Test Loss: 0.0591\n",
            "Epoch [2699/6000], Train Loss: 0.0523, Test Loss: 0.0592\n",
            "Epoch [2700/6000], Train Loss: 0.0523, Test Loss: 0.0591\n",
            "Epoch [2701/6000], Train Loss: 0.0522, Test Loss: 0.0591\n",
            "Epoch [2702/6000], Train Loss: 0.0522, Test Loss: 0.0591\n",
            "Epoch [2703/6000], Train Loss: 0.0521, Test Loss: 0.0590\n",
            "Epoch [2704/6000], Train Loss: 0.0521, Test Loss: 0.0590\n",
            "Epoch [2705/6000], Train Loss: 0.0521, Test Loss: 0.0589\n",
            "Epoch [2706/6000], Train Loss: 0.0520, Test Loss: 0.0589\n",
            "Epoch [2707/6000], Train Loss: 0.0520, Test Loss: 0.0589\n",
            "Epoch [2708/6000], Train Loss: 0.0519, Test Loss: 0.0589\n",
            "Epoch [2709/6000], Train Loss: 0.0519, Test Loss: 0.0588\n",
            "Epoch [2710/6000], Train Loss: 0.0518, Test Loss: 0.0589\n",
            "Epoch [2711/6000], Train Loss: 0.0518, Test Loss: 0.0589\n",
            "Epoch [2712/6000], Train Loss: 0.0517, Test Loss: 0.0588\n",
            "Epoch [2713/6000], Train Loss: 0.0517, Test Loss: 0.0587\n",
            "Epoch [2714/6000], Train Loss: 0.0516, Test Loss: 0.0586\n",
            "Epoch [2715/6000], Train Loss: 0.0516, Test Loss: 0.0585\n",
            "Epoch [2716/6000], Train Loss: 0.0516, Test Loss: 0.0584\n",
            "Epoch [2717/6000], Train Loss: 0.0515, Test Loss: 0.0585\n",
            "Epoch [2718/6000], Train Loss: 0.0515, Test Loss: 0.0585\n",
            "Epoch [2719/6000], Train Loss: 0.0514, Test Loss: 0.0586\n",
            "Epoch [2720/6000], Train Loss: 0.0514, Test Loss: 0.0587\n",
            "Epoch [2721/6000], Train Loss: 0.0513, Test Loss: 0.0587\n",
            "Epoch [2722/6000], Train Loss: 0.0513, Test Loss: 0.0586\n",
            "Epoch [2723/6000], Train Loss: 0.0512, Test Loss: 0.0586\n",
            "Epoch [2724/6000], Train Loss: 0.0512, Test Loss: 0.0585\n",
            "Epoch [2725/6000], Train Loss: 0.0511, Test Loss: 0.0583\n",
            "Epoch [2726/6000], Train Loss: 0.0511, Test Loss: 0.0583\n",
            "Epoch [2727/6000], Train Loss: 0.0511, Test Loss: 0.0583\n",
            "Epoch [2728/6000], Train Loss: 0.0510, Test Loss: 0.0583\n",
            "Epoch [2729/6000], Train Loss: 0.0510, Test Loss: 0.0583\n",
            "Epoch [2730/6000], Train Loss: 0.0509, Test Loss: 0.0582\n",
            "Epoch [2731/6000], Train Loss: 0.0509, Test Loss: 0.0581\n",
            "Epoch [2732/6000], Train Loss: 0.0508, Test Loss: 0.0580\n",
            "Epoch [2733/6000], Train Loss: 0.0508, Test Loss: 0.0580\n",
            "Epoch [2734/6000], Train Loss: 0.0507, Test Loss: 0.0580\n",
            "Epoch [2735/6000], Train Loss: 0.0507, Test Loss: 0.0580\n",
            "Epoch [2736/6000], Train Loss: 0.0507, Test Loss: 0.0580\n",
            "Epoch [2737/6000], Train Loss: 0.0506, Test Loss: 0.0580\n",
            "Epoch [2738/6000], Train Loss: 0.0506, Test Loss: 0.0578\n",
            "Epoch [2739/6000], Train Loss: 0.0505, Test Loss: 0.0578\n",
            "Epoch [2740/6000], Train Loss: 0.0505, Test Loss: 0.0578\n",
            "Epoch [2741/6000], Train Loss: 0.0504, Test Loss: 0.0578\n",
            "Epoch [2742/6000], Train Loss: 0.0504, Test Loss: 0.0578\n",
            "Epoch [2743/6000], Train Loss: 0.0503, Test Loss: 0.0577\n",
            "Epoch [2744/6000], Train Loss: 0.0503, Test Loss: 0.0576\n",
            "Epoch [2745/6000], Train Loss: 0.0502, Test Loss: 0.0576\n",
            "Epoch [2746/6000], Train Loss: 0.0502, Test Loss: 0.0575\n",
            "Epoch [2747/6000], Train Loss: 0.0502, Test Loss: 0.0576\n",
            "Epoch [2748/6000], Train Loss: 0.0501, Test Loss: 0.0576\n",
            "Epoch [2749/6000], Train Loss: 0.0501, Test Loss: 0.0575\n",
            "Epoch [2750/6000], Train Loss: 0.0500, Test Loss: 0.0574\n",
            "Epoch [2751/6000], Train Loss: 0.0500, Test Loss: 0.0574\n",
            "Epoch [2752/6000], Train Loss: 0.0499, Test Loss: 0.0573\n",
            "Epoch [2753/6000], Train Loss: 0.0499, Test Loss: 0.0574\n",
            "Epoch [2754/6000], Train Loss: 0.0498, Test Loss: 0.0574\n",
            "Epoch [2755/6000], Train Loss: 0.0498, Test Loss: 0.0574\n",
            "Epoch [2756/6000], Train Loss: 0.0498, Test Loss: 0.0573\n",
            "Epoch [2757/6000], Train Loss: 0.0497, Test Loss: 0.0571\n",
            "Epoch [2758/6000], Train Loss: 0.0497, Test Loss: 0.0570\n",
            "Epoch [2759/6000], Train Loss: 0.0496, Test Loss: 0.0570\n",
            "Epoch [2760/6000], Train Loss: 0.0496, Test Loss: 0.0571\n",
            "Epoch [2761/6000], Train Loss: 0.0495, Test Loss: 0.0572\n",
            "Epoch [2762/6000], Train Loss: 0.0495, Test Loss: 0.0571\n",
            "Epoch [2763/6000], Train Loss: 0.0495, Test Loss: 0.0570\n",
            "Epoch [2764/6000], Train Loss: 0.0494, Test Loss: 0.0570\n",
            "Epoch [2765/6000], Train Loss: 0.0494, Test Loss: 0.0570\n",
            "Epoch [2766/6000], Train Loss: 0.0493, Test Loss: 0.0569\n",
            "Epoch [2767/6000], Train Loss: 0.0493, Test Loss: 0.0568\n",
            "Epoch [2768/6000], Train Loss: 0.0492, Test Loss: 0.0568\n",
            "Epoch [2769/6000], Train Loss: 0.0492, Test Loss: 0.0568\n",
            "Epoch [2770/6000], Train Loss: 0.0491, Test Loss: 0.0568\n",
            "Epoch [2771/6000], Train Loss: 0.0491, Test Loss: 0.0567\n",
            "Epoch [2772/6000], Train Loss: 0.0491, Test Loss: 0.0567\n",
            "Epoch [2773/6000], Train Loss: 0.0490, Test Loss: 0.0565\n",
            "Epoch [2774/6000], Train Loss: 0.0490, Test Loss: 0.0565\n",
            "Epoch [2775/6000], Train Loss: 0.0489, Test Loss: 0.0565\n",
            "Epoch [2776/6000], Train Loss: 0.0489, Test Loss: 0.0565\n",
            "Epoch [2777/6000], Train Loss: 0.0488, Test Loss: 0.0565\n",
            "Epoch [2778/6000], Train Loss: 0.0488, Test Loss: 0.0565\n",
            "Epoch [2779/6000], Train Loss: 0.0488, Test Loss: 0.0564\n",
            "Epoch [2780/6000], Train Loss: 0.0487, Test Loss: 0.0564\n",
            "Epoch [2781/6000], Train Loss: 0.0487, Test Loss: 0.0564\n",
            "Epoch [2782/6000], Train Loss: 0.0486, Test Loss: 0.0564\n",
            "Epoch [2783/6000], Train Loss: 0.0486, Test Loss: 0.0563\n",
            "Epoch [2784/6000], Train Loss: 0.0485, Test Loss: 0.0563\n",
            "Epoch [2785/6000], Train Loss: 0.0485, Test Loss: 0.0563\n",
            "Epoch [2786/6000], Train Loss: 0.0485, Test Loss: 0.0562\n",
            "Epoch [2787/6000], Train Loss: 0.0484, Test Loss: 0.0562\n",
            "Epoch [2788/6000], Train Loss: 0.0484, Test Loss: 0.0561\n",
            "Epoch [2789/6000], Train Loss: 0.0483, Test Loss: 0.0561\n",
            "Epoch [2790/6000], Train Loss: 0.0483, Test Loss: 0.0560\n",
            "Epoch [2791/6000], Train Loss: 0.0483, Test Loss: 0.0561\n",
            "Epoch [2792/6000], Train Loss: 0.0482, Test Loss: 0.0561\n",
            "Epoch [2793/6000], Train Loss: 0.0482, Test Loss: 0.0560\n",
            "Epoch [2794/6000], Train Loss: 0.0481, Test Loss: 0.0560\n",
            "Epoch [2795/6000], Train Loss: 0.0481, Test Loss: 0.0559\n",
            "Epoch [2796/6000], Train Loss: 0.0480, Test Loss: 0.0559\n",
            "Epoch [2797/6000], Train Loss: 0.0480, Test Loss: 0.0559\n",
            "Epoch [2798/6000], Train Loss: 0.0479, Test Loss: 0.0558\n",
            "Epoch [2799/6000], Train Loss: 0.0479, Test Loss: 0.0558\n",
            "Epoch [2800/6000], Train Loss: 0.0479, Test Loss: 0.0557\n",
            "Epoch [2801/6000], Train Loss: 0.0478, Test Loss: 0.0557\n",
            "Epoch [2802/6000], Train Loss: 0.0478, Test Loss: 0.0557\n",
            "Epoch [2803/6000], Train Loss: 0.0477, Test Loss: 0.0557\n",
            "Epoch [2804/6000], Train Loss: 0.0477, Test Loss: 0.0556\n",
            "Epoch [2805/6000], Train Loss: 0.0476, Test Loss: 0.0556\n",
            "Epoch [2806/6000], Train Loss: 0.0476, Test Loss: 0.0555\n",
            "Epoch [2807/6000], Train Loss: 0.0476, Test Loss: 0.0555\n",
            "Epoch [2808/6000], Train Loss: 0.0475, Test Loss: 0.0554\n",
            "Epoch [2809/6000], Train Loss: 0.0475, Test Loss: 0.0554\n",
            "Epoch [2810/6000], Train Loss: 0.0474, Test Loss: 0.0554\n",
            "Epoch [2811/6000], Train Loss: 0.0474, Test Loss: 0.0554\n",
            "Epoch [2812/6000], Train Loss: 0.0473, Test Loss: 0.0553\n",
            "Epoch [2813/6000], Train Loss: 0.0473, Test Loss: 0.0553\n",
            "Epoch [2814/6000], Train Loss: 0.0473, Test Loss: 0.0553\n",
            "Epoch [2815/6000], Train Loss: 0.0472, Test Loss: 0.0553\n",
            "Epoch [2816/6000], Train Loss: 0.0472, Test Loss: 0.0552\n",
            "Epoch [2817/6000], Train Loss: 0.0471, Test Loss: 0.0552\n",
            "Epoch [2818/6000], Train Loss: 0.0471, Test Loss: 0.0551\n",
            "Epoch [2819/6000], Train Loss: 0.0471, Test Loss: 0.0550\n",
            "Epoch [2820/6000], Train Loss: 0.0470, Test Loss: 0.0550\n",
            "Epoch [2821/6000], Train Loss: 0.0470, Test Loss: 0.0550\n",
            "Epoch [2822/6000], Train Loss: 0.0469, Test Loss: 0.0550\n",
            "Epoch [2823/6000], Train Loss: 0.0469, Test Loss: 0.0550\n",
            "Epoch [2824/6000], Train Loss: 0.0469, Test Loss: 0.0549\n",
            "Epoch [2825/6000], Train Loss: 0.0468, Test Loss: 0.0548\n",
            "Epoch [2826/6000], Train Loss: 0.0468, Test Loss: 0.0547\n",
            "Epoch [2827/6000], Train Loss: 0.0467, Test Loss: 0.0548\n",
            "Epoch [2828/6000], Train Loss: 0.0467, Test Loss: 0.0547\n",
            "Epoch [2829/6000], Train Loss: 0.0467, Test Loss: 0.0547\n",
            "Epoch [2830/6000], Train Loss: 0.0466, Test Loss: 0.0547\n",
            "Epoch [2831/6000], Train Loss: 0.0466, Test Loss: 0.0547\n",
            "Epoch [2832/6000], Train Loss: 0.0465, Test Loss: 0.0547\n",
            "Epoch [2833/6000], Train Loss: 0.0465, Test Loss: 0.0546\n",
            "Epoch [2834/6000], Train Loss: 0.0464, Test Loss: 0.0546\n",
            "Epoch [2835/6000], Train Loss: 0.0464, Test Loss: 0.0545\n",
            "Epoch [2836/6000], Train Loss: 0.0464, Test Loss: 0.0545\n",
            "Epoch [2837/6000], Train Loss: 0.0463, Test Loss: 0.0544\n",
            "Epoch [2838/6000], Train Loss: 0.0463, Test Loss: 0.0544\n",
            "Epoch [2839/6000], Train Loss: 0.0462, Test Loss: 0.0543\n",
            "Epoch [2840/6000], Train Loss: 0.0462, Test Loss: 0.0543\n",
            "Epoch [2841/6000], Train Loss: 0.0462, Test Loss: 0.0543\n",
            "Epoch [2842/6000], Train Loss: 0.0461, Test Loss: 0.0542\n",
            "Epoch [2843/6000], Train Loss: 0.0461, Test Loss: 0.0543\n",
            "Epoch [2844/6000], Train Loss: 0.0460, Test Loss: 0.0543\n",
            "Epoch [2845/6000], Train Loss: 0.0460, Test Loss: 0.0542\n",
            "Epoch [2846/6000], Train Loss: 0.0459, Test Loss: 0.0541\n",
            "Epoch [2847/6000], Train Loss: 0.0459, Test Loss: 0.0541\n",
            "Epoch [2848/6000], Train Loss: 0.0459, Test Loss: 0.0540\n",
            "Epoch [2849/6000], Train Loss: 0.0458, Test Loss: 0.0539\n",
            "Epoch [2850/6000], Train Loss: 0.0458, Test Loss: 0.0539\n",
            "Epoch [2851/6000], Train Loss: 0.0457, Test Loss: 0.0540\n",
            "Epoch [2852/6000], Train Loss: 0.0457, Test Loss: 0.0539\n",
            "Epoch [2853/6000], Train Loss: 0.0457, Test Loss: 0.0539\n",
            "Epoch [2854/6000], Train Loss: 0.0456, Test Loss: 0.0538\n",
            "Epoch [2855/6000], Train Loss: 0.0456, Test Loss: 0.0537\n",
            "Epoch [2856/6000], Train Loss: 0.0455, Test Loss: 0.0538\n",
            "Epoch [2857/6000], Train Loss: 0.0455, Test Loss: 0.0537\n",
            "Epoch [2858/6000], Train Loss: 0.0455, Test Loss: 0.0537\n",
            "Epoch [2859/6000], Train Loss: 0.0454, Test Loss: 0.0537\n",
            "Epoch [2860/6000], Train Loss: 0.0454, Test Loss: 0.0538\n",
            "Epoch [2861/6000], Train Loss: 0.0453, Test Loss: 0.0538\n",
            "Epoch [2862/6000], Train Loss: 0.0453, Test Loss: 0.0537\n",
            "Epoch [2863/6000], Train Loss: 0.0453, Test Loss: 0.0536\n",
            "Epoch [2864/6000], Train Loss: 0.0452, Test Loss: 0.0536\n",
            "Epoch [2865/6000], Train Loss: 0.0452, Test Loss: 0.0536\n",
            "Epoch [2866/6000], Train Loss: 0.0451, Test Loss: 0.0535\n",
            "Epoch [2867/6000], Train Loss: 0.0451, Test Loss: 0.0535\n",
            "Epoch [2868/6000], Train Loss: 0.0451, Test Loss: 0.0534\n",
            "Epoch [2869/6000], Train Loss: 0.0450, Test Loss: 0.0534\n",
            "Epoch [2870/6000], Train Loss: 0.0450, Test Loss: 0.0534\n",
            "Epoch [2871/6000], Train Loss: 0.0449, Test Loss: 0.0534\n",
            "Epoch [2872/6000], Train Loss: 0.0449, Test Loss: 0.0534\n",
            "Epoch [2873/6000], Train Loss: 0.0449, Test Loss: 0.0533\n",
            "Epoch [2874/6000], Train Loss: 0.0448, Test Loss: 0.0533\n",
            "Epoch [2875/6000], Train Loss: 0.0448, Test Loss: 0.0531\n",
            "Epoch [2876/6000], Train Loss: 0.0448, Test Loss: 0.0531\n",
            "Epoch [2877/6000], Train Loss: 0.0447, Test Loss: 0.0532\n",
            "Epoch [2878/6000], Train Loss: 0.0447, Test Loss: 0.0531\n",
            "Epoch [2879/6000], Train Loss: 0.0446, Test Loss: 0.0531\n",
            "Epoch [2880/6000], Train Loss: 0.0446, Test Loss: 0.0530\n",
            "Epoch [2881/6000], Train Loss: 0.0445, Test Loss: 0.0530\n",
            "Epoch [2882/6000], Train Loss: 0.0445, Test Loss: 0.0529\n",
            "Epoch [2883/6000], Train Loss: 0.0445, Test Loss: 0.0530\n",
            "Epoch [2884/6000], Train Loss: 0.0444, Test Loss: 0.0529\n",
            "Epoch [2885/6000], Train Loss: 0.0444, Test Loss: 0.0528\n",
            "Epoch [2886/6000], Train Loss: 0.0444, Test Loss: 0.0528\n",
            "Epoch [2887/6000], Train Loss: 0.0443, Test Loss: 0.0527\n",
            "Epoch [2888/6000], Train Loss: 0.0443, Test Loss: 0.0527\n",
            "Epoch [2889/6000], Train Loss: 0.0442, Test Loss: 0.0527\n",
            "Epoch [2890/6000], Train Loss: 0.0442, Test Loss: 0.0527\n",
            "Epoch [2891/6000], Train Loss: 0.0442, Test Loss: 0.0526\n",
            "Epoch [2892/6000], Train Loss: 0.0441, Test Loss: 0.0525\n",
            "Epoch [2893/6000], Train Loss: 0.0441, Test Loss: 0.0526\n",
            "Epoch [2894/6000], Train Loss: 0.0440, Test Loss: 0.0526\n",
            "Epoch [2895/6000], Train Loss: 0.0440, Test Loss: 0.0525\n",
            "Epoch [2896/6000], Train Loss: 0.0440, Test Loss: 0.0525\n",
            "Epoch [2897/6000], Train Loss: 0.0439, Test Loss: 0.0524\n",
            "Epoch [2898/6000], Train Loss: 0.0439, Test Loss: 0.0525\n",
            "Epoch [2899/6000], Train Loss: 0.0438, Test Loss: 0.0524\n",
            "Epoch [2900/6000], Train Loss: 0.0438, Test Loss: 0.0524\n",
            "Epoch [2901/6000], Train Loss: 0.0438, Test Loss: 0.0523\n",
            "Epoch [2902/6000], Train Loss: 0.0437, Test Loss: 0.0522\n",
            "Epoch [2903/6000], Train Loss: 0.0437, Test Loss: 0.0522\n",
            "Epoch [2904/6000], Train Loss: 0.0437, Test Loss: 0.0523\n",
            "Epoch [2905/6000], Train Loss: 0.0436, Test Loss: 0.0523\n",
            "Epoch [2906/6000], Train Loss: 0.0436, Test Loss: 0.0522\n",
            "Epoch [2907/6000], Train Loss: 0.0435, Test Loss: 0.0522\n",
            "Epoch [2908/6000], Train Loss: 0.0435, Test Loss: 0.0521\n",
            "Epoch [2909/6000], Train Loss: 0.0435, Test Loss: 0.0520\n",
            "Epoch [2910/6000], Train Loss: 0.0434, Test Loss: 0.0521\n",
            "Epoch [2911/6000], Train Loss: 0.0434, Test Loss: 0.0520\n",
            "Epoch [2912/6000], Train Loss: 0.0433, Test Loss: 0.0519\n",
            "Epoch [2913/6000], Train Loss: 0.0433, Test Loss: 0.0519\n",
            "Epoch [2914/6000], Train Loss: 0.0433, Test Loss: 0.0518\n",
            "Epoch [2915/6000], Train Loss: 0.0432, Test Loss: 0.0519\n",
            "Epoch [2916/6000], Train Loss: 0.0432, Test Loss: 0.0519\n",
            "Epoch [2917/6000], Train Loss: 0.0432, Test Loss: 0.0518\n",
            "Epoch [2918/6000], Train Loss: 0.0431, Test Loss: 0.0518\n",
            "Epoch [2919/6000], Train Loss: 0.0431, Test Loss: 0.0518\n",
            "Epoch [2920/6000], Train Loss: 0.0430, Test Loss: 0.0517\n",
            "Epoch [2921/6000], Train Loss: 0.0430, Test Loss: 0.0517\n",
            "Epoch [2922/6000], Train Loss: 0.0430, Test Loss: 0.0516\n",
            "Epoch [2923/6000], Train Loss: 0.0429, Test Loss: 0.0515\n",
            "Epoch [2924/6000], Train Loss: 0.0429, Test Loss: 0.0516\n",
            "Epoch [2925/6000], Train Loss: 0.0429, Test Loss: 0.0516\n",
            "Epoch [2926/6000], Train Loss: 0.0428, Test Loss: 0.0516\n",
            "Epoch [2927/6000], Train Loss: 0.0428, Test Loss: 0.0515\n",
            "Epoch [2928/6000], Train Loss: 0.0427, Test Loss: 0.0515\n",
            "Epoch [2929/6000], Train Loss: 0.0427, Test Loss: 0.0514\n",
            "Epoch [2930/6000], Train Loss: 0.0427, Test Loss: 0.0513\n",
            "Epoch [2931/6000], Train Loss: 0.0426, Test Loss: 0.0513\n",
            "Epoch [2932/6000], Train Loss: 0.0426, Test Loss: 0.0513\n",
            "Epoch [2933/6000], Train Loss: 0.0426, Test Loss: 0.0514\n",
            "Epoch [2934/6000], Train Loss: 0.0425, Test Loss: 0.0514\n",
            "Epoch [2935/6000], Train Loss: 0.0425, Test Loss: 0.0513\n",
            "Epoch [2936/6000], Train Loss: 0.0424, Test Loss: 0.0512\n",
            "Epoch [2937/6000], Train Loss: 0.0424, Test Loss: 0.0511\n",
            "Epoch [2938/6000], Train Loss: 0.0424, Test Loss: 0.0511\n",
            "Epoch [2939/6000], Train Loss: 0.0423, Test Loss: 0.0511\n",
            "Epoch [2940/6000], Train Loss: 0.0423, Test Loss: 0.0510\n",
            "Epoch [2941/6000], Train Loss: 0.0422, Test Loss: 0.0510\n",
            "Epoch [2942/6000], Train Loss: 0.0422, Test Loss: 0.0510\n",
            "Epoch [2943/6000], Train Loss: 0.0422, Test Loss: 0.0509\n",
            "Epoch [2944/6000], Train Loss: 0.0421, Test Loss: 0.0509\n",
            "Epoch [2945/6000], Train Loss: 0.0421, Test Loss: 0.0509\n",
            "Epoch [2946/6000], Train Loss: 0.0421, Test Loss: 0.0508\n",
            "Epoch [2947/6000], Train Loss: 0.0420, Test Loss: 0.0507\n",
            "Epoch [2948/6000], Train Loss: 0.0420, Test Loss: 0.0508\n",
            "Epoch [2949/6000], Train Loss: 0.0419, Test Loss: 0.0507\n",
            "Epoch [2950/6000], Train Loss: 0.0419, Test Loss: 0.0507\n",
            "Epoch [2951/6000], Train Loss: 0.0419, Test Loss: 0.0507\n",
            "Epoch [2952/6000], Train Loss: 0.0418, Test Loss: 0.0506\n",
            "Epoch [2953/6000], Train Loss: 0.0418, Test Loss: 0.0507\n",
            "Epoch [2954/6000], Train Loss: 0.0418, Test Loss: 0.0506\n",
            "Epoch [2955/6000], Train Loss: 0.0417, Test Loss: 0.0506\n",
            "Epoch [2956/6000], Train Loss: 0.0417, Test Loss: 0.0506\n",
            "Epoch [2957/6000], Train Loss: 0.0417, Test Loss: 0.0506\n",
            "Epoch [2958/6000], Train Loss: 0.0416, Test Loss: 0.0506\n",
            "Epoch [2959/6000], Train Loss: 0.0416, Test Loss: 0.0505\n",
            "Epoch [2960/6000], Train Loss: 0.0415, Test Loss: 0.0504\n",
            "Epoch [2961/6000], Train Loss: 0.0415, Test Loss: 0.0504\n",
            "Epoch [2962/6000], Train Loss: 0.0415, Test Loss: 0.0504\n",
            "Epoch [2963/6000], Train Loss: 0.0414, Test Loss: 0.0503\n",
            "Epoch [2964/6000], Train Loss: 0.0414, Test Loss: 0.0503\n",
            "Epoch [2965/6000], Train Loss: 0.0414, Test Loss: 0.0503\n",
            "Epoch [2966/6000], Train Loss: 0.0413, Test Loss: 0.0503\n",
            "Epoch [2967/6000], Train Loss: 0.0413, Test Loss: 0.0502\n",
            "Epoch [2968/6000], Train Loss: 0.0413, Test Loss: 0.0501\n",
            "Epoch [2969/6000], Train Loss: 0.0412, Test Loss: 0.0502\n",
            "Epoch [2970/6000], Train Loss: 0.0412, Test Loss: 0.0501\n",
            "Epoch [2971/6000], Train Loss: 0.0412, Test Loss: 0.0501\n",
            "Epoch [2972/6000], Train Loss: 0.0411, Test Loss: 0.0500\n",
            "Epoch [2973/6000], Train Loss: 0.0411, Test Loss: 0.0500\n",
            "Epoch [2974/6000], Train Loss: 0.0410, Test Loss: 0.0499\n",
            "Epoch [2975/6000], Train Loss: 0.0410, Test Loss: 0.0499\n",
            "Epoch [2976/6000], Train Loss: 0.0410, Test Loss: 0.0499\n",
            "Epoch [2977/6000], Train Loss: 0.0409, Test Loss: 0.0499\n",
            "Epoch [2978/6000], Train Loss: 0.0409, Test Loss: 0.0498\n",
            "Epoch [2979/6000], Train Loss: 0.0409, Test Loss: 0.0497\n",
            "Epoch [2980/6000], Train Loss: 0.0408, Test Loss: 0.0498\n",
            "Epoch [2981/6000], Train Loss: 0.0408, Test Loss: 0.0497\n",
            "Epoch [2982/6000], Train Loss: 0.0407, Test Loss: 0.0497\n",
            "Epoch [2983/6000], Train Loss: 0.0407, Test Loss: 0.0496\n",
            "Epoch [2984/6000], Train Loss: 0.0407, Test Loss: 0.0496\n",
            "Epoch [2985/6000], Train Loss: 0.0406, Test Loss: 0.0496\n",
            "Epoch [2986/6000], Train Loss: 0.0406, Test Loss: 0.0495\n",
            "Epoch [2987/6000], Train Loss: 0.0406, Test Loss: 0.0494\n",
            "Epoch [2988/6000], Train Loss: 0.0405, Test Loss: 0.0495\n",
            "Epoch [2989/6000], Train Loss: 0.0405, Test Loss: 0.0494\n",
            "Epoch [2990/6000], Train Loss: 0.0405, Test Loss: 0.0494\n",
            "Epoch [2991/6000], Train Loss: 0.0404, Test Loss: 0.0493\n",
            "Epoch [2992/6000], Train Loss: 0.0404, Test Loss: 0.0494\n",
            "Epoch [2993/6000], Train Loss: 0.0404, Test Loss: 0.0494\n",
            "Epoch [2994/6000], Train Loss: 0.0403, Test Loss: 0.0493\n",
            "Epoch [2995/6000], Train Loss: 0.0403, Test Loss: 0.0493\n",
            "Epoch [2996/6000], Train Loss: 0.0402, Test Loss: 0.0492\n",
            "Epoch [2997/6000], Train Loss: 0.0402, Test Loss: 0.0491\n",
            "Epoch [2998/6000], Train Loss: 0.0402, Test Loss: 0.0491\n",
            "Epoch [2999/6000], Train Loss: 0.0401, Test Loss: 0.0491\n",
            "Epoch [3000/6000], Train Loss: 0.0401, Test Loss: 0.0492\n",
            "Epoch [3001/6000], Train Loss: 0.0401, Test Loss: 0.0492\n",
            "Epoch [3002/6000], Train Loss: 0.0400, Test Loss: 0.0491\n",
            "Epoch [3003/6000], Train Loss: 0.0400, Test Loss: 0.0490\n",
            "Epoch [3004/6000], Train Loss: 0.0400, Test Loss: 0.0489\n",
            "Epoch [3005/6000], Train Loss: 0.0399, Test Loss: 0.0489\n",
            "Epoch [3006/6000], Train Loss: 0.0399, Test Loss: 0.0489\n",
            "Epoch [3007/6000], Train Loss: 0.0399, Test Loss: 0.0489\n",
            "Epoch [3008/6000], Train Loss: 0.0398, Test Loss: 0.0489\n",
            "Epoch [3009/6000], Train Loss: 0.0398, Test Loss: 0.0488\n",
            "Epoch [3010/6000], Train Loss: 0.0398, Test Loss: 0.0488\n",
            "Epoch [3011/6000], Train Loss: 0.0397, Test Loss: 0.0487\n",
            "Epoch [3012/6000], Train Loss: 0.0397, Test Loss: 0.0487\n",
            "Epoch [3013/6000], Train Loss: 0.0397, Test Loss: 0.0487\n",
            "Epoch [3014/6000], Train Loss: 0.0396, Test Loss: 0.0487\n",
            "Epoch [3015/6000], Train Loss: 0.0396, Test Loss: 0.0486\n",
            "Epoch [3016/6000], Train Loss: 0.0395, Test Loss: 0.0487\n",
            "Epoch [3017/6000], Train Loss: 0.0395, Test Loss: 0.0486\n",
            "Epoch [3018/6000], Train Loss: 0.0395, Test Loss: 0.0485\n",
            "Epoch [3019/6000], Train Loss: 0.0394, Test Loss: 0.0485\n",
            "Epoch [3020/6000], Train Loss: 0.0394, Test Loss: 0.0485\n",
            "Epoch [3021/6000], Train Loss: 0.0394, Test Loss: 0.0485\n",
            "Epoch [3022/6000], Train Loss: 0.0393, Test Loss: 0.0485\n",
            "Epoch [3023/6000], Train Loss: 0.0393, Test Loss: 0.0485\n",
            "Epoch [3024/6000], Train Loss: 0.0393, Test Loss: 0.0484\n",
            "Epoch [3025/6000], Train Loss: 0.0392, Test Loss: 0.0483\n",
            "Epoch [3026/6000], Train Loss: 0.0392, Test Loss: 0.0483\n",
            "Epoch [3027/6000], Train Loss: 0.0392, Test Loss: 0.0483\n",
            "Epoch [3028/6000], Train Loss: 0.0391, Test Loss: 0.0483\n",
            "Epoch [3029/6000], Train Loss: 0.0391, Test Loss: 0.0483\n",
            "Epoch [3030/6000], Train Loss: 0.0391, Test Loss: 0.0482\n",
            "Epoch [3031/6000], Train Loss: 0.0390, Test Loss: 0.0481\n",
            "Epoch [3032/6000], Train Loss: 0.0390, Test Loss: 0.0481\n",
            "Epoch [3033/6000], Train Loss: 0.0390, Test Loss: 0.0481\n",
            "Epoch [3034/6000], Train Loss: 0.0389, Test Loss: 0.0481\n",
            "Epoch [3035/6000], Train Loss: 0.0389, Test Loss: 0.0481\n",
            "Epoch [3036/6000], Train Loss: 0.0389, Test Loss: 0.0480\n",
            "Epoch [3037/6000], Train Loss: 0.0388, Test Loss: 0.0479\n",
            "Epoch [3038/6000], Train Loss: 0.0388, Test Loss: 0.0479\n",
            "Epoch [3039/6000], Train Loss: 0.0387, Test Loss: 0.0479\n",
            "Epoch [3040/6000], Train Loss: 0.0387, Test Loss: 0.0480\n",
            "Epoch [3041/6000], Train Loss: 0.0387, Test Loss: 0.0479\n",
            "Epoch [3042/6000], Train Loss: 0.0386, Test Loss: 0.0479\n",
            "Epoch [3043/6000], Train Loss: 0.0386, Test Loss: 0.0478\n",
            "Epoch [3044/6000], Train Loss: 0.0386, Test Loss: 0.0478\n",
            "Epoch [3045/6000], Train Loss: 0.0385, Test Loss: 0.0477\n",
            "Epoch [3046/6000], Train Loss: 0.0385, Test Loss: 0.0477\n",
            "Epoch [3047/6000], Train Loss: 0.0385, Test Loss: 0.0477\n",
            "Epoch [3048/6000], Train Loss: 0.0384, Test Loss: 0.0476\n",
            "Epoch [3049/6000], Train Loss: 0.0384, Test Loss: 0.0476\n",
            "Epoch [3050/6000], Train Loss: 0.0384, Test Loss: 0.0476\n",
            "Epoch [3051/6000], Train Loss: 0.0383, Test Loss: 0.0475\n",
            "Epoch [3052/6000], Train Loss: 0.0383, Test Loss: 0.0475\n",
            "Epoch [3053/6000], Train Loss: 0.0383, Test Loss: 0.0474\n",
            "Epoch [3054/6000], Train Loss: 0.0382, Test Loss: 0.0474\n",
            "Epoch [3055/6000], Train Loss: 0.0382, Test Loss: 0.0475\n",
            "Epoch [3056/6000], Train Loss: 0.0382, Test Loss: 0.0474\n",
            "Epoch [3057/6000], Train Loss: 0.0381, Test Loss: 0.0474\n",
            "Epoch [3058/6000], Train Loss: 0.0381, Test Loss: 0.0473\n",
            "Epoch [3059/6000], Train Loss: 0.0381, Test Loss: 0.0472\n",
            "Epoch [3060/6000], Train Loss: 0.0380, Test Loss: 0.0471\n",
            "Epoch [3061/6000], Train Loss: 0.0380, Test Loss: 0.0472\n",
            "Epoch [3062/6000], Train Loss: 0.0380, Test Loss: 0.0472\n",
            "Epoch [3063/6000], Train Loss: 0.0379, Test Loss: 0.0472\n",
            "Epoch [3064/6000], Train Loss: 0.0379, Test Loss: 0.0471\n",
            "Epoch [3065/6000], Train Loss: 0.0379, Test Loss: 0.0470\n",
            "Epoch [3066/6000], Train Loss: 0.0378, Test Loss: 0.0471\n",
            "Epoch [3067/6000], Train Loss: 0.0378, Test Loss: 0.0471\n",
            "Epoch [3068/6000], Train Loss: 0.0378, Test Loss: 0.0470\n",
            "Epoch [3069/6000], Train Loss: 0.0377, Test Loss: 0.0470\n",
            "Epoch [3070/6000], Train Loss: 0.0377, Test Loss: 0.0470\n",
            "Epoch [3071/6000], Train Loss: 0.0377, Test Loss: 0.0469\n",
            "Epoch [3072/6000], Train Loss: 0.0376, Test Loss: 0.0469\n",
            "Epoch [3073/6000], Train Loss: 0.0376, Test Loss: 0.0470\n",
            "Epoch [3074/6000], Train Loss: 0.0376, Test Loss: 0.0469\n",
            "Epoch [3075/6000], Train Loss: 0.0375, Test Loss: 0.0469\n",
            "Epoch [3076/6000], Train Loss: 0.0375, Test Loss: 0.0469\n",
            "Epoch [3077/6000], Train Loss: 0.0375, Test Loss: 0.0467\n",
            "Epoch [3078/6000], Train Loss: 0.0374, Test Loss: 0.0467\n",
            "Epoch [3079/6000], Train Loss: 0.0374, Test Loss: 0.0466\n",
            "Epoch [3080/6000], Train Loss: 0.0374, Test Loss: 0.0466\n",
            "Epoch [3081/6000], Train Loss: 0.0374, Test Loss: 0.0466\n",
            "Epoch [3082/6000], Train Loss: 0.0373, Test Loss: 0.0466\n",
            "Epoch [3083/6000], Train Loss: 0.0373, Test Loss: 0.0466\n",
            "Epoch [3084/6000], Train Loss: 0.0372, Test Loss: 0.0465\n",
            "Epoch [3085/6000], Train Loss: 0.0372, Test Loss: 0.0465\n",
            "Epoch [3086/6000], Train Loss: 0.0372, Test Loss: 0.0464\n",
            "Epoch [3087/6000], Train Loss: 0.0371, Test Loss: 0.0464\n",
            "Epoch [3088/6000], Train Loss: 0.0371, Test Loss: 0.0464\n",
            "Epoch [3089/6000], Train Loss: 0.0371, Test Loss: 0.0463\n",
            "Epoch [3090/6000], Train Loss: 0.0371, Test Loss: 0.0464\n",
            "Epoch [3091/6000], Train Loss: 0.0370, Test Loss: 0.0464\n",
            "Epoch [3092/6000], Train Loss: 0.0370, Test Loss: 0.0464\n",
            "Epoch [3093/6000], Train Loss: 0.0370, Test Loss: 0.0463\n",
            "Epoch [3094/6000], Train Loss: 0.0369, Test Loss: 0.0462\n",
            "Epoch [3095/6000], Train Loss: 0.0369, Test Loss: 0.0462\n",
            "Epoch [3096/6000], Train Loss: 0.0369, Test Loss: 0.0462\n",
            "Epoch [3097/6000], Train Loss: 0.0368, Test Loss: 0.0462\n",
            "Epoch [3098/6000], Train Loss: 0.0368, Test Loss: 0.0462\n",
            "Epoch [3099/6000], Train Loss: 0.0368, Test Loss: 0.0462\n",
            "Epoch [3100/6000], Train Loss: 0.0367, Test Loss: 0.0461\n",
            "Epoch [3101/6000], Train Loss: 0.0367, Test Loss: 0.0460\n",
            "Epoch [3102/6000], Train Loss: 0.0367, Test Loss: 0.0460\n",
            "Epoch [3103/6000], Train Loss: 0.0366, Test Loss: 0.0460\n",
            "Epoch [3104/6000], Train Loss: 0.0366, Test Loss: 0.0459\n",
            "Epoch [3105/6000], Train Loss: 0.0366, Test Loss: 0.0459\n",
            "Epoch [3106/6000], Train Loss: 0.0365, Test Loss: 0.0458\n",
            "Epoch [3107/6000], Train Loss: 0.0365, Test Loss: 0.0458\n",
            "Epoch [3108/6000], Train Loss: 0.0365, Test Loss: 0.0457\n",
            "Epoch [3109/6000], Train Loss: 0.0365, Test Loss: 0.0458\n",
            "Epoch [3110/6000], Train Loss: 0.0364, Test Loss: 0.0458\n",
            "Epoch [3111/6000], Train Loss: 0.0364, Test Loss: 0.0457\n",
            "Epoch [3112/6000], Train Loss: 0.0363, Test Loss: 0.0457\n",
            "Epoch [3113/6000], Train Loss: 0.0363, Test Loss: 0.0456\n",
            "Epoch [3114/6000], Train Loss: 0.0363, Test Loss: 0.0455\n",
            "Epoch [3115/6000], Train Loss: 0.0362, Test Loss: 0.0455\n",
            "Epoch [3116/6000], Train Loss: 0.0362, Test Loss: 0.0455\n",
            "Epoch [3117/6000], Train Loss: 0.0362, Test Loss: 0.0455\n",
            "Epoch [3118/6000], Train Loss: 0.0362, Test Loss: 0.0455\n",
            "Epoch [3119/6000], Train Loss: 0.0361, Test Loss: 0.0455\n",
            "Epoch [3120/6000], Train Loss: 0.0361, Test Loss: 0.0454\n",
            "Epoch [3121/6000], Train Loss: 0.0361, Test Loss: 0.0454\n",
            "Epoch [3122/6000], Train Loss: 0.0360, Test Loss: 0.0453\n",
            "Epoch [3123/6000], Train Loss: 0.0360, Test Loss: 0.0454\n",
            "Epoch [3124/6000], Train Loss: 0.0360, Test Loss: 0.0454\n",
            "Epoch [3125/6000], Train Loss: 0.0359, Test Loss: 0.0453\n",
            "Epoch [3126/6000], Train Loss: 0.0359, Test Loss: 0.0453\n",
            "Epoch [3127/6000], Train Loss: 0.0359, Test Loss: 0.0452\n",
            "Epoch [3128/6000], Train Loss: 0.0358, Test Loss: 0.0452\n",
            "Epoch [3129/6000], Train Loss: 0.0358, Test Loss: 0.0452\n",
            "Epoch [3130/6000], Train Loss: 0.0358, Test Loss: 0.0452\n",
            "Epoch [3131/6000], Train Loss: 0.0357, Test Loss: 0.0451\n",
            "Epoch [3132/6000], Train Loss: 0.0357, Test Loss: 0.0451\n",
            "Epoch [3133/6000], Train Loss: 0.0357, Test Loss: 0.0451\n",
            "Epoch [3134/6000], Train Loss: 0.0356, Test Loss: 0.0450\n",
            "Epoch [3135/6000], Train Loss: 0.0356, Test Loss: 0.0450\n",
            "Epoch [3136/6000], Train Loss: 0.0356, Test Loss: 0.0450\n",
            "Epoch [3137/6000], Train Loss: 0.0355, Test Loss: 0.0449\n",
            "Epoch [3138/6000], Train Loss: 0.0355, Test Loss: 0.0450\n",
            "Epoch [3139/6000], Train Loss: 0.0355, Test Loss: 0.0449\n",
            "Epoch [3140/6000], Train Loss: 0.0355, Test Loss: 0.0448\n",
            "Epoch [3141/6000], Train Loss: 0.0354, Test Loss: 0.0448\n",
            "Epoch [3142/6000], Train Loss: 0.0354, Test Loss: 0.0448\n",
            "Epoch [3143/6000], Train Loss: 0.0354, Test Loss: 0.0447\n",
            "Epoch [3144/6000], Train Loss: 0.0353, Test Loss: 0.0447\n",
            "Epoch [3145/6000], Train Loss: 0.0353, Test Loss: 0.0447\n",
            "Epoch [3146/6000], Train Loss: 0.0353, Test Loss: 0.0447\n",
            "Epoch [3147/6000], Train Loss: 0.0352, Test Loss: 0.0447\n",
            "Epoch [3148/6000], Train Loss: 0.0352, Test Loss: 0.0446\n",
            "Epoch [3149/6000], Train Loss: 0.0352, Test Loss: 0.0446\n",
            "Epoch [3150/6000], Train Loss: 0.0352, Test Loss: 0.0446\n",
            "Epoch [3151/6000], Train Loss: 0.0351, Test Loss: 0.0445\n",
            "Epoch [3152/6000], Train Loss: 0.0351, Test Loss: 0.0445\n",
            "Epoch [3153/6000], Train Loss: 0.0351, Test Loss: 0.0445\n",
            "Epoch [3154/6000], Train Loss: 0.0350, Test Loss: 0.0444\n",
            "Epoch [3155/6000], Train Loss: 0.0350, Test Loss: 0.0444\n",
            "Epoch [3156/6000], Train Loss: 0.0350, Test Loss: 0.0443\n",
            "Epoch [3157/6000], Train Loss: 0.0349, Test Loss: 0.0443\n",
            "Epoch [3158/6000], Train Loss: 0.0349, Test Loss: 0.0443\n",
            "Epoch [3159/6000], Train Loss: 0.0349, Test Loss: 0.0443\n",
            "Epoch [3160/6000], Train Loss: 0.0349, Test Loss: 0.0443\n",
            "Epoch [3161/6000], Train Loss: 0.0348, Test Loss: 0.0443\n",
            "Epoch [3162/6000], Train Loss: 0.0348, Test Loss: 0.0442\n",
            "Epoch [3163/6000], Train Loss: 0.0347, Test Loss: 0.0442\n",
            "Epoch [3164/6000], Train Loss: 0.0347, Test Loss: 0.0441\n",
            "Epoch [3165/6000], Train Loss: 0.0347, Test Loss: 0.0441\n",
            "Epoch [3166/6000], Train Loss: 0.0347, Test Loss: 0.0442\n",
            "Epoch [3167/6000], Train Loss: 0.0346, Test Loss: 0.0442\n",
            "Epoch [3168/6000], Train Loss: 0.0346, Test Loss: 0.0442\n",
            "Epoch [3169/6000], Train Loss: 0.0346, Test Loss: 0.0441\n",
            "Epoch [3170/6000], Train Loss: 0.0345, Test Loss: 0.0440\n",
            "Epoch [3171/6000], Train Loss: 0.0345, Test Loss: 0.0439\n",
            "Epoch [3172/6000], Train Loss: 0.0345, Test Loss: 0.0439\n",
            "Epoch [3173/6000], Train Loss: 0.0344, Test Loss: 0.0439\n",
            "Epoch [3174/6000], Train Loss: 0.0344, Test Loss: 0.0438\n",
            "Epoch [3175/6000], Train Loss: 0.0344, Test Loss: 0.0438\n",
            "Epoch [3176/6000], Train Loss: 0.0344, Test Loss: 0.0439\n",
            "Epoch [3177/6000], Train Loss: 0.0343, Test Loss: 0.0439\n",
            "Epoch [3178/6000], Train Loss: 0.0343, Test Loss: 0.0439\n",
            "Epoch [3179/6000], Train Loss: 0.0343, Test Loss: 0.0439\n",
            "Epoch [3180/6000], Train Loss: 0.0342, Test Loss: 0.0438\n",
            "Epoch [3181/6000], Train Loss: 0.0342, Test Loss: 0.0437\n",
            "Epoch [3182/6000], Train Loss: 0.0342, Test Loss: 0.0436\n",
            "Epoch [3183/6000], Train Loss: 0.0341, Test Loss: 0.0436\n",
            "Epoch [3184/6000], Train Loss: 0.0341, Test Loss: 0.0435\n",
            "Epoch [3185/6000], Train Loss: 0.0341, Test Loss: 0.0434\n",
            "Epoch [3186/6000], Train Loss: 0.0341, Test Loss: 0.0434\n",
            "Epoch [3187/6000], Train Loss: 0.0340, Test Loss: 0.0435\n",
            "Epoch [3188/6000], Train Loss: 0.0340, Test Loss: 0.0436\n",
            "Epoch [3189/6000], Train Loss: 0.0340, Test Loss: 0.0435\n",
            "Epoch [3190/6000], Train Loss: 0.0339, Test Loss: 0.0435\n",
            "Epoch [3191/6000], Train Loss: 0.0339, Test Loss: 0.0434\n",
            "Epoch [3192/6000], Train Loss: 0.0339, Test Loss: 0.0433\n",
            "Epoch [3193/6000], Train Loss: 0.0338, Test Loss: 0.0433\n",
            "Epoch [3194/6000], Train Loss: 0.0338, Test Loss: 0.0433\n",
            "Epoch [3195/6000], Train Loss: 0.0338, Test Loss: 0.0433\n",
            "Epoch [3196/6000], Train Loss: 0.0338, Test Loss: 0.0433\n",
            "Epoch [3197/6000], Train Loss: 0.0337, Test Loss: 0.0432\n",
            "Epoch [3198/6000], Train Loss: 0.0337, Test Loss: 0.0432\n",
            "Epoch [3199/6000], Train Loss: 0.0337, Test Loss: 0.0431\n",
            "Epoch [3200/6000], Train Loss: 0.0336, Test Loss: 0.0431\n",
            "Epoch [3201/6000], Train Loss: 0.0336, Test Loss: 0.0431\n",
            "Epoch [3202/6000], Train Loss: 0.0336, Test Loss: 0.0431\n",
            "Epoch [3203/6000], Train Loss: 0.0336, Test Loss: 0.0431\n",
            "Epoch [3204/6000], Train Loss: 0.0335, Test Loss: 0.0431\n",
            "Epoch [3205/6000], Train Loss: 0.0335, Test Loss: 0.0430\n",
            "Epoch [3206/6000], Train Loss: 0.0335, Test Loss: 0.0430\n",
            "Epoch [3207/6000], Train Loss: 0.0334, Test Loss: 0.0430\n",
            "Epoch [3208/6000], Train Loss: 0.0334, Test Loss: 0.0429\n",
            "Epoch [3209/6000], Train Loss: 0.0334, Test Loss: 0.0429\n",
            "Epoch [3210/6000], Train Loss: 0.0333, Test Loss: 0.0428\n",
            "Epoch [3211/6000], Train Loss: 0.0333, Test Loss: 0.0428\n",
            "Epoch [3212/6000], Train Loss: 0.0333, Test Loss: 0.0427\n",
            "Epoch [3213/6000], Train Loss: 0.0333, Test Loss: 0.0428\n",
            "Epoch [3214/6000], Train Loss: 0.0332, Test Loss: 0.0428\n",
            "Epoch [3215/6000], Train Loss: 0.0332, Test Loss: 0.0428\n",
            "Epoch [3216/6000], Train Loss: 0.0332, Test Loss: 0.0427\n",
            "Epoch [3217/6000], Train Loss: 0.0331, Test Loss: 0.0426\n",
            "Epoch [3218/6000], Train Loss: 0.0331, Test Loss: 0.0425\n",
            "Epoch [3219/6000], Train Loss: 0.0331, Test Loss: 0.0425\n",
            "Epoch [3220/6000], Train Loss: 0.0331, Test Loss: 0.0425\n",
            "Epoch [3221/6000], Train Loss: 0.0330, Test Loss: 0.0425\n",
            "Epoch [3222/6000], Train Loss: 0.0330, Test Loss: 0.0425\n",
            "Epoch [3223/6000], Train Loss: 0.0330, Test Loss: 0.0425\n",
            "Epoch [3224/6000], Train Loss: 0.0329, Test Loss: 0.0425\n",
            "Epoch [3225/6000], Train Loss: 0.0329, Test Loss: 0.0424\n",
            "Epoch [3226/6000], Train Loss: 0.0329, Test Loss: 0.0424\n",
            "Epoch [3227/6000], Train Loss: 0.0329, Test Loss: 0.0423\n",
            "Epoch [3228/6000], Train Loss: 0.0328, Test Loss: 0.0423\n",
            "Epoch [3229/6000], Train Loss: 0.0328, Test Loss: 0.0423\n",
            "Epoch [3230/6000], Train Loss: 0.0328, Test Loss: 0.0423\n",
            "Epoch [3231/6000], Train Loss: 0.0327, Test Loss: 0.0423\n",
            "Epoch [3232/6000], Train Loss: 0.0327, Test Loss: 0.0422\n",
            "Epoch [3233/6000], Train Loss: 0.0327, Test Loss: 0.0422\n",
            "Epoch [3234/6000], Train Loss: 0.0327, Test Loss: 0.0422\n",
            "Epoch [3235/6000], Train Loss: 0.0326, Test Loss: 0.0422\n",
            "Epoch [3236/6000], Train Loss: 0.0326, Test Loss: 0.0421\n",
            "Epoch [3237/6000], Train Loss: 0.0326, Test Loss: 0.0421\n",
            "Epoch [3238/6000], Train Loss: 0.0325, Test Loss: 0.0421\n",
            "Epoch [3239/6000], Train Loss: 0.0325, Test Loss: 0.0420\n",
            "Epoch [3240/6000], Train Loss: 0.0325, Test Loss: 0.0420\n",
            "Epoch [3241/6000], Train Loss: 0.0325, Test Loss: 0.0420\n",
            "Epoch [3242/6000], Train Loss: 0.0324, Test Loss: 0.0420\n",
            "Epoch [3243/6000], Train Loss: 0.0324, Test Loss: 0.0420\n",
            "Epoch [3244/6000], Train Loss: 0.0324, Test Loss: 0.0419\n",
            "Epoch [3245/6000], Train Loss: 0.0323, Test Loss: 0.0418\n",
            "Epoch [3246/6000], Train Loss: 0.0323, Test Loss: 0.0418\n",
            "Epoch [3247/6000], Train Loss: 0.0323, Test Loss: 0.0417\n",
            "Epoch [3248/6000], Train Loss: 0.0323, Test Loss: 0.0417\n",
            "Epoch [3249/6000], Train Loss: 0.0322, Test Loss: 0.0418\n",
            "Epoch [3250/6000], Train Loss: 0.0322, Test Loss: 0.0417\n",
            "Epoch [3251/6000], Train Loss: 0.0322, Test Loss: 0.0417\n",
            "Epoch [3252/6000], Train Loss: 0.0321, Test Loss: 0.0416\n",
            "Epoch [3253/6000], Train Loss: 0.0321, Test Loss: 0.0416\n",
            "Epoch [3254/6000], Train Loss: 0.0321, Test Loss: 0.0416\n",
            "Epoch [3255/6000], Train Loss: 0.0321, Test Loss: 0.0416\n",
            "Epoch [3256/6000], Train Loss: 0.0320, Test Loss: 0.0415\n",
            "Epoch [3257/6000], Train Loss: 0.0320, Test Loss: 0.0415\n",
            "Epoch [3258/6000], Train Loss: 0.0320, Test Loss: 0.0415\n",
            "Epoch [3259/6000], Train Loss: 0.0319, Test Loss: 0.0415\n",
            "Epoch [3260/6000], Train Loss: 0.0319, Test Loss: 0.0415\n",
            "Epoch [3261/6000], Train Loss: 0.0319, Test Loss: 0.0415\n",
            "Epoch [3262/6000], Train Loss: 0.0319, Test Loss: 0.0414\n",
            "Epoch [3263/6000], Train Loss: 0.0318, Test Loss: 0.0413\n",
            "Epoch [3264/6000], Train Loss: 0.0318, Test Loss: 0.0413\n",
            "Epoch [3265/6000], Train Loss: 0.0318, Test Loss: 0.0413\n",
            "Epoch [3266/6000], Train Loss: 0.0317, Test Loss: 0.0413\n",
            "Epoch [3267/6000], Train Loss: 0.0317, Test Loss: 0.0413\n",
            "Epoch [3268/6000], Train Loss: 0.0317, Test Loss: 0.0414\n",
            "Epoch [3269/6000], Train Loss: 0.0317, Test Loss: 0.0413\n",
            "Epoch [3270/6000], Train Loss: 0.0316, Test Loss: 0.0413\n",
            "Epoch [3271/6000], Train Loss: 0.0316, Test Loss: 0.0412\n",
            "Epoch [3272/6000], Train Loss: 0.0316, Test Loss: 0.0411\n",
            "Epoch [3273/6000], Train Loss: 0.0316, Test Loss: 0.0411\n",
            "Epoch [3274/6000], Train Loss: 0.0315, Test Loss: 0.0410\n",
            "Epoch [3275/6000], Train Loss: 0.0315, Test Loss: 0.0410\n",
            "Epoch [3276/6000], Train Loss: 0.0315, Test Loss: 0.0410\n",
            "Epoch [3277/6000], Train Loss: 0.0314, Test Loss: 0.0409\n",
            "Epoch [3278/6000], Train Loss: 0.0314, Test Loss: 0.0409\n",
            "Epoch [3279/6000], Train Loss: 0.0314, Test Loss: 0.0409\n",
            "Epoch [3280/6000], Train Loss: 0.0314, Test Loss: 0.0408\n",
            "Epoch [3281/6000], Train Loss: 0.0313, Test Loss: 0.0408\n",
            "Epoch [3282/6000], Train Loss: 0.0313, Test Loss: 0.0407\n",
            "Epoch [3283/6000], Train Loss: 0.0313, Test Loss: 0.0408\n",
            "Epoch [3284/6000], Train Loss: 0.0313, Test Loss: 0.0408\n",
            "Epoch [3285/6000], Train Loss: 0.0312, Test Loss: 0.0408\n",
            "Epoch [3286/6000], Train Loss: 0.0312, Test Loss: 0.0407\n",
            "Epoch [3287/6000], Train Loss: 0.0312, Test Loss: 0.0406\n",
            "Epoch [3288/6000], Train Loss: 0.0311, Test Loss: 0.0406\n",
            "Epoch [3289/6000], Train Loss: 0.0311, Test Loss: 0.0406\n",
            "Epoch [3290/6000], Train Loss: 0.0311, Test Loss: 0.0406\n",
            "Epoch [3291/6000], Train Loss: 0.0311, Test Loss: 0.0406\n",
            "Epoch [3292/6000], Train Loss: 0.0310, Test Loss: 0.0406\n",
            "Epoch [3293/6000], Train Loss: 0.0310, Test Loss: 0.0405\n",
            "Epoch [3294/6000], Train Loss: 0.0310, Test Loss: 0.0405\n",
            "Epoch [3295/6000], Train Loss: 0.0310, Test Loss: 0.0405\n",
            "Epoch [3296/6000], Train Loss: 0.0309, Test Loss: 0.0405\n",
            "Epoch [3297/6000], Train Loss: 0.0309, Test Loss: 0.0404\n",
            "Epoch [3298/6000], Train Loss: 0.0309, Test Loss: 0.0404\n",
            "Epoch [3299/6000], Train Loss: 0.0308, Test Loss: 0.0404\n",
            "Epoch [3300/6000], Train Loss: 0.0308, Test Loss: 0.0403\n",
            "Epoch [3301/6000], Train Loss: 0.0308, Test Loss: 0.0403\n",
            "Epoch [3302/6000], Train Loss: 0.0308, Test Loss: 0.0403\n",
            "Epoch [3303/6000], Train Loss: 0.0307, Test Loss: 0.0402\n",
            "Epoch [3304/6000], Train Loss: 0.0307, Test Loss: 0.0402\n",
            "Epoch [3305/6000], Train Loss: 0.0307, Test Loss: 0.0402\n",
            "Epoch [3306/6000], Train Loss: 0.0307, Test Loss: 0.0401\n",
            "Epoch [3307/6000], Train Loss: 0.0306, Test Loss: 0.0401\n",
            "Epoch [3308/6000], Train Loss: 0.0306, Test Loss: 0.0401\n",
            "Epoch [3309/6000], Train Loss: 0.0306, Test Loss: 0.0401\n",
            "Epoch [3310/6000], Train Loss: 0.0305, Test Loss: 0.0401\n",
            "Epoch [3311/6000], Train Loss: 0.0305, Test Loss: 0.0401\n",
            "Epoch [3312/6000], Train Loss: 0.0305, Test Loss: 0.0401\n",
            "Epoch [3313/6000], Train Loss: 0.0305, Test Loss: 0.0401\n",
            "Epoch [3314/6000], Train Loss: 0.0304, Test Loss: 0.0400\n",
            "Epoch [3315/6000], Train Loss: 0.0304, Test Loss: 0.0399\n",
            "Epoch [3316/6000], Train Loss: 0.0304, Test Loss: 0.0399\n",
            "Epoch [3317/6000], Train Loss: 0.0304, Test Loss: 0.0399\n",
            "Epoch [3318/6000], Train Loss: 0.0303, Test Loss: 0.0399\n",
            "Epoch [3319/6000], Train Loss: 0.0303, Test Loss: 0.0398\n",
            "Epoch [3320/6000], Train Loss: 0.0303, Test Loss: 0.0398\n",
            "Epoch [3321/6000], Train Loss: 0.0303, Test Loss: 0.0398\n",
            "Epoch [3322/6000], Train Loss: 0.0302, Test Loss: 0.0398\n",
            "Epoch [3323/6000], Train Loss: 0.0302, Test Loss: 0.0398\n",
            "Epoch [3324/6000], Train Loss: 0.0302, Test Loss: 0.0397\n",
            "Epoch [3325/6000], Train Loss: 0.0301, Test Loss: 0.0396\n",
            "Epoch [3326/6000], Train Loss: 0.0301, Test Loss: 0.0396\n",
            "Epoch [3327/6000], Train Loss: 0.0301, Test Loss: 0.0396\n",
            "Epoch [3328/6000], Train Loss: 0.0301, Test Loss: 0.0396\n",
            "Epoch [3329/6000], Train Loss: 0.0300, Test Loss: 0.0396\n",
            "Epoch [3330/6000], Train Loss: 0.0300, Test Loss: 0.0396\n",
            "Epoch [3331/6000], Train Loss: 0.0300, Test Loss: 0.0395\n",
            "Epoch [3332/6000], Train Loss: 0.0300, Test Loss: 0.0395\n",
            "Epoch [3333/6000], Train Loss: 0.0299, Test Loss: 0.0395\n",
            "Epoch [3334/6000], Train Loss: 0.0299, Test Loss: 0.0395\n",
            "Epoch [3335/6000], Train Loss: 0.0299, Test Loss: 0.0394\n",
            "Epoch [3336/6000], Train Loss: 0.0299, Test Loss: 0.0394\n",
            "Epoch [3337/6000], Train Loss: 0.0298, Test Loss: 0.0394\n",
            "Epoch [3338/6000], Train Loss: 0.0298, Test Loss: 0.0393\n",
            "Epoch [3339/6000], Train Loss: 0.0298, Test Loss: 0.0393\n",
            "Epoch [3340/6000], Train Loss: 0.0298, Test Loss: 0.0393\n",
            "Epoch [3341/6000], Train Loss: 0.0297, Test Loss: 0.0393\n",
            "Epoch [3342/6000], Train Loss: 0.0297, Test Loss: 0.0392\n",
            "Epoch [3343/6000], Train Loss: 0.0297, Test Loss: 0.0391\n",
            "Epoch [3344/6000], Train Loss: 0.0297, Test Loss: 0.0391\n",
            "Epoch [3345/6000], Train Loss: 0.0296, Test Loss: 0.0391\n",
            "Epoch [3346/6000], Train Loss: 0.0296, Test Loss: 0.0392\n",
            "Epoch [3347/6000], Train Loss: 0.0296, Test Loss: 0.0392\n",
            "Epoch [3348/6000], Train Loss: 0.0296, Test Loss: 0.0392\n",
            "Epoch [3349/6000], Train Loss: 0.0295, Test Loss: 0.0392\n",
            "Epoch [3350/6000], Train Loss: 0.0295, Test Loss: 0.0391\n",
            "Epoch [3351/6000], Train Loss: 0.0295, Test Loss: 0.0389\n",
            "Epoch [3352/6000], Train Loss: 0.0295, Test Loss: 0.0389\n",
            "Epoch [3353/6000], Train Loss: 0.0294, Test Loss: 0.0388\n",
            "Epoch [3354/6000], Train Loss: 0.0294, Test Loss: 0.0389\n",
            "Epoch [3355/6000], Train Loss: 0.0294, Test Loss: 0.0389\n",
            "Epoch [3356/6000], Train Loss: 0.0293, Test Loss: 0.0389\n",
            "Epoch [3357/6000], Train Loss: 0.0293, Test Loss: 0.0389\n",
            "Epoch [3358/6000], Train Loss: 0.0293, Test Loss: 0.0388\n",
            "Epoch [3359/6000], Train Loss: 0.0293, Test Loss: 0.0387\n",
            "Epoch [3360/6000], Train Loss: 0.0292, Test Loss: 0.0387\n",
            "Epoch [3361/6000], Train Loss: 0.0292, Test Loss: 0.0387\n",
            "Epoch [3362/6000], Train Loss: 0.0292, Test Loss: 0.0387\n",
            "Epoch [3363/6000], Train Loss: 0.0292, Test Loss: 0.0387\n",
            "Epoch [3364/6000], Train Loss: 0.0291, Test Loss: 0.0387\n",
            "Epoch [3365/6000], Train Loss: 0.0291, Test Loss: 0.0387\n",
            "Epoch [3366/6000], Train Loss: 0.0291, Test Loss: 0.0387\n",
            "Epoch [3367/6000], Train Loss: 0.0291, Test Loss: 0.0386\n",
            "Epoch [3368/6000], Train Loss: 0.0290, Test Loss: 0.0385\n",
            "Epoch [3369/6000], Train Loss: 0.0290, Test Loss: 0.0384\n",
            "Epoch [3370/6000], Train Loss: 0.0290, Test Loss: 0.0384\n",
            "Epoch [3371/6000], Train Loss: 0.0290, Test Loss: 0.0385\n",
            "Epoch [3372/6000], Train Loss: 0.0289, Test Loss: 0.0385\n",
            "Epoch [3373/6000], Train Loss: 0.0289, Test Loss: 0.0385\n",
            "Epoch [3374/6000], Train Loss: 0.0289, Test Loss: 0.0385\n",
            "Epoch [3375/6000], Train Loss: 0.0289, Test Loss: 0.0384\n",
            "Epoch [3376/6000], Train Loss: 0.0288, Test Loss: 0.0383\n",
            "Epoch [3377/6000], Train Loss: 0.0288, Test Loss: 0.0383\n",
            "Epoch [3378/6000], Train Loss: 0.0288, Test Loss: 0.0383\n",
            "Epoch [3379/6000], Train Loss: 0.0288, Test Loss: 0.0383\n",
            "Epoch [3380/6000], Train Loss: 0.0287, Test Loss: 0.0382\n",
            "Epoch [3381/6000], Train Loss: 0.0287, Test Loss: 0.0382\n",
            "Epoch [3382/6000], Train Loss: 0.0287, Test Loss: 0.0381\n",
            "Epoch [3383/6000], Train Loss: 0.0287, Test Loss: 0.0381\n",
            "Epoch [3384/6000], Train Loss: 0.0286, Test Loss: 0.0381\n",
            "Epoch [3385/6000], Train Loss: 0.0286, Test Loss: 0.0381\n",
            "Epoch [3386/6000], Train Loss: 0.0286, Test Loss: 0.0380\n",
            "Epoch [3387/6000], Train Loss: 0.0286, Test Loss: 0.0380\n",
            "Epoch [3388/6000], Train Loss: 0.0285, Test Loss: 0.0380\n",
            "Epoch [3389/6000], Train Loss: 0.0285, Test Loss: 0.0380\n",
            "Epoch [3390/6000], Train Loss: 0.0285, Test Loss: 0.0380\n",
            "Epoch [3391/6000], Train Loss: 0.0285, Test Loss: 0.0380\n",
            "Epoch [3392/6000], Train Loss: 0.0284, Test Loss: 0.0379\n",
            "Epoch [3393/6000], Train Loss: 0.0284, Test Loss: 0.0378\n",
            "Epoch [3394/6000], Train Loss: 0.0284, Test Loss: 0.0378\n",
            "Epoch [3395/6000], Train Loss: 0.0284, Test Loss: 0.0378\n",
            "Epoch [3396/6000], Train Loss: 0.0283, Test Loss: 0.0378\n",
            "Epoch [3397/6000], Train Loss: 0.0283, Test Loss: 0.0378\n",
            "Epoch [3398/6000], Train Loss: 0.0283, Test Loss: 0.0378\n",
            "Epoch [3399/6000], Train Loss: 0.0283, Test Loss: 0.0377\n",
            "Epoch [3400/6000], Train Loss: 0.0282, Test Loss: 0.0376\n",
            "Epoch [3401/6000], Train Loss: 0.0282, Test Loss: 0.0376\n",
            "Epoch [3402/6000], Train Loss: 0.0282, Test Loss: 0.0376\n",
            "Epoch [3403/6000], Train Loss: 0.0282, Test Loss: 0.0376\n",
            "Epoch [3404/6000], Train Loss: 0.0281, Test Loss: 0.0376\n",
            "Epoch [3405/6000], Train Loss: 0.0281, Test Loss: 0.0376\n",
            "Epoch [3406/6000], Train Loss: 0.0281, Test Loss: 0.0376\n",
            "Epoch [3407/6000], Train Loss: 0.0281, Test Loss: 0.0376\n",
            "Epoch [3408/6000], Train Loss: 0.0280, Test Loss: 0.0375\n",
            "Epoch [3409/6000], Train Loss: 0.0280, Test Loss: 0.0375\n",
            "Epoch [3410/6000], Train Loss: 0.0280, Test Loss: 0.0374\n",
            "Epoch [3411/6000], Train Loss: 0.0280, Test Loss: 0.0374\n",
            "Epoch [3412/6000], Train Loss: 0.0279, Test Loss: 0.0374\n",
            "Epoch [3413/6000], Train Loss: 0.0279, Test Loss: 0.0374\n",
            "Epoch [3414/6000], Train Loss: 0.0279, Test Loss: 0.0373\n",
            "Epoch [3415/6000], Train Loss: 0.0279, Test Loss: 0.0373\n",
            "Epoch [3416/6000], Train Loss: 0.0279, Test Loss: 0.0373\n",
            "Epoch [3417/6000], Train Loss: 0.0278, Test Loss: 0.0373\n",
            "Epoch [3418/6000], Train Loss: 0.0278, Test Loss: 0.0373\n",
            "Epoch [3419/6000], Train Loss: 0.0278, Test Loss: 0.0372\n",
            "Epoch [3420/6000], Train Loss: 0.0278, Test Loss: 0.0372\n",
            "Epoch [3421/6000], Train Loss: 0.0277, Test Loss: 0.0372\n",
            "Epoch [3422/6000], Train Loss: 0.0277, Test Loss: 0.0372\n",
            "Epoch [3423/6000], Train Loss: 0.0277, Test Loss: 0.0371\n",
            "Epoch [3424/6000], Train Loss: 0.0277, Test Loss: 0.0371\n",
            "Epoch [3425/6000], Train Loss: 0.0276, Test Loss: 0.0371\n",
            "Epoch [3426/6000], Train Loss: 0.0276, Test Loss: 0.0370\n",
            "Epoch [3427/6000], Train Loss: 0.0276, Test Loss: 0.0370\n",
            "Epoch [3428/6000], Train Loss: 0.0276, Test Loss: 0.0370\n",
            "Epoch [3429/6000], Train Loss: 0.0275, Test Loss: 0.0370\n",
            "Epoch [3430/6000], Train Loss: 0.0275, Test Loss: 0.0370\n",
            "Epoch [3431/6000], Train Loss: 0.0275, Test Loss: 0.0369\n",
            "Epoch [3432/6000], Train Loss: 0.0275, Test Loss: 0.0369\n",
            "Epoch [3433/6000], Train Loss: 0.0274, Test Loss: 0.0369\n",
            "Epoch [3434/6000], Train Loss: 0.0274, Test Loss: 0.0368\n",
            "Epoch [3435/6000], Train Loss: 0.0274, Test Loss: 0.0368\n",
            "Epoch [3436/6000], Train Loss: 0.0274, Test Loss: 0.0368\n",
            "Epoch [3437/6000], Train Loss: 0.0273, Test Loss: 0.0367\n",
            "Epoch [3438/6000], Train Loss: 0.0273, Test Loss: 0.0367\n",
            "Epoch [3439/6000], Train Loss: 0.0273, Test Loss: 0.0367\n",
            "Epoch [3440/6000], Train Loss: 0.0273, Test Loss: 0.0367\n",
            "Epoch [3441/6000], Train Loss: 0.0273, Test Loss: 0.0367\n",
            "Epoch [3442/6000], Train Loss: 0.0272, Test Loss: 0.0366\n",
            "Epoch [3443/6000], Train Loss: 0.0272, Test Loss: 0.0365\n",
            "Epoch [3444/6000], Train Loss: 0.0272, Test Loss: 0.0366\n",
            "Epoch [3445/6000], Train Loss: 0.0272, Test Loss: 0.0366\n",
            "Epoch [3446/6000], Train Loss: 0.0271, Test Loss: 0.0366\n",
            "Epoch [3447/6000], Train Loss: 0.0271, Test Loss: 0.0365\n",
            "Epoch [3448/6000], Train Loss: 0.0271, Test Loss: 0.0365\n",
            "Epoch [3449/6000], Train Loss: 0.0271, Test Loss: 0.0364\n",
            "Epoch [3450/6000], Train Loss: 0.0270, Test Loss: 0.0364\n",
            "Epoch [3451/6000], Train Loss: 0.0270, Test Loss: 0.0365\n",
            "Epoch [3452/6000], Train Loss: 0.0270, Test Loss: 0.0365\n",
            "Epoch [3453/6000], Train Loss: 0.0270, Test Loss: 0.0364\n",
            "Epoch [3454/6000], Train Loss: 0.0269, Test Loss: 0.0364\n",
            "Epoch [3455/6000], Train Loss: 0.0269, Test Loss: 0.0363\n",
            "Epoch [3456/6000], Train Loss: 0.0269, Test Loss: 0.0363\n",
            "Epoch [3457/6000], Train Loss: 0.0269, Test Loss: 0.0362\n",
            "Epoch [3458/6000], Train Loss: 0.0269, Test Loss: 0.0363\n",
            "Epoch [3459/6000], Train Loss: 0.0268, Test Loss: 0.0362\n",
            "Epoch [3460/6000], Train Loss: 0.0268, Test Loss: 0.0362\n",
            "Epoch [3461/6000], Train Loss: 0.0268, Test Loss: 0.0362\n",
            "Epoch [3462/6000], Train Loss: 0.0268, Test Loss: 0.0361\n",
            "Epoch [3463/6000], Train Loss: 0.0267, Test Loss: 0.0360\n",
            "Epoch [3464/6000], Train Loss: 0.0267, Test Loss: 0.0361\n",
            "Epoch [3465/6000], Train Loss: 0.0267, Test Loss: 0.0361\n",
            "Epoch [3466/6000], Train Loss: 0.0267, Test Loss: 0.0361\n",
            "Epoch [3467/6000], Train Loss: 0.0266, Test Loss: 0.0360\n",
            "Epoch [3468/6000], Train Loss: 0.0266, Test Loss: 0.0359\n",
            "Epoch [3469/6000], Train Loss: 0.0266, Test Loss: 0.0359\n",
            "Epoch [3470/6000], Train Loss: 0.0266, Test Loss: 0.0359\n",
            "Epoch [3471/6000], Train Loss: 0.0265, Test Loss: 0.0359\n",
            "Epoch [3472/6000], Train Loss: 0.0265, Test Loss: 0.0359\n",
            "Epoch [3473/6000], Train Loss: 0.0265, Test Loss: 0.0358\n",
            "Epoch [3474/6000], Train Loss: 0.0265, Test Loss: 0.0358\n",
            "Epoch [3475/6000], Train Loss: 0.0265, Test Loss: 0.0358\n",
            "Epoch [3476/6000], Train Loss: 0.0264, Test Loss: 0.0358\n",
            "Epoch [3477/6000], Train Loss: 0.0264, Test Loss: 0.0357\n",
            "Epoch [3478/6000], Train Loss: 0.0264, Test Loss: 0.0357\n",
            "Epoch [3479/6000], Train Loss: 0.0264, Test Loss: 0.0357\n",
            "Epoch [3480/6000], Train Loss: 0.0263, Test Loss: 0.0356\n",
            "Epoch [3481/6000], Train Loss: 0.0263, Test Loss: 0.0356\n",
            "Epoch [3482/6000], Train Loss: 0.0263, Test Loss: 0.0356\n",
            "Epoch [3483/6000], Train Loss: 0.0263, Test Loss: 0.0356\n",
            "Epoch [3484/6000], Train Loss: 0.0263, Test Loss: 0.0356\n",
            "Epoch [3485/6000], Train Loss: 0.0262, Test Loss: 0.0355\n",
            "Epoch [3486/6000], Train Loss: 0.0262, Test Loss: 0.0355\n",
            "Epoch [3487/6000], Train Loss: 0.0262, Test Loss: 0.0356\n",
            "Epoch [3488/6000], Train Loss: 0.0262, Test Loss: 0.0356\n",
            "Epoch [3489/6000], Train Loss: 0.0261, Test Loss: 0.0356\n",
            "Epoch [3490/6000], Train Loss: 0.0261, Test Loss: 0.0355\n",
            "Epoch [3491/6000], Train Loss: 0.0261, Test Loss: 0.0354\n",
            "Epoch [3492/6000], Train Loss: 0.0261, Test Loss: 0.0354\n",
            "Epoch [3493/6000], Train Loss: 0.0261, Test Loss: 0.0354\n",
            "Epoch [3494/6000], Train Loss: 0.0260, Test Loss: 0.0353\n",
            "Epoch [3495/6000], Train Loss: 0.0260, Test Loss: 0.0353\n",
            "Epoch [3496/6000], Train Loss: 0.0260, Test Loss: 0.0353\n",
            "Epoch [3497/6000], Train Loss: 0.0260, Test Loss: 0.0353\n",
            "Epoch [3498/6000], Train Loss: 0.0259, Test Loss: 0.0353\n",
            "Epoch [3499/6000], Train Loss: 0.0259, Test Loss: 0.0352\n",
            "Epoch [3500/6000], Train Loss: 0.0259, Test Loss: 0.0352\n",
            "Epoch [3501/6000], Train Loss: 0.0259, Test Loss: 0.0352\n",
            "Epoch [3502/6000], Train Loss: 0.0258, Test Loss: 0.0351\n",
            "Epoch [3503/6000], Train Loss: 0.0258, Test Loss: 0.0352\n",
            "Epoch [3504/6000], Train Loss: 0.0258, Test Loss: 0.0351\n",
            "Epoch [3505/6000], Train Loss: 0.0258, Test Loss: 0.0351\n",
            "Epoch [3506/6000], Train Loss: 0.0258, Test Loss: 0.0350\n",
            "Epoch [3507/6000], Train Loss: 0.0257, Test Loss: 0.0350\n",
            "Epoch [3508/6000], Train Loss: 0.0257, Test Loss: 0.0350\n",
            "Epoch [3509/6000], Train Loss: 0.0257, Test Loss: 0.0350\n",
            "Epoch [3510/6000], Train Loss: 0.0257, Test Loss: 0.0350\n",
            "Epoch [3511/6000], Train Loss: 0.0256, Test Loss: 0.0349\n",
            "Epoch [3512/6000], Train Loss: 0.0256, Test Loss: 0.0349\n",
            "Epoch [3513/6000], Train Loss: 0.0256, Test Loss: 0.0349\n",
            "Epoch [3514/6000], Train Loss: 0.0256, Test Loss: 0.0348\n",
            "Epoch [3515/6000], Train Loss: 0.0256, Test Loss: 0.0349\n",
            "Epoch [3516/6000], Train Loss: 0.0255, Test Loss: 0.0348\n",
            "Epoch [3517/6000], Train Loss: 0.0255, Test Loss: 0.0348\n",
            "Epoch [3518/6000], Train Loss: 0.0255, Test Loss: 0.0348\n",
            "Epoch [3519/6000], Train Loss: 0.0255, Test Loss: 0.0347\n",
            "Epoch [3520/6000], Train Loss: 0.0254, Test Loss: 0.0347\n",
            "Epoch [3521/6000], Train Loss: 0.0254, Test Loss: 0.0347\n",
            "Epoch [3522/6000], Train Loss: 0.0254, Test Loss: 0.0347\n",
            "Epoch [3523/6000], Train Loss: 0.0254, Test Loss: 0.0347\n",
            "Epoch [3524/6000], Train Loss: 0.0254, Test Loss: 0.0346\n",
            "Epoch [3525/6000], Train Loss: 0.0253, Test Loss: 0.0346\n",
            "Epoch [3526/6000], Train Loss: 0.0253, Test Loss: 0.0345\n",
            "Epoch [3527/6000], Train Loss: 0.0253, Test Loss: 0.0345\n",
            "Epoch [3528/6000], Train Loss: 0.0253, Test Loss: 0.0346\n",
            "Epoch [3529/6000], Train Loss: 0.0252, Test Loss: 0.0346\n",
            "Epoch [3530/6000], Train Loss: 0.0252, Test Loss: 0.0345\n",
            "Epoch [3531/6000], Train Loss: 0.0252, Test Loss: 0.0345\n",
            "Epoch [3532/6000], Train Loss: 0.0252, Test Loss: 0.0344\n",
            "Epoch [3533/6000], Train Loss: 0.0252, Test Loss: 0.0344\n",
            "Epoch [3534/6000], Train Loss: 0.0251, Test Loss: 0.0344\n",
            "Epoch [3535/6000], Train Loss: 0.0251, Test Loss: 0.0344\n",
            "Epoch [3536/6000], Train Loss: 0.0251, Test Loss: 0.0343\n",
            "Epoch [3537/6000], Train Loss: 0.0251, Test Loss: 0.0343\n",
            "Epoch [3538/6000], Train Loss: 0.0251, Test Loss: 0.0343\n",
            "Epoch [3539/6000], Train Loss: 0.0250, Test Loss: 0.0342\n",
            "Epoch [3540/6000], Train Loss: 0.0250, Test Loss: 0.0342\n",
            "Epoch [3541/6000], Train Loss: 0.0250, Test Loss: 0.0343\n",
            "Epoch [3542/6000], Train Loss: 0.0250, Test Loss: 0.0343\n",
            "Epoch [3543/6000], Train Loss: 0.0249, Test Loss: 0.0342\n",
            "Epoch [3544/6000], Train Loss: 0.0249, Test Loss: 0.0342\n",
            "Epoch [3545/6000], Train Loss: 0.0249, Test Loss: 0.0341\n",
            "Epoch [3546/6000], Train Loss: 0.0249, Test Loss: 0.0341\n",
            "Epoch [3547/6000], Train Loss: 0.0249, Test Loss: 0.0341\n",
            "Epoch [3548/6000], Train Loss: 0.0248, Test Loss: 0.0340\n",
            "Epoch [3549/6000], Train Loss: 0.0248, Test Loss: 0.0340\n",
            "Epoch [3550/6000], Train Loss: 0.0248, Test Loss: 0.0340\n",
            "Epoch [3551/6000], Train Loss: 0.0248, Test Loss: 0.0340\n",
            "Epoch [3552/6000], Train Loss: 0.0247, Test Loss: 0.0339\n",
            "Epoch [3553/6000], Train Loss: 0.0247, Test Loss: 0.0339\n",
            "Epoch [3554/6000], Train Loss: 0.0247, Test Loss: 0.0339\n",
            "Epoch [3555/6000], Train Loss: 0.0247, Test Loss: 0.0339\n",
            "Epoch [3556/6000], Train Loss: 0.0247, Test Loss: 0.0339\n",
            "Epoch [3557/6000], Train Loss: 0.0246, Test Loss: 0.0339\n",
            "Epoch [3558/6000], Train Loss: 0.0246, Test Loss: 0.0338\n",
            "Epoch [3559/6000], Train Loss: 0.0246, Test Loss: 0.0338\n",
            "Epoch [3560/6000], Train Loss: 0.0246, Test Loss: 0.0337\n",
            "Epoch [3561/6000], Train Loss: 0.0246, Test Loss: 0.0337\n",
            "Epoch [3562/6000], Train Loss: 0.0245, Test Loss: 0.0337\n",
            "Epoch [3563/6000], Train Loss: 0.0245, Test Loss: 0.0337\n",
            "Epoch [3564/6000], Train Loss: 0.0245, Test Loss: 0.0337\n",
            "Epoch [3565/6000], Train Loss: 0.0245, Test Loss: 0.0336\n",
            "Epoch [3566/6000], Train Loss: 0.0245, Test Loss: 0.0337\n",
            "Epoch [3567/6000], Train Loss: 0.0244, Test Loss: 0.0336\n",
            "Epoch [3568/6000], Train Loss: 0.0244, Test Loss: 0.0336\n",
            "Epoch [3569/6000], Train Loss: 0.0244, Test Loss: 0.0336\n",
            "Epoch [3570/6000], Train Loss: 0.0244, Test Loss: 0.0335\n",
            "Epoch [3571/6000], Train Loss: 0.0243, Test Loss: 0.0335\n",
            "Epoch [3572/6000], Train Loss: 0.0243, Test Loss: 0.0335\n",
            "Epoch [3573/6000], Train Loss: 0.0243, Test Loss: 0.0335\n",
            "Epoch [3574/6000], Train Loss: 0.0243, Test Loss: 0.0334\n",
            "Epoch [3575/6000], Train Loss: 0.0243, Test Loss: 0.0334\n",
            "Epoch [3576/6000], Train Loss: 0.0242, Test Loss: 0.0333\n",
            "Epoch [3577/6000], Train Loss: 0.0242, Test Loss: 0.0333\n",
            "Epoch [3578/6000], Train Loss: 0.0242, Test Loss: 0.0333\n",
            "Epoch [3579/6000], Train Loss: 0.0242, Test Loss: 0.0334\n",
            "Epoch [3580/6000], Train Loss: 0.0242, Test Loss: 0.0333\n",
            "Epoch [3581/6000], Train Loss: 0.0241, Test Loss: 0.0333\n",
            "Epoch [3582/6000], Train Loss: 0.0241, Test Loss: 0.0333\n",
            "Epoch [3583/6000], Train Loss: 0.0241, Test Loss: 0.0333\n",
            "Epoch [3584/6000], Train Loss: 0.0241, Test Loss: 0.0332\n",
            "Epoch [3585/6000], Train Loss: 0.0241, Test Loss: 0.0332\n",
            "Epoch [3586/6000], Train Loss: 0.0240, Test Loss: 0.0332\n",
            "Epoch [3587/6000], Train Loss: 0.0240, Test Loss: 0.0331\n",
            "Epoch [3588/6000], Train Loss: 0.0240, Test Loss: 0.0331\n",
            "Epoch [3589/6000], Train Loss: 0.0240, Test Loss: 0.0331\n",
            "Epoch [3590/6000], Train Loss: 0.0239, Test Loss: 0.0331\n",
            "Epoch [3591/6000], Train Loss: 0.0239, Test Loss: 0.0331\n",
            "Epoch [3592/6000], Train Loss: 0.0239, Test Loss: 0.0331\n",
            "Epoch [3593/6000], Train Loss: 0.0239, Test Loss: 0.0331\n",
            "Epoch [3594/6000], Train Loss: 0.0239, Test Loss: 0.0330\n",
            "Epoch [3595/6000], Train Loss: 0.0238, Test Loss: 0.0330\n",
            "Epoch [3596/6000], Train Loss: 0.0238, Test Loss: 0.0330\n",
            "Epoch [3597/6000], Train Loss: 0.0238, Test Loss: 0.0329\n",
            "Epoch [3598/6000], Train Loss: 0.0238, Test Loss: 0.0329\n",
            "Epoch [3599/6000], Train Loss: 0.0238, Test Loss: 0.0329\n",
            "Epoch [3600/6000], Train Loss: 0.0237, Test Loss: 0.0329\n",
            "Epoch [3601/6000], Train Loss: 0.0237, Test Loss: 0.0329\n",
            "Epoch [3602/6000], Train Loss: 0.0237, Test Loss: 0.0328\n",
            "Epoch [3603/6000], Train Loss: 0.0237, Test Loss: 0.0328\n",
            "Epoch [3604/6000], Train Loss: 0.0237, Test Loss: 0.0328\n",
            "Epoch [3605/6000], Train Loss: 0.0236, Test Loss: 0.0328\n",
            "Epoch [3606/6000], Train Loss: 0.0236, Test Loss: 0.0327\n",
            "Epoch [3607/6000], Train Loss: 0.0236, Test Loss: 0.0327\n",
            "Epoch [3608/6000], Train Loss: 0.0236, Test Loss: 0.0327\n",
            "Epoch [3609/6000], Train Loss: 0.0236, Test Loss: 0.0326\n",
            "Epoch [3610/6000], Train Loss: 0.0235, Test Loss: 0.0326\n",
            "Epoch [3611/6000], Train Loss: 0.0235, Test Loss: 0.0326\n",
            "Epoch [3612/6000], Train Loss: 0.0235, Test Loss: 0.0326\n",
            "Epoch [3613/6000], Train Loss: 0.0235, Test Loss: 0.0326\n",
            "Epoch [3614/6000], Train Loss: 0.0235, Test Loss: 0.0326\n",
            "Epoch [3615/6000], Train Loss: 0.0234, Test Loss: 0.0325\n",
            "Epoch [3616/6000], Train Loss: 0.0234, Test Loss: 0.0325\n",
            "Epoch [3617/6000], Train Loss: 0.0234, Test Loss: 0.0324\n",
            "Epoch [3618/6000], Train Loss: 0.0234, Test Loss: 0.0325\n",
            "Epoch [3619/6000], Train Loss: 0.0234, Test Loss: 0.0325\n",
            "Epoch [3620/6000], Train Loss: 0.0233, Test Loss: 0.0324\n",
            "Epoch [3621/6000], Train Loss: 0.0233, Test Loss: 0.0324\n",
            "Epoch [3622/6000], Train Loss: 0.0233, Test Loss: 0.0323\n",
            "Epoch [3623/6000], Train Loss: 0.0233, Test Loss: 0.0323\n",
            "Epoch [3624/6000], Train Loss: 0.0233, Test Loss: 0.0323\n",
            "Epoch [3625/6000], Train Loss: 0.0232, Test Loss: 0.0323\n",
            "Epoch [3626/6000], Train Loss: 0.0232, Test Loss: 0.0323\n",
            "Epoch [3627/6000], Train Loss: 0.0232, Test Loss: 0.0323\n",
            "Epoch [3628/6000], Train Loss: 0.0232, Test Loss: 0.0323\n",
            "Epoch [3629/6000], Train Loss: 0.0232, Test Loss: 0.0322\n",
            "Epoch [3630/6000], Train Loss: 0.0231, Test Loss: 0.0322\n",
            "Epoch [3631/6000], Train Loss: 0.0231, Test Loss: 0.0322\n",
            "Epoch [3632/6000], Train Loss: 0.0231, Test Loss: 0.0322\n",
            "Epoch [3633/6000], Train Loss: 0.0231, Test Loss: 0.0322\n",
            "Epoch [3634/6000], Train Loss: 0.0231, Test Loss: 0.0321\n",
            "Epoch [3635/6000], Train Loss: 0.0230, Test Loss: 0.0321\n",
            "Epoch [3636/6000], Train Loss: 0.0230, Test Loss: 0.0320\n",
            "Epoch [3637/6000], Train Loss: 0.0230, Test Loss: 0.0320\n",
            "Epoch [3638/6000], Train Loss: 0.0230, Test Loss: 0.0320\n",
            "Epoch [3639/6000], Train Loss: 0.0229, Test Loss: 0.0320\n",
            "Epoch [3640/6000], Train Loss: 0.0229, Test Loss: 0.0320\n",
            "Epoch [3641/6000], Train Loss: 0.0229, Test Loss: 0.0319\n",
            "Epoch [3642/6000], Train Loss: 0.0229, Test Loss: 0.0320\n",
            "Epoch [3643/6000], Train Loss: 0.0229, Test Loss: 0.0319\n",
            "Epoch [3644/6000], Train Loss: 0.0229, Test Loss: 0.0319\n",
            "Epoch [3645/6000], Train Loss: 0.0228, Test Loss: 0.0318\n",
            "Epoch [3646/6000], Train Loss: 0.0228, Test Loss: 0.0318\n",
            "Epoch [3647/6000], Train Loss: 0.0228, Test Loss: 0.0318\n",
            "Epoch [3648/6000], Train Loss: 0.0228, Test Loss: 0.0318\n",
            "Epoch [3649/6000], Train Loss: 0.0228, Test Loss: 0.0318\n",
            "Epoch [3650/6000], Train Loss: 0.0227, Test Loss: 0.0318\n",
            "Epoch [3651/6000], Train Loss: 0.0227, Test Loss: 0.0317\n",
            "Epoch [3652/6000], Train Loss: 0.0227, Test Loss: 0.0317\n",
            "Epoch [3653/6000], Train Loss: 0.0227, Test Loss: 0.0316\n",
            "Epoch [3654/6000], Train Loss: 0.0227, Test Loss: 0.0316\n",
            "Epoch [3655/6000], Train Loss: 0.0226, Test Loss: 0.0316\n",
            "Epoch [3656/6000], Train Loss: 0.0226, Test Loss: 0.0317\n",
            "Epoch [3657/6000], Train Loss: 0.0226, Test Loss: 0.0316\n",
            "Epoch [3658/6000], Train Loss: 0.0226, Test Loss: 0.0316\n",
            "Epoch [3659/6000], Train Loss: 0.0226, Test Loss: 0.0315\n",
            "Epoch [3660/6000], Train Loss: 0.0225, Test Loss: 0.0315\n",
            "Epoch [3661/6000], Train Loss: 0.0225, Test Loss: 0.0315\n",
            "Epoch [3662/6000], Train Loss: 0.0225, Test Loss: 0.0315\n",
            "Epoch [3663/6000], Train Loss: 0.0225, Test Loss: 0.0315\n",
            "Epoch [3664/6000], Train Loss: 0.0225, Test Loss: 0.0315\n",
            "Epoch [3665/6000], Train Loss: 0.0224, Test Loss: 0.0314\n",
            "Epoch [3666/6000], Train Loss: 0.0224, Test Loss: 0.0314\n",
            "Epoch [3667/6000], Train Loss: 0.0224, Test Loss: 0.0314\n",
            "Epoch [3668/6000], Train Loss: 0.0224, Test Loss: 0.0314\n",
            "Epoch [3669/6000], Train Loss: 0.0224, Test Loss: 0.0313\n",
            "Epoch [3670/6000], Train Loss: 0.0223, Test Loss: 0.0313\n",
            "Epoch [3671/6000], Train Loss: 0.0223, Test Loss: 0.0313\n",
            "Epoch [3672/6000], Train Loss: 0.0223, Test Loss: 0.0313\n",
            "Epoch [3673/6000], Train Loss: 0.0223, Test Loss: 0.0312\n",
            "Epoch [3674/6000], Train Loss: 0.0223, Test Loss: 0.0312\n",
            "Epoch [3675/6000], Train Loss: 0.0223, Test Loss: 0.0312\n",
            "Epoch [3676/6000], Train Loss: 0.0222, Test Loss: 0.0311\n",
            "Epoch [3677/6000], Train Loss: 0.0222, Test Loss: 0.0311\n",
            "Epoch [3678/6000], Train Loss: 0.0222, Test Loss: 0.0311\n",
            "Epoch [3679/6000], Train Loss: 0.0222, Test Loss: 0.0311\n",
            "Epoch [3680/6000], Train Loss: 0.0222, Test Loss: 0.0311\n",
            "Epoch [3681/6000], Train Loss: 0.0221, Test Loss: 0.0311\n",
            "Epoch [3682/6000], Train Loss: 0.0221, Test Loss: 0.0311\n",
            "Epoch [3683/6000], Train Loss: 0.0221, Test Loss: 0.0311\n",
            "Epoch [3684/6000], Train Loss: 0.0221, Test Loss: 0.0310\n",
            "Epoch [3685/6000], Train Loss: 0.0221, Test Loss: 0.0310\n",
            "Epoch [3686/6000], Train Loss: 0.0220, Test Loss: 0.0309\n",
            "Epoch [3687/6000], Train Loss: 0.0220, Test Loss: 0.0309\n",
            "Epoch [3688/6000], Train Loss: 0.0220, Test Loss: 0.0310\n",
            "Epoch [3689/6000], Train Loss: 0.0220, Test Loss: 0.0310\n",
            "Epoch [3690/6000], Train Loss: 0.0220, Test Loss: 0.0309\n",
            "Epoch [3691/6000], Train Loss: 0.0219, Test Loss: 0.0309\n",
            "Epoch [3692/6000], Train Loss: 0.0219, Test Loss: 0.0308\n",
            "Epoch [3693/6000], Train Loss: 0.0219, Test Loss: 0.0308\n",
            "Epoch [3694/6000], Train Loss: 0.0219, Test Loss: 0.0307\n",
            "Epoch [3695/6000], Train Loss: 0.0219, Test Loss: 0.0307\n",
            "Epoch [3696/6000], Train Loss: 0.0218, Test Loss: 0.0307\n",
            "Epoch [3697/6000], Train Loss: 0.0218, Test Loss: 0.0307\n",
            "Epoch [3698/6000], Train Loss: 0.0218, Test Loss: 0.0307\n",
            "Epoch [3699/6000], Train Loss: 0.0218, Test Loss: 0.0307\n",
            "Epoch [3700/6000], Train Loss: 0.0218, Test Loss: 0.0306\n",
            "Epoch [3701/6000], Train Loss: 0.0218, Test Loss: 0.0306\n",
            "Epoch [3702/6000], Train Loss: 0.0217, Test Loss: 0.0305\n",
            "Epoch [3703/6000], Train Loss: 0.0217, Test Loss: 0.0306\n",
            "Epoch [3704/6000], Train Loss: 0.0217, Test Loss: 0.0306\n",
            "Epoch [3705/6000], Train Loss: 0.0217, Test Loss: 0.0306\n",
            "Epoch [3706/6000], Train Loss: 0.0217, Test Loss: 0.0305\n",
            "Epoch [3707/6000], Train Loss: 0.0216, Test Loss: 0.0305\n",
            "Epoch [3708/6000], Train Loss: 0.0216, Test Loss: 0.0305\n",
            "Epoch [3709/6000], Train Loss: 0.0216, Test Loss: 0.0304\n",
            "Epoch [3710/6000], Train Loss: 0.0216, Test Loss: 0.0304\n",
            "Epoch [3711/6000], Train Loss: 0.0216, Test Loss: 0.0304\n",
            "Epoch [3712/6000], Train Loss: 0.0215, Test Loss: 0.0304\n",
            "Epoch [3713/6000], Train Loss: 0.0215, Test Loss: 0.0304\n",
            "Epoch [3714/6000], Train Loss: 0.0215, Test Loss: 0.0303\n",
            "Epoch [3715/6000], Train Loss: 0.0215, Test Loss: 0.0303\n",
            "Epoch [3716/6000], Train Loss: 0.0215, Test Loss: 0.0303\n",
            "Epoch [3717/6000], Train Loss: 0.0215, Test Loss: 0.0303\n",
            "Epoch [3718/6000], Train Loss: 0.0214, Test Loss: 0.0303\n",
            "Epoch [3719/6000], Train Loss: 0.0214, Test Loss: 0.0302\n",
            "Epoch [3720/6000], Train Loss: 0.0214, Test Loss: 0.0302\n",
            "Epoch [3721/6000], Train Loss: 0.0214, Test Loss: 0.0302\n",
            "Epoch [3722/6000], Train Loss: 0.0214, Test Loss: 0.0302\n",
            "Epoch [3723/6000], Train Loss: 0.0214, Test Loss: 0.0302\n",
            "Epoch [3724/6000], Train Loss: 0.0213, Test Loss: 0.0302\n",
            "Epoch [3725/6000], Train Loss: 0.0213, Test Loss: 0.0301\n",
            "Epoch [3726/6000], Train Loss: 0.0213, Test Loss: 0.0301\n",
            "Epoch [3727/6000], Train Loss: 0.0213, Test Loss: 0.0301\n",
            "Epoch [3728/6000], Train Loss: 0.0213, Test Loss: 0.0301\n",
            "Epoch [3729/6000], Train Loss: 0.0212, Test Loss: 0.0300\n",
            "Epoch [3730/6000], Train Loss: 0.0212, Test Loss: 0.0300\n",
            "Epoch [3731/6000], Train Loss: 0.0212, Test Loss: 0.0300\n",
            "Epoch [3732/6000], Train Loss: 0.0212, Test Loss: 0.0300\n",
            "Epoch [3733/6000], Train Loss: 0.0212, Test Loss: 0.0299\n",
            "Epoch [3734/6000], Train Loss: 0.0211, Test Loss: 0.0299\n",
            "Epoch [3735/6000], Train Loss: 0.0211, Test Loss: 0.0299\n",
            "Epoch [3736/6000], Train Loss: 0.0211, Test Loss: 0.0299\n",
            "Epoch [3737/6000], Train Loss: 0.0211, Test Loss: 0.0299\n",
            "Epoch [3738/6000], Train Loss: 0.0211, Test Loss: 0.0298\n",
            "Epoch [3739/6000], Train Loss: 0.0211, Test Loss: 0.0299\n",
            "Epoch [3740/6000], Train Loss: 0.0210, Test Loss: 0.0299\n",
            "Epoch [3741/6000], Train Loss: 0.0210, Test Loss: 0.0298\n",
            "Epoch [3742/6000], Train Loss: 0.0210, Test Loss: 0.0298\n",
            "Epoch [3743/6000], Train Loss: 0.0210, Test Loss: 0.0297\n",
            "Epoch [3744/6000], Train Loss: 0.0210, Test Loss: 0.0298\n",
            "Epoch [3745/6000], Train Loss: 0.0209, Test Loss: 0.0297\n",
            "Epoch [3746/6000], Train Loss: 0.0209, Test Loss: 0.0297\n",
            "Epoch [3747/6000], Train Loss: 0.0209, Test Loss: 0.0297\n",
            "Epoch [3748/6000], Train Loss: 0.0209, Test Loss: 0.0297\n",
            "Epoch [3749/6000], Train Loss: 0.0209, Test Loss: 0.0296\n",
            "Epoch [3750/6000], Train Loss: 0.0209, Test Loss: 0.0296\n",
            "Epoch [3751/6000], Train Loss: 0.0208, Test Loss: 0.0295\n",
            "Epoch [3752/6000], Train Loss: 0.0208, Test Loss: 0.0295\n",
            "Epoch [3753/6000], Train Loss: 0.0208, Test Loss: 0.0296\n",
            "Epoch [3754/6000], Train Loss: 0.0208, Test Loss: 0.0296\n",
            "Epoch [3755/6000], Train Loss: 0.0208, Test Loss: 0.0296\n",
            "Epoch [3756/6000], Train Loss: 0.0208, Test Loss: 0.0295\n",
            "Epoch [3757/6000], Train Loss: 0.0207, Test Loss: 0.0295\n",
            "Epoch [3758/6000], Train Loss: 0.0207, Test Loss: 0.0294\n",
            "Epoch [3759/6000], Train Loss: 0.0207, Test Loss: 0.0294\n",
            "Epoch [3760/6000], Train Loss: 0.0207, Test Loss: 0.0294\n",
            "Epoch [3761/6000], Train Loss: 0.0207, Test Loss: 0.0294\n",
            "Epoch [3762/6000], Train Loss: 0.0207, Test Loss: 0.0294\n",
            "Epoch [3763/6000], Train Loss: 0.0206, Test Loss: 0.0294\n",
            "Epoch [3764/6000], Train Loss: 0.0206, Test Loss: 0.0294\n",
            "Epoch [3765/6000], Train Loss: 0.0206, Test Loss: 0.0293\n",
            "Epoch [3766/6000], Train Loss: 0.0206, Test Loss: 0.0292\n",
            "Epoch [3767/6000], Train Loss: 0.0206, Test Loss: 0.0292\n",
            "Epoch [3768/6000], Train Loss: 0.0205, Test Loss: 0.0292\n",
            "Epoch [3769/6000], Train Loss: 0.0205, Test Loss: 0.0292\n",
            "Epoch [3770/6000], Train Loss: 0.0205, Test Loss: 0.0292\n",
            "Epoch [3771/6000], Train Loss: 0.0205, Test Loss: 0.0292\n",
            "Epoch [3772/6000], Train Loss: 0.0205, Test Loss: 0.0291\n",
            "Epoch [3773/6000], Train Loss: 0.0205, Test Loss: 0.0291\n",
            "Epoch [3774/6000], Train Loss: 0.0204, Test Loss: 0.0290\n",
            "Epoch [3775/6000], Train Loss: 0.0204, Test Loss: 0.0291\n",
            "Epoch [3776/6000], Train Loss: 0.0204, Test Loss: 0.0291\n",
            "Epoch [3777/6000], Train Loss: 0.0204, Test Loss: 0.0291\n",
            "Epoch [3778/6000], Train Loss: 0.0204, Test Loss: 0.0291\n",
            "Epoch [3779/6000], Train Loss: 0.0203, Test Loss: 0.0290\n",
            "Epoch [3780/6000], Train Loss: 0.0203, Test Loss: 0.0289\n",
            "Epoch [3781/6000], Train Loss: 0.0203, Test Loss: 0.0289\n",
            "Epoch [3782/6000], Train Loss: 0.0203, Test Loss: 0.0289\n",
            "Epoch [3783/6000], Train Loss: 0.0203, Test Loss: 0.0289\n",
            "Epoch [3784/6000], Train Loss: 0.0203, Test Loss: 0.0289\n",
            "Epoch [3785/6000], Train Loss: 0.0202, Test Loss: 0.0289\n",
            "Epoch [3786/6000], Train Loss: 0.0202, Test Loss: 0.0289\n",
            "Epoch [3787/6000], Train Loss: 0.0202, Test Loss: 0.0289\n",
            "Epoch [3788/6000], Train Loss: 0.0202, Test Loss: 0.0288\n",
            "Epoch [3789/6000], Train Loss: 0.0202, Test Loss: 0.0288\n",
            "Epoch [3790/6000], Train Loss: 0.0202, Test Loss: 0.0288\n",
            "Epoch [3791/6000], Train Loss: 0.0201, Test Loss: 0.0287\n",
            "Epoch [3792/6000], Train Loss: 0.0201, Test Loss: 0.0287\n",
            "Epoch [3793/6000], Train Loss: 0.0201, Test Loss: 0.0287\n",
            "Epoch [3794/6000], Train Loss: 0.0201, Test Loss: 0.0286\n",
            "Epoch [3795/6000], Train Loss: 0.0201, Test Loss: 0.0286\n",
            "Epoch [3796/6000], Train Loss: 0.0200, Test Loss: 0.0286\n",
            "Epoch [3797/6000], Train Loss: 0.0200, Test Loss: 0.0287\n",
            "Epoch [3798/6000], Train Loss: 0.0200, Test Loss: 0.0286\n",
            "Epoch [3799/6000], Train Loss: 0.0200, Test Loss: 0.0286\n",
            "Epoch [3800/6000], Train Loss: 0.0200, Test Loss: 0.0286\n",
            "Epoch [3801/6000], Train Loss: 0.0200, Test Loss: 0.0285\n",
            "Epoch [3802/6000], Train Loss: 0.0199, Test Loss: 0.0285\n",
            "Epoch [3803/6000], Train Loss: 0.0199, Test Loss: 0.0285\n",
            "Epoch [3804/6000], Train Loss: 0.0199, Test Loss: 0.0284\n",
            "Epoch [3805/6000], Train Loss: 0.0199, Test Loss: 0.0285\n",
            "Epoch [3806/6000], Train Loss: 0.0199, Test Loss: 0.0285\n",
            "Epoch [3807/6000], Train Loss: 0.0199, Test Loss: 0.0284\n",
            "Epoch [3808/6000], Train Loss: 0.0198, Test Loss: 0.0284\n",
            "Epoch [3809/6000], Train Loss: 0.0198, Test Loss: 0.0283\n",
            "Epoch [3810/6000], Train Loss: 0.0198, Test Loss: 0.0284\n",
            "Epoch [3811/6000], Train Loss: 0.0198, Test Loss: 0.0284\n",
            "Epoch [3812/6000], Train Loss: 0.0198, Test Loss: 0.0283\n",
            "Epoch [3813/6000], Train Loss: 0.0198, Test Loss: 0.0283\n",
            "Epoch [3814/6000], Train Loss: 0.0197, Test Loss: 0.0283\n",
            "Epoch [3815/6000], Train Loss: 0.0197, Test Loss: 0.0283\n",
            "Epoch [3816/6000], Train Loss: 0.0197, Test Loss: 0.0282\n",
            "Epoch [3817/6000], Train Loss: 0.0197, Test Loss: 0.0282\n",
            "Epoch [3818/6000], Train Loss: 0.0197, Test Loss: 0.0282\n",
            "Epoch [3819/6000], Train Loss: 0.0197, Test Loss: 0.0282\n",
            "Epoch [3820/6000], Train Loss: 0.0196, Test Loss: 0.0281\n",
            "Epoch [3821/6000], Train Loss: 0.0196, Test Loss: 0.0281\n",
            "Epoch [3822/6000], Train Loss: 0.0196, Test Loss: 0.0281\n",
            "Epoch [3823/6000], Train Loss: 0.0196, Test Loss: 0.0281\n",
            "Epoch [3824/6000], Train Loss: 0.0196, Test Loss: 0.0281\n",
            "Epoch [3825/6000], Train Loss: 0.0196, Test Loss: 0.0281\n",
            "Epoch [3826/6000], Train Loss: 0.0195, Test Loss: 0.0281\n",
            "Epoch [3827/6000], Train Loss: 0.0195, Test Loss: 0.0281\n",
            "Epoch [3828/6000], Train Loss: 0.0195, Test Loss: 0.0280\n",
            "Epoch [3829/6000], Train Loss: 0.0195, Test Loss: 0.0280\n",
            "Epoch [3830/6000], Train Loss: 0.0195, Test Loss: 0.0280\n",
            "Epoch [3831/6000], Train Loss: 0.0195, Test Loss: 0.0279\n",
            "Epoch [3832/6000], Train Loss: 0.0194, Test Loss: 0.0279\n",
            "Epoch [3833/6000], Train Loss: 0.0194, Test Loss: 0.0279\n",
            "Epoch [3834/6000], Train Loss: 0.0194, Test Loss: 0.0279\n",
            "Epoch [3835/6000], Train Loss: 0.0194, Test Loss: 0.0279\n",
            "Epoch [3836/6000], Train Loss: 0.0194, Test Loss: 0.0279\n",
            "Epoch [3837/6000], Train Loss: 0.0194, Test Loss: 0.0278\n",
            "Epoch [3838/6000], Train Loss: 0.0193, Test Loss: 0.0278\n",
            "Epoch [3839/6000], Train Loss: 0.0193, Test Loss: 0.0278\n",
            "Epoch [3840/6000], Train Loss: 0.0193, Test Loss: 0.0278\n",
            "Epoch [3841/6000], Train Loss: 0.0193, Test Loss: 0.0278\n",
            "Epoch [3842/6000], Train Loss: 0.0193, Test Loss: 0.0278\n",
            "Epoch [3843/6000], Train Loss: 0.0193, Test Loss: 0.0277\n",
            "Epoch [3844/6000], Train Loss: 0.0192, Test Loss: 0.0277\n",
            "Epoch [3845/6000], Train Loss: 0.0192, Test Loss: 0.0277\n",
            "Epoch [3846/6000], Train Loss: 0.0192, Test Loss: 0.0277\n",
            "Epoch [3847/6000], Train Loss: 0.0192, Test Loss: 0.0277\n",
            "Epoch [3848/6000], Train Loss: 0.0192, Test Loss: 0.0276\n",
            "Epoch [3849/6000], Train Loss: 0.0192, Test Loss: 0.0276\n",
            "Epoch [3850/6000], Train Loss: 0.0191, Test Loss: 0.0276\n",
            "Epoch [3851/6000], Train Loss: 0.0191, Test Loss: 0.0276\n",
            "Epoch [3852/6000], Train Loss: 0.0191, Test Loss: 0.0275\n",
            "Epoch [3853/6000], Train Loss: 0.0191, Test Loss: 0.0275\n",
            "Epoch [3854/6000], Train Loss: 0.0191, Test Loss: 0.0275\n",
            "Epoch [3855/6000], Train Loss: 0.0191, Test Loss: 0.0275\n",
            "Epoch [3856/6000], Train Loss: 0.0190, Test Loss: 0.0275\n",
            "Epoch [3857/6000], Train Loss: 0.0190, Test Loss: 0.0274\n",
            "Epoch [3858/6000], Train Loss: 0.0190, Test Loss: 0.0274\n",
            "Epoch [3859/6000], Train Loss: 0.0190, Test Loss: 0.0274\n",
            "Epoch [3860/6000], Train Loss: 0.0190, Test Loss: 0.0275\n",
            "Epoch [3861/6000], Train Loss: 0.0190, Test Loss: 0.0274\n",
            "Epoch [3862/6000], Train Loss: 0.0189, Test Loss: 0.0274\n",
            "Epoch [3863/6000], Train Loss: 0.0189, Test Loss: 0.0273\n",
            "Epoch [3864/6000], Train Loss: 0.0189, Test Loss: 0.0272\n",
            "Epoch [3865/6000], Train Loss: 0.0189, Test Loss: 0.0272\n",
            "Epoch [3866/6000], Train Loss: 0.0189, Test Loss: 0.0272\n",
            "Epoch [3867/6000], Train Loss: 0.0189, Test Loss: 0.0272\n",
            "Epoch [3868/6000], Train Loss: 0.0189, Test Loss: 0.0272\n",
            "Epoch [3869/6000], Train Loss: 0.0188, Test Loss: 0.0272\n",
            "Epoch [3870/6000], Train Loss: 0.0188, Test Loss: 0.0272\n",
            "Epoch [3871/6000], Train Loss: 0.0188, Test Loss: 0.0272\n",
            "Epoch [3872/6000], Train Loss: 0.0188, Test Loss: 0.0271\n",
            "Epoch [3873/6000], Train Loss: 0.0188, Test Loss: 0.0271\n",
            "Epoch [3874/6000], Train Loss: 0.0188, Test Loss: 0.0271\n",
            "Epoch [3875/6000], Train Loss: 0.0187, Test Loss: 0.0271\n",
            "Epoch [3876/6000], Train Loss: 0.0187, Test Loss: 0.0271\n",
            "Epoch [3877/6000], Train Loss: 0.0187, Test Loss: 0.0271\n",
            "Epoch [3878/6000], Train Loss: 0.0187, Test Loss: 0.0270\n",
            "Epoch [3879/6000], Train Loss: 0.0187, Test Loss: 0.0270\n",
            "Epoch [3880/6000], Train Loss: 0.0187, Test Loss: 0.0269\n",
            "Epoch [3881/6000], Train Loss: 0.0186, Test Loss: 0.0270\n",
            "Epoch [3882/6000], Train Loss: 0.0186, Test Loss: 0.0270\n",
            "Epoch [3883/6000], Train Loss: 0.0186, Test Loss: 0.0270\n",
            "Epoch [3884/6000], Train Loss: 0.0186, Test Loss: 0.0270\n",
            "Epoch [3885/6000], Train Loss: 0.0186, Test Loss: 0.0270\n",
            "Epoch [3886/6000], Train Loss: 0.0186, Test Loss: 0.0269\n",
            "Epoch [3887/6000], Train Loss: 0.0185, Test Loss: 0.0268\n",
            "Epoch [3888/6000], Train Loss: 0.0185, Test Loss: 0.0268\n",
            "Epoch [3889/6000], Train Loss: 0.0185, Test Loss: 0.0268\n",
            "Epoch [3890/6000], Train Loss: 0.0185, Test Loss: 0.0268\n",
            "Epoch [3891/6000], Train Loss: 0.0185, Test Loss: 0.0268\n",
            "Epoch [3892/6000], Train Loss: 0.0185, Test Loss: 0.0268\n",
            "Epoch [3893/6000], Train Loss: 0.0185, Test Loss: 0.0268\n",
            "Epoch [3894/6000], Train Loss: 0.0184, Test Loss: 0.0268\n",
            "Epoch [3895/6000], Train Loss: 0.0184, Test Loss: 0.0267\n",
            "Epoch [3896/6000], Train Loss: 0.0184, Test Loss: 0.0267\n",
            "Epoch [3897/6000], Train Loss: 0.0184, Test Loss: 0.0266\n",
            "Epoch [3898/6000], Train Loss: 0.0184, Test Loss: 0.0267\n",
            "Epoch [3899/6000], Train Loss: 0.0184, Test Loss: 0.0266\n",
            "Epoch [3900/6000], Train Loss: 0.0183, Test Loss: 0.0266\n",
            "Epoch [3901/6000], Train Loss: 0.0183, Test Loss: 0.0266\n",
            "Epoch [3902/6000], Train Loss: 0.0183, Test Loss: 0.0266\n",
            "Epoch [3903/6000], Train Loss: 0.0183, Test Loss: 0.0265\n",
            "Epoch [3904/6000], Train Loss: 0.0183, Test Loss: 0.0265\n",
            "Epoch [3905/6000], Train Loss: 0.0183, Test Loss: 0.0265\n",
            "Epoch [3906/6000], Train Loss: 0.0183, Test Loss: 0.0265\n",
            "Epoch [3907/6000], Train Loss: 0.0182, Test Loss: 0.0265\n",
            "Epoch [3908/6000], Train Loss: 0.0182, Test Loss: 0.0265\n",
            "Epoch [3909/6000], Train Loss: 0.0182, Test Loss: 0.0264\n",
            "Epoch [3910/6000], Train Loss: 0.0182, Test Loss: 0.0264\n",
            "Epoch [3911/6000], Train Loss: 0.0182, Test Loss: 0.0264\n",
            "Epoch [3912/6000], Train Loss: 0.0182, Test Loss: 0.0264\n",
            "Epoch [3913/6000], Train Loss: 0.0181, Test Loss: 0.0264\n",
            "Epoch [3914/6000], Train Loss: 0.0181, Test Loss: 0.0263\n",
            "Epoch [3915/6000], Train Loss: 0.0181, Test Loss: 0.0263\n",
            "Epoch [3916/6000], Train Loss: 0.0181, Test Loss: 0.0263\n",
            "Epoch [3917/6000], Train Loss: 0.0181, Test Loss: 0.0263\n",
            "Epoch [3918/6000], Train Loss: 0.0181, Test Loss: 0.0263\n",
            "Epoch [3919/6000], Train Loss: 0.0180, Test Loss: 0.0263\n",
            "Epoch [3920/6000], Train Loss: 0.0180, Test Loss: 0.0263\n",
            "Epoch [3921/6000], Train Loss: 0.0180, Test Loss: 0.0263\n",
            "Epoch [3922/6000], Train Loss: 0.0180, Test Loss: 0.0262\n",
            "Epoch [3923/6000], Train Loss: 0.0180, Test Loss: 0.0262\n",
            "Epoch [3924/6000], Train Loss: 0.0180, Test Loss: 0.0261\n",
            "Epoch [3925/6000], Train Loss: 0.0180, Test Loss: 0.0261\n",
            "Epoch [3926/6000], Train Loss: 0.0179, Test Loss: 0.0261\n",
            "Epoch [3927/6000], Train Loss: 0.0179, Test Loss: 0.0262\n",
            "Epoch [3928/6000], Train Loss: 0.0179, Test Loss: 0.0261\n",
            "Epoch [3929/6000], Train Loss: 0.0179, Test Loss: 0.0261\n",
            "Epoch [3930/6000], Train Loss: 0.0179, Test Loss: 0.0261\n",
            "Epoch [3931/6000], Train Loss: 0.0179, Test Loss: 0.0260\n",
            "Epoch [3932/6000], Train Loss: 0.0178, Test Loss: 0.0260\n",
            "Epoch [3933/6000], Train Loss: 0.0178, Test Loss: 0.0260\n",
            "Epoch [3934/6000], Train Loss: 0.0178, Test Loss: 0.0260\n",
            "Epoch [3935/6000], Train Loss: 0.0178, Test Loss: 0.0260\n",
            "Epoch [3936/6000], Train Loss: 0.0178, Test Loss: 0.0260\n",
            "Epoch [3937/6000], Train Loss: 0.0178, Test Loss: 0.0259\n",
            "Epoch [3938/6000], Train Loss: 0.0178, Test Loss: 0.0259\n",
            "Epoch [3939/6000], Train Loss: 0.0177, Test Loss: 0.0259\n",
            "Epoch [3940/6000], Train Loss: 0.0177, Test Loss: 0.0258\n",
            "Epoch [3941/6000], Train Loss: 0.0177, Test Loss: 0.0259\n",
            "Epoch [3942/6000], Train Loss: 0.0177, Test Loss: 0.0259\n",
            "Epoch [3943/6000], Train Loss: 0.0177, Test Loss: 0.0259\n",
            "Epoch [3944/6000], Train Loss: 0.0177, Test Loss: 0.0258\n",
            "Epoch [3945/6000], Train Loss: 0.0177, Test Loss: 0.0258\n",
            "Epoch [3946/6000], Train Loss: 0.0176, Test Loss: 0.0257\n",
            "Epoch [3947/6000], Train Loss: 0.0176, Test Loss: 0.0257\n",
            "Epoch [3948/6000], Train Loss: 0.0176, Test Loss: 0.0257\n",
            "Epoch [3949/6000], Train Loss: 0.0176, Test Loss: 0.0257\n",
            "Epoch [3950/6000], Train Loss: 0.0176, Test Loss: 0.0257\n",
            "Epoch [3951/6000], Train Loss: 0.0176, Test Loss: 0.0257\n",
            "Epoch [3952/6000], Train Loss: 0.0175, Test Loss: 0.0257\n",
            "Epoch [3953/6000], Train Loss: 0.0175, Test Loss: 0.0256\n",
            "Epoch [3954/6000], Train Loss: 0.0175, Test Loss: 0.0257\n",
            "Epoch [3955/6000], Train Loss: 0.0175, Test Loss: 0.0257\n",
            "Epoch [3956/6000], Train Loss: 0.0175, Test Loss: 0.0256\n",
            "Epoch [3957/6000], Train Loss: 0.0175, Test Loss: 0.0256\n",
            "Epoch [3958/6000], Train Loss: 0.0175, Test Loss: 0.0255\n",
            "Epoch [3959/6000], Train Loss: 0.0174, Test Loss: 0.0255\n",
            "Epoch [3960/6000], Train Loss: 0.0174, Test Loss: 0.0254\n",
            "Epoch [3961/6000], Train Loss: 0.0174, Test Loss: 0.0255\n",
            "Epoch [3962/6000], Train Loss: 0.0174, Test Loss: 0.0254\n",
            "Epoch [3963/6000], Train Loss: 0.0174, Test Loss: 0.0254\n",
            "Epoch [3964/6000], Train Loss: 0.0174, Test Loss: 0.0254\n",
            "Epoch [3965/6000], Train Loss: 0.0174, Test Loss: 0.0254\n",
            "Epoch [3966/6000], Train Loss: 0.0173, Test Loss: 0.0254\n",
            "Epoch [3967/6000], Train Loss: 0.0173, Test Loss: 0.0254\n",
            "Epoch [3968/6000], Train Loss: 0.0173, Test Loss: 0.0254\n",
            "Epoch [3969/6000], Train Loss: 0.0173, Test Loss: 0.0254\n",
            "Epoch [3970/6000], Train Loss: 0.0173, Test Loss: 0.0254\n",
            "Epoch [3971/6000], Train Loss: 0.0173, Test Loss: 0.0253\n",
            "Epoch [3972/6000], Train Loss: 0.0173, Test Loss: 0.0253\n",
            "Epoch [3973/6000], Train Loss: 0.0172, Test Loss: 0.0252\n",
            "Epoch [3974/6000], Train Loss: 0.0172, Test Loss: 0.0253\n",
            "Epoch [3975/6000], Train Loss: 0.0172, Test Loss: 0.0252\n",
            "Epoch [3976/6000], Train Loss: 0.0172, Test Loss: 0.0252\n",
            "Epoch [3977/6000], Train Loss: 0.0172, Test Loss: 0.0252\n",
            "Epoch [3978/6000], Train Loss: 0.0172, Test Loss: 0.0252\n",
            "Epoch [3979/6000], Train Loss: 0.0171, Test Loss: 0.0252\n",
            "Epoch [3980/6000], Train Loss: 0.0171, Test Loss: 0.0252\n",
            "Epoch [3981/6000], Train Loss: 0.0171, Test Loss: 0.0251\n",
            "Epoch [3982/6000], Train Loss: 0.0171, Test Loss: 0.0251\n",
            "Epoch [3983/6000], Train Loss: 0.0171, Test Loss: 0.0251\n",
            "Epoch [3984/6000], Train Loss: 0.0171, Test Loss: 0.0251\n",
            "Epoch [3985/6000], Train Loss: 0.0171, Test Loss: 0.0251\n",
            "Epoch [3986/6000], Train Loss: 0.0170, Test Loss: 0.0250\n",
            "Epoch [3987/6000], Train Loss: 0.0170, Test Loss: 0.0250\n",
            "Epoch [3988/6000], Train Loss: 0.0170, Test Loss: 0.0250\n",
            "Epoch [3989/6000], Train Loss: 0.0170, Test Loss: 0.0250\n",
            "Epoch [3990/6000], Train Loss: 0.0170, Test Loss: 0.0249\n",
            "Epoch [3991/6000], Train Loss: 0.0170, Test Loss: 0.0249\n",
            "Epoch [3992/6000], Train Loss: 0.0170, Test Loss: 0.0249\n",
            "Epoch [3993/6000], Train Loss: 0.0169, Test Loss: 0.0249\n",
            "Epoch [3994/6000], Train Loss: 0.0169, Test Loss: 0.0249\n",
            "Epoch [3995/6000], Train Loss: 0.0169, Test Loss: 0.0249\n",
            "Epoch [3996/6000], Train Loss: 0.0169, Test Loss: 0.0248\n",
            "Epoch [3997/6000], Train Loss: 0.0169, Test Loss: 0.0248\n",
            "Epoch [3998/6000], Train Loss: 0.0169, Test Loss: 0.0248\n",
            "Epoch [3999/6000], Train Loss: 0.0169, Test Loss: 0.0248\n",
            "Epoch [4000/6000], Train Loss: 0.0168, Test Loss: 0.0248\n",
            "Epoch [4001/6000], Train Loss: 0.0168, Test Loss: 0.0248\n",
            "Epoch [4002/6000], Train Loss: 0.0168, Test Loss: 0.0247\n",
            "Epoch [4003/6000], Train Loss: 0.0168, Test Loss: 0.0247\n",
            "Epoch [4004/6000], Train Loss: 0.0168, Test Loss: 0.0247\n",
            "Epoch [4005/6000], Train Loss: 0.0168, Test Loss: 0.0247\n",
            "Epoch [4006/6000], Train Loss: 0.0168, Test Loss: 0.0247\n",
            "Epoch [4007/6000], Train Loss: 0.0167, Test Loss: 0.0247\n",
            "Epoch [4008/6000], Train Loss: 0.0167, Test Loss: 0.0247\n",
            "Epoch [4009/6000], Train Loss: 0.0167, Test Loss: 0.0247\n",
            "Epoch [4010/6000], Train Loss: 0.0167, Test Loss: 0.0246\n",
            "Epoch [4011/6000], Train Loss: 0.0167, Test Loss: 0.0246\n",
            "Epoch [4012/6000], Train Loss: 0.0167, Test Loss: 0.0246\n",
            "Epoch [4013/6000], Train Loss: 0.0167, Test Loss: 0.0245\n",
            "Epoch [4014/6000], Train Loss: 0.0166, Test Loss: 0.0245\n",
            "Epoch [4015/6000], Train Loss: 0.0166, Test Loss: 0.0245\n",
            "Epoch [4016/6000], Train Loss: 0.0166, Test Loss: 0.0245\n",
            "Epoch [4017/6000], Train Loss: 0.0166, Test Loss: 0.0245\n",
            "Epoch [4018/6000], Train Loss: 0.0166, Test Loss: 0.0245\n",
            "Epoch [4019/6000], Train Loss: 0.0166, Test Loss: 0.0245\n",
            "Epoch [4020/6000], Train Loss: 0.0166, Test Loss: 0.0244\n",
            "Epoch [4021/6000], Train Loss: 0.0165, Test Loss: 0.0244\n",
            "Epoch [4022/6000], Train Loss: 0.0165, Test Loss: 0.0244\n",
            "Epoch [4023/6000], Train Loss: 0.0165, Test Loss: 0.0244\n",
            "Epoch [4024/6000], Train Loss: 0.0165, Test Loss: 0.0244\n",
            "Epoch [4025/6000], Train Loss: 0.0165, Test Loss: 0.0243\n",
            "Epoch [4026/6000], Train Loss: 0.0165, Test Loss: 0.0243\n",
            "Epoch [4027/6000], Train Loss: 0.0165, Test Loss: 0.0243\n",
            "Epoch [4028/6000], Train Loss: 0.0164, Test Loss: 0.0243\n",
            "Epoch [4029/6000], Train Loss: 0.0164, Test Loss: 0.0243\n",
            "Epoch [4030/6000], Train Loss: 0.0164, Test Loss: 0.0243\n",
            "Epoch [4031/6000], Train Loss: 0.0164, Test Loss: 0.0243\n",
            "Epoch [4032/6000], Train Loss: 0.0164, Test Loss: 0.0243\n",
            "Epoch [4033/6000], Train Loss: 0.0164, Test Loss: 0.0242\n",
            "Epoch [4034/6000], Train Loss: 0.0164, Test Loss: 0.0242\n",
            "Epoch [4035/6000], Train Loss: 0.0163, Test Loss: 0.0242\n",
            "Epoch [4036/6000], Train Loss: 0.0163, Test Loss: 0.0241\n",
            "Epoch [4037/6000], Train Loss: 0.0163, Test Loss: 0.0241\n",
            "Epoch [4038/6000], Train Loss: 0.0163, Test Loss: 0.0241\n",
            "Epoch [4039/6000], Train Loss: 0.0163, Test Loss: 0.0241\n",
            "Epoch [4040/6000], Train Loss: 0.0163, Test Loss: 0.0241\n",
            "Epoch [4041/6000], Train Loss: 0.0163, Test Loss: 0.0241\n",
            "Epoch [4042/6000], Train Loss: 0.0163, Test Loss: 0.0241\n",
            "Epoch [4043/6000], Train Loss: 0.0162, Test Loss: 0.0240\n",
            "Epoch [4044/6000], Train Loss: 0.0162, Test Loss: 0.0240\n",
            "Epoch [4045/6000], Train Loss: 0.0162, Test Loss: 0.0240\n",
            "Epoch [4046/6000], Train Loss: 0.0162, Test Loss: 0.0240\n",
            "Epoch [4047/6000], Train Loss: 0.0162, Test Loss: 0.0240\n",
            "Epoch [4048/6000], Train Loss: 0.0162, Test Loss: 0.0240\n",
            "Epoch [4049/6000], Train Loss: 0.0162, Test Loss: 0.0239\n",
            "Epoch [4050/6000], Train Loss: 0.0161, Test Loss: 0.0239\n",
            "Epoch [4051/6000], Train Loss: 0.0161, Test Loss: 0.0239\n",
            "Epoch [4052/6000], Train Loss: 0.0161, Test Loss: 0.0239\n",
            "Epoch [4053/6000], Train Loss: 0.0161, Test Loss: 0.0239\n",
            "Epoch [4054/6000], Train Loss: 0.0161, Test Loss: 0.0238\n",
            "Epoch [4055/6000], Train Loss: 0.0161, Test Loss: 0.0238\n",
            "Epoch [4056/6000], Train Loss: 0.0161, Test Loss: 0.0239\n",
            "Epoch [4057/6000], Train Loss: 0.0160, Test Loss: 0.0239\n",
            "Epoch [4058/6000], Train Loss: 0.0160, Test Loss: 0.0238\n",
            "Epoch [4059/6000], Train Loss: 0.0160, Test Loss: 0.0238\n",
            "Epoch [4060/6000], Train Loss: 0.0160, Test Loss: 0.0238\n",
            "Epoch [4061/6000], Train Loss: 0.0160, Test Loss: 0.0237\n",
            "Epoch [4062/6000], Train Loss: 0.0160, Test Loss: 0.0237\n",
            "Epoch [4063/6000], Train Loss: 0.0160, Test Loss: 0.0237\n",
            "Epoch [4064/6000], Train Loss: 0.0159, Test Loss: 0.0237\n",
            "Epoch [4065/6000], Train Loss: 0.0159, Test Loss: 0.0237\n",
            "Epoch [4066/6000], Train Loss: 0.0159, Test Loss: 0.0237\n",
            "Epoch [4067/6000], Train Loss: 0.0159, Test Loss: 0.0237\n",
            "Epoch [4068/6000], Train Loss: 0.0159, Test Loss: 0.0236\n",
            "Epoch [4069/6000], Train Loss: 0.0159, Test Loss: 0.0236\n",
            "Epoch [4070/6000], Train Loss: 0.0159, Test Loss: 0.0236\n",
            "Epoch [4071/6000], Train Loss: 0.0159, Test Loss: 0.0236\n",
            "Epoch [4072/6000], Train Loss: 0.0158, Test Loss: 0.0236\n",
            "Epoch [4073/6000], Train Loss: 0.0158, Test Loss: 0.0235\n",
            "Epoch [4074/6000], Train Loss: 0.0158, Test Loss: 0.0236\n",
            "Epoch [4075/6000], Train Loss: 0.0158, Test Loss: 0.0235\n",
            "Epoch [4076/6000], Train Loss: 0.0158, Test Loss: 0.0235\n",
            "Epoch [4077/6000], Train Loss: 0.0158, Test Loss: 0.0235\n",
            "Epoch [4078/6000], Train Loss: 0.0158, Test Loss: 0.0235\n",
            "Epoch [4079/6000], Train Loss: 0.0158, Test Loss: 0.0235\n",
            "Epoch [4080/6000], Train Loss: 0.0157, Test Loss: 0.0234\n",
            "Epoch [4081/6000], Train Loss: 0.0157, Test Loss: 0.0234\n",
            "Epoch [4082/6000], Train Loss: 0.0157, Test Loss: 0.0234\n",
            "Epoch [4083/6000], Train Loss: 0.0157, Test Loss: 0.0233\n",
            "Epoch [4084/6000], Train Loss: 0.0157, Test Loss: 0.0234\n",
            "Epoch [4085/6000], Train Loss: 0.0157, Test Loss: 0.0234\n",
            "Epoch [4086/6000], Train Loss: 0.0157, Test Loss: 0.0233\n",
            "Epoch [4087/6000], Train Loss: 0.0156, Test Loss: 0.0233\n",
            "Epoch [4088/6000], Train Loss: 0.0156, Test Loss: 0.0233\n",
            "Epoch [4089/6000], Train Loss: 0.0156, Test Loss: 0.0233\n",
            "Epoch [4090/6000], Train Loss: 0.0156, Test Loss: 0.0232\n",
            "Epoch [4091/6000], Train Loss: 0.0156, Test Loss: 0.0232\n",
            "Epoch [4092/6000], Train Loss: 0.0156, Test Loss: 0.0232\n",
            "Epoch [4093/6000], Train Loss: 0.0156, Test Loss: 0.0231\n",
            "Epoch [4094/6000], Train Loss: 0.0156, Test Loss: 0.0232\n",
            "Epoch [4095/6000], Train Loss: 0.0155, Test Loss: 0.0232\n",
            "Epoch [4096/6000], Train Loss: 0.0155, Test Loss: 0.0232\n",
            "Epoch [4097/6000], Train Loss: 0.0155, Test Loss: 0.0232\n",
            "Epoch [4098/6000], Train Loss: 0.0155, Test Loss: 0.0231\n",
            "Epoch [4099/6000], Train Loss: 0.0155, Test Loss: 0.0231\n",
            "Epoch [4100/6000], Train Loss: 0.0155, Test Loss: 0.0231\n",
            "Epoch [4101/6000], Train Loss: 0.0155, Test Loss: 0.0230\n",
            "Epoch [4102/6000], Train Loss: 0.0155, Test Loss: 0.0231\n",
            "Epoch [4103/6000], Train Loss: 0.0154, Test Loss: 0.0230\n",
            "Epoch [4104/6000], Train Loss: 0.0154, Test Loss: 0.0230\n",
            "Epoch [4105/6000], Train Loss: 0.0154, Test Loss: 0.0230\n",
            "Epoch [4106/6000], Train Loss: 0.0154, Test Loss: 0.0230\n",
            "Epoch [4107/6000], Train Loss: 0.0154, Test Loss: 0.0230\n",
            "Epoch [4108/6000], Train Loss: 0.0154, Test Loss: 0.0229\n",
            "Epoch [4109/6000], Train Loss: 0.0154, Test Loss: 0.0229\n",
            "Epoch [4110/6000], Train Loss: 0.0153, Test Loss: 0.0229\n",
            "Epoch [4111/6000], Train Loss: 0.0153, Test Loss: 0.0229\n",
            "Epoch [4112/6000], Train Loss: 0.0153, Test Loss: 0.0229\n",
            "Epoch [4113/6000], Train Loss: 0.0153, Test Loss: 0.0229\n",
            "Epoch [4114/6000], Train Loss: 0.0153, Test Loss: 0.0228\n",
            "Epoch [4115/6000], Train Loss: 0.0153, Test Loss: 0.0228\n",
            "Epoch [4116/6000], Train Loss: 0.0153, Test Loss: 0.0228\n",
            "Epoch [4117/6000], Train Loss: 0.0153, Test Loss: 0.0228\n",
            "Epoch [4118/6000], Train Loss: 0.0152, Test Loss: 0.0228\n",
            "Epoch [4119/6000], Train Loss: 0.0152, Test Loss: 0.0228\n",
            "Epoch [4120/6000], Train Loss: 0.0152, Test Loss: 0.0227\n",
            "Epoch [4121/6000], Train Loss: 0.0152, Test Loss: 0.0227\n",
            "Epoch [4122/6000], Train Loss: 0.0152, Test Loss: 0.0227\n",
            "Epoch [4123/6000], Train Loss: 0.0152, Test Loss: 0.0227\n",
            "Epoch [4124/6000], Train Loss: 0.0152, Test Loss: 0.0227\n",
            "Epoch [4125/6000], Train Loss: 0.0151, Test Loss: 0.0227\n",
            "Epoch [4126/6000], Train Loss: 0.0151, Test Loss: 0.0227\n",
            "Epoch [4127/6000], Train Loss: 0.0151, Test Loss: 0.0227\n",
            "Epoch [4128/6000], Train Loss: 0.0151, Test Loss: 0.0226\n",
            "Epoch [4129/6000], Train Loss: 0.0151, Test Loss: 0.0226\n",
            "Epoch [4130/6000], Train Loss: 0.0151, Test Loss: 0.0225\n",
            "Epoch [4131/6000], Train Loss: 0.0151, Test Loss: 0.0225\n",
            "Epoch [4132/6000], Train Loss: 0.0151, Test Loss: 0.0225\n",
            "Epoch [4133/6000], Train Loss: 0.0150, Test Loss: 0.0226\n",
            "Epoch [4134/6000], Train Loss: 0.0150, Test Loss: 0.0226\n",
            "Epoch [4135/6000], Train Loss: 0.0150, Test Loss: 0.0226\n",
            "Epoch [4136/6000], Train Loss: 0.0150, Test Loss: 0.0226\n",
            "Epoch [4137/6000], Train Loss: 0.0150, Test Loss: 0.0225\n",
            "Epoch [4138/6000], Train Loss: 0.0150, Test Loss: 0.0225\n",
            "Epoch [4139/6000], Train Loss: 0.0150, Test Loss: 0.0224\n",
            "Epoch [4140/6000], Train Loss: 0.0150, Test Loss: 0.0224\n",
            "Epoch [4141/6000], Train Loss: 0.0149, Test Loss: 0.0224\n",
            "Epoch [4142/6000], Train Loss: 0.0149, Test Loss: 0.0224\n",
            "Epoch [4143/6000], Train Loss: 0.0149, Test Loss: 0.0224\n",
            "Epoch [4144/6000], Train Loss: 0.0149, Test Loss: 0.0223\n",
            "Epoch [4145/6000], Train Loss: 0.0149, Test Loss: 0.0224\n",
            "Epoch [4146/6000], Train Loss: 0.0149, Test Loss: 0.0223\n",
            "Epoch [4147/6000], Train Loss: 0.0149, Test Loss: 0.0223\n",
            "Epoch [4148/6000], Train Loss: 0.0149, Test Loss: 0.0223\n",
            "Epoch [4149/6000], Train Loss: 0.0148, Test Loss: 0.0223\n",
            "Epoch [4150/6000], Train Loss: 0.0148, Test Loss: 0.0222\n",
            "Epoch [4151/6000], Train Loss: 0.0148, Test Loss: 0.0222\n",
            "Epoch [4152/6000], Train Loss: 0.0148, Test Loss: 0.0222\n",
            "Epoch [4153/6000], Train Loss: 0.0148, Test Loss: 0.0223\n",
            "Epoch [4154/6000], Train Loss: 0.0148, Test Loss: 0.0223\n",
            "Epoch [4155/6000], Train Loss: 0.0148, Test Loss: 0.0223\n",
            "Epoch [4156/6000], Train Loss: 0.0148, Test Loss: 0.0222\n",
            "Epoch [4157/6000], Train Loss: 0.0147, Test Loss: 0.0222\n",
            "Epoch [4158/6000], Train Loss: 0.0147, Test Loss: 0.0221\n",
            "Epoch [4159/6000], Train Loss: 0.0147, Test Loss: 0.0221\n",
            "Epoch [4160/6000], Train Loss: 0.0147, Test Loss: 0.0221\n",
            "Epoch [4161/6000], Train Loss: 0.0147, Test Loss: 0.0221\n",
            "Epoch [4162/6000], Train Loss: 0.0147, Test Loss: 0.0221\n",
            "Epoch [4163/6000], Train Loss: 0.0147, Test Loss: 0.0221\n",
            "Epoch [4164/6000], Train Loss: 0.0147, Test Loss: 0.0221\n",
            "Epoch [4165/6000], Train Loss: 0.0146, Test Loss: 0.0220\n",
            "Epoch [4166/6000], Train Loss: 0.0146, Test Loss: 0.0220\n",
            "Epoch [4167/6000], Train Loss: 0.0146, Test Loss: 0.0220\n",
            "Epoch [4168/6000], Train Loss: 0.0146, Test Loss: 0.0220\n",
            "Epoch [4169/6000], Train Loss: 0.0146, Test Loss: 0.0220\n",
            "Epoch [4170/6000], Train Loss: 0.0146, Test Loss: 0.0219\n",
            "Epoch [4171/6000], Train Loss: 0.0146, Test Loss: 0.0219\n",
            "Epoch [4172/6000], Train Loss: 0.0146, Test Loss: 0.0219\n",
            "Epoch [4173/6000], Train Loss: 0.0145, Test Loss: 0.0219\n",
            "Epoch [4174/6000], Train Loss: 0.0145, Test Loss: 0.0219\n",
            "Epoch [4175/6000], Train Loss: 0.0145, Test Loss: 0.0219\n",
            "Epoch [4176/6000], Train Loss: 0.0145, Test Loss: 0.0218\n",
            "Epoch [4177/6000], Train Loss: 0.0145, Test Loss: 0.0218\n",
            "Epoch [4178/6000], Train Loss: 0.0145, Test Loss: 0.0218\n",
            "Epoch [4179/6000], Train Loss: 0.0145, Test Loss: 0.0218\n",
            "Epoch [4180/6000], Train Loss: 0.0145, Test Loss: 0.0218\n",
            "Epoch [4181/6000], Train Loss: 0.0144, Test Loss: 0.0218\n",
            "Epoch [4182/6000], Train Loss: 0.0144, Test Loss: 0.0217\n",
            "Epoch [4183/6000], Train Loss: 0.0144, Test Loss: 0.0218\n",
            "Epoch [4184/6000], Train Loss: 0.0144, Test Loss: 0.0218\n",
            "Epoch [4185/6000], Train Loss: 0.0144, Test Loss: 0.0217\n",
            "Epoch [4186/6000], Train Loss: 0.0144, Test Loss: 0.0217\n",
            "Epoch [4187/6000], Train Loss: 0.0144, Test Loss: 0.0217\n",
            "Epoch [4188/6000], Train Loss: 0.0144, Test Loss: 0.0217\n",
            "Epoch [4189/6000], Train Loss: 0.0143, Test Loss: 0.0217\n",
            "Epoch [4190/6000], Train Loss: 0.0143, Test Loss: 0.0216\n",
            "Epoch [4191/6000], Train Loss: 0.0143, Test Loss: 0.0216\n",
            "Epoch [4192/6000], Train Loss: 0.0143, Test Loss: 0.0216\n",
            "Epoch [4193/6000], Train Loss: 0.0143, Test Loss: 0.0216\n",
            "Epoch [4194/6000], Train Loss: 0.0143, Test Loss: 0.0216\n",
            "Epoch [4195/6000], Train Loss: 0.0143, Test Loss: 0.0216\n",
            "Epoch [4196/6000], Train Loss: 0.0143, Test Loss: 0.0215\n",
            "Epoch [4197/6000], Train Loss: 0.0143, Test Loss: 0.0216\n",
            "Epoch [4198/6000], Train Loss: 0.0142, Test Loss: 0.0215\n",
            "Epoch [4199/6000], Train Loss: 0.0142, Test Loss: 0.0215\n",
            "Epoch [4200/6000], Train Loss: 0.0142, Test Loss: 0.0215\n",
            "Epoch [4201/6000], Train Loss: 0.0142, Test Loss: 0.0214\n",
            "Epoch [4202/6000], Train Loss: 0.0142, Test Loss: 0.0214\n",
            "Epoch [4203/6000], Train Loss: 0.0142, Test Loss: 0.0214\n",
            "Epoch [4204/6000], Train Loss: 0.0142, Test Loss: 0.0214\n",
            "Epoch [4205/6000], Train Loss: 0.0142, Test Loss: 0.0214\n",
            "Epoch [4206/6000], Train Loss: 0.0141, Test Loss: 0.0214\n",
            "Epoch [4207/6000], Train Loss: 0.0141, Test Loss: 0.0214\n",
            "Epoch [4208/6000], Train Loss: 0.0141, Test Loss: 0.0213\n",
            "Epoch [4209/6000], Train Loss: 0.0141, Test Loss: 0.0213\n",
            "Epoch [4210/6000], Train Loss: 0.0141, Test Loss: 0.0213\n",
            "Epoch [4211/6000], Train Loss: 0.0141, Test Loss: 0.0213\n",
            "Epoch [4212/6000], Train Loss: 0.0141, Test Loss: 0.0213\n",
            "Epoch [4213/6000], Train Loss: 0.0141, Test Loss: 0.0213\n",
            "Epoch [4214/6000], Train Loss: 0.0141, Test Loss: 0.0213\n",
            "Epoch [4215/6000], Train Loss: 0.0140, Test Loss: 0.0212\n",
            "Epoch [4216/6000], Train Loss: 0.0140, Test Loss: 0.0212\n",
            "Epoch [4217/6000], Train Loss: 0.0140, Test Loss: 0.0212\n",
            "Epoch [4218/6000], Train Loss: 0.0140, Test Loss: 0.0212\n",
            "Epoch [4219/6000], Train Loss: 0.0140, Test Loss: 0.0212\n",
            "Epoch [4220/6000], Train Loss: 0.0140, Test Loss: 0.0212\n",
            "Epoch [4221/6000], Train Loss: 0.0140, Test Loss: 0.0212\n",
            "Epoch [4222/6000], Train Loss: 0.0140, Test Loss: 0.0211\n",
            "Epoch [4223/6000], Train Loss: 0.0139, Test Loss: 0.0211\n",
            "Epoch [4224/6000], Train Loss: 0.0139, Test Loss: 0.0211\n",
            "Epoch [4225/6000], Train Loss: 0.0139, Test Loss: 0.0211\n",
            "Epoch [4226/6000], Train Loss: 0.0139, Test Loss: 0.0211\n",
            "Epoch [4227/6000], Train Loss: 0.0139, Test Loss: 0.0211\n",
            "Epoch [4228/6000], Train Loss: 0.0139, Test Loss: 0.0211\n",
            "Epoch [4229/6000], Train Loss: 0.0139, Test Loss: 0.0210\n",
            "Epoch [4230/6000], Train Loss: 0.0139, Test Loss: 0.0210\n",
            "Epoch [4231/6000], Train Loss: 0.0139, Test Loss: 0.0210\n",
            "Epoch [4232/6000], Train Loss: 0.0138, Test Loss: 0.0210\n",
            "Epoch [4233/6000], Train Loss: 0.0138, Test Loss: 0.0210\n",
            "Epoch [4234/6000], Train Loss: 0.0138, Test Loss: 0.0210\n",
            "Epoch [4235/6000], Train Loss: 0.0138, Test Loss: 0.0210\n",
            "Epoch [4236/6000], Train Loss: 0.0138, Test Loss: 0.0210\n",
            "Epoch [4237/6000], Train Loss: 0.0138, Test Loss: 0.0209\n",
            "Epoch [4238/6000], Train Loss: 0.0138, Test Loss: 0.0209\n",
            "Epoch [4239/6000], Train Loss: 0.0138, Test Loss: 0.0209\n",
            "Epoch [4240/6000], Train Loss: 0.0137, Test Loss: 0.0209\n",
            "Epoch [4241/6000], Train Loss: 0.0137, Test Loss: 0.0208\n",
            "Epoch [4242/6000], Train Loss: 0.0137, Test Loss: 0.0209\n",
            "Epoch [4243/6000], Train Loss: 0.0137, Test Loss: 0.0209\n",
            "Epoch [4244/6000], Train Loss: 0.0137, Test Loss: 0.0208\n",
            "Epoch [4245/6000], Train Loss: 0.0137, Test Loss: 0.0208\n",
            "Epoch [4246/6000], Train Loss: 0.0137, Test Loss: 0.0208\n",
            "Epoch [4247/6000], Train Loss: 0.0137, Test Loss: 0.0207\n",
            "Epoch [4248/6000], Train Loss: 0.0137, Test Loss: 0.0208\n",
            "Epoch [4249/6000], Train Loss: 0.0136, Test Loss: 0.0207\n",
            "Epoch [4250/6000], Train Loss: 0.0136, Test Loss: 0.0207\n",
            "Epoch [4251/6000], Train Loss: 0.0136, Test Loss: 0.0207\n",
            "Epoch [4252/6000], Train Loss: 0.0136, Test Loss: 0.0207\n",
            "Epoch [4253/6000], Train Loss: 0.0136, Test Loss: 0.0207\n",
            "Epoch [4254/6000], Train Loss: 0.0136, Test Loss: 0.0206\n",
            "Epoch [4255/6000], Train Loss: 0.0136, Test Loss: 0.0206\n",
            "Epoch [4256/6000], Train Loss: 0.0136, Test Loss: 0.0206\n",
            "Epoch [4257/6000], Train Loss: 0.0135, Test Loss: 0.0206\n",
            "Epoch [4258/6000], Train Loss: 0.0135, Test Loss: 0.0206\n",
            "Epoch [4259/6000], Train Loss: 0.0135, Test Loss: 0.0206\n",
            "Epoch [4260/6000], Train Loss: 0.0135, Test Loss: 0.0205\n",
            "Epoch [4261/6000], Train Loss: 0.0135, Test Loss: 0.0205\n",
            "Epoch [4262/6000], Train Loss: 0.0135, Test Loss: 0.0205\n",
            "Epoch [4263/6000], Train Loss: 0.0135, Test Loss: 0.0205\n",
            "Epoch [4264/6000], Train Loss: 0.0135, Test Loss: 0.0205\n",
            "Epoch [4265/6000], Train Loss: 0.0135, Test Loss: 0.0205\n",
            "Epoch [4266/6000], Train Loss: 0.0134, Test Loss: 0.0205\n",
            "Epoch [4267/6000], Train Loss: 0.0134, Test Loss: 0.0205\n",
            "Epoch [4268/6000], Train Loss: 0.0134, Test Loss: 0.0204\n",
            "Epoch [4269/6000], Train Loss: 0.0134, Test Loss: 0.0204\n",
            "Epoch [4270/6000], Train Loss: 0.0134, Test Loss: 0.0204\n",
            "Epoch [4271/6000], Train Loss: 0.0134, Test Loss: 0.0204\n",
            "Epoch [4272/6000], Train Loss: 0.0134, Test Loss: 0.0204\n",
            "Epoch [4273/6000], Train Loss: 0.0134, Test Loss: 0.0204\n",
            "Epoch [4274/6000], Train Loss: 0.0134, Test Loss: 0.0203\n",
            "Epoch [4275/6000], Train Loss: 0.0133, Test Loss: 0.0203\n",
            "Epoch [4276/6000], Train Loss: 0.0133, Test Loss: 0.0203\n",
            "Epoch [4277/6000], Train Loss: 0.0133, Test Loss: 0.0203\n",
            "Epoch [4278/6000], Train Loss: 0.0133, Test Loss: 0.0203\n",
            "Epoch [4279/6000], Train Loss: 0.0133, Test Loss: 0.0203\n",
            "Epoch [4280/6000], Train Loss: 0.0133, Test Loss: 0.0203\n",
            "Epoch [4281/6000], Train Loss: 0.0133, Test Loss: 0.0203\n",
            "Epoch [4282/6000], Train Loss: 0.0133, Test Loss: 0.0202\n",
            "Epoch [4283/6000], Train Loss: 0.0133, Test Loss: 0.0202\n",
            "Epoch [4284/6000], Train Loss: 0.0132, Test Loss: 0.0202\n",
            "Epoch [4285/6000], Train Loss: 0.0132, Test Loss: 0.0202\n",
            "Epoch [4286/6000], Train Loss: 0.0132, Test Loss: 0.0202\n",
            "Epoch [4287/6000], Train Loss: 0.0132, Test Loss: 0.0202\n",
            "Epoch [4288/6000], Train Loss: 0.0132, Test Loss: 0.0201\n",
            "Epoch [4289/6000], Train Loss: 0.0132, Test Loss: 0.0202\n",
            "Epoch [4290/6000], Train Loss: 0.0132, Test Loss: 0.0201\n",
            "Epoch [4291/6000], Train Loss: 0.0132, Test Loss: 0.0201\n",
            "Epoch [4292/6000], Train Loss: 0.0132, Test Loss: 0.0201\n",
            "Epoch [4293/6000], Train Loss: 0.0131, Test Loss: 0.0201\n",
            "Epoch [4294/6000], Train Loss: 0.0131, Test Loss: 0.0200\n",
            "Epoch [4295/6000], Train Loss: 0.0131, Test Loss: 0.0201\n",
            "Epoch [4296/6000], Train Loss: 0.0131, Test Loss: 0.0201\n",
            "Epoch [4297/6000], Train Loss: 0.0131, Test Loss: 0.0200\n",
            "Epoch [4298/6000], Train Loss: 0.0131, Test Loss: 0.0200\n",
            "Epoch [4299/6000], Train Loss: 0.0131, Test Loss: 0.0200\n",
            "Epoch [4300/6000], Train Loss: 0.0131, Test Loss: 0.0200\n",
            "Epoch [4301/6000], Train Loss: 0.0131, Test Loss: 0.0200\n",
            "Epoch [4302/6000], Train Loss: 0.0130, Test Loss: 0.0200\n",
            "Epoch [4303/6000], Train Loss: 0.0130, Test Loss: 0.0200\n",
            "Epoch [4304/6000], Train Loss: 0.0130, Test Loss: 0.0199\n",
            "Epoch [4305/6000], Train Loss: 0.0130, Test Loss: 0.0199\n",
            "Epoch [4306/6000], Train Loss: 0.0130, Test Loss: 0.0199\n",
            "Epoch [4307/6000], Train Loss: 0.0130, Test Loss: 0.0199\n",
            "Epoch [4308/6000], Train Loss: 0.0130, Test Loss: 0.0198\n",
            "Epoch [4309/6000], Train Loss: 0.0130, Test Loss: 0.0198\n",
            "Epoch [4310/6000], Train Loss: 0.0130, Test Loss: 0.0198\n",
            "Epoch [4311/6000], Train Loss: 0.0129, Test Loss: 0.0198\n",
            "Epoch [4312/6000], Train Loss: 0.0129, Test Loss: 0.0198\n",
            "Epoch [4313/6000], Train Loss: 0.0129, Test Loss: 0.0198\n",
            "Epoch [4314/6000], Train Loss: 0.0129, Test Loss: 0.0198\n",
            "Epoch [4315/6000], Train Loss: 0.0129, Test Loss: 0.0198\n",
            "Epoch [4316/6000], Train Loss: 0.0129, Test Loss: 0.0197\n",
            "Epoch [4317/6000], Train Loss: 0.0129, Test Loss: 0.0197\n",
            "Epoch [4318/6000], Train Loss: 0.0129, Test Loss: 0.0197\n",
            "Epoch [4319/6000], Train Loss: 0.0129, Test Loss: 0.0197\n",
            "Epoch [4320/6000], Train Loss: 0.0128, Test Loss: 0.0197\n",
            "Epoch [4321/6000], Train Loss: 0.0128, Test Loss: 0.0197\n",
            "Epoch [4322/6000], Train Loss: 0.0128, Test Loss: 0.0196\n",
            "Epoch [4323/6000], Train Loss: 0.0128, Test Loss: 0.0196\n",
            "Epoch [4324/6000], Train Loss: 0.0128, Test Loss: 0.0196\n",
            "Epoch [4325/6000], Train Loss: 0.0128, Test Loss: 0.0196\n",
            "Epoch [4326/6000], Train Loss: 0.0128, Test Loss: 0.0196\n",
            "Epoch [4327/6000], Train Loss: 0.0128, Test Loss: 0.0196\n",
            "Epoch [4328/6000], Train Loss: 0.0128, Test Loss: 0.0196\n",
            "Epoch [4329/6000], Train Loss: 0.0128, Test Loss: 0.0196\n",
            "Epoch [4330/6000], Train Loss: 0.0127, Test Loss: 0.0196\n",
            "Epoch [4331/6000], Train Loss: 0.0127, Test Loss: 0.0195\n",
            "Epoch [4332/6000], Train Loss: 0.0127, Test Loss: 0.0195\n",
            "Epoch [4333/6000], Train Loss: 0.0127, Test Loss: 0.0195\n",
            "Epoch [4334/6000], Train Loss: 0.0127, Test Loss: 0.0195\n",
            "Epoch [4335/6000], Train Loss: 0.0127, Test Loss: 0.0195\n",
            "Epoch [4336/6000], Train Loss: 0.0127, Test Loss: 0.0195\n",
            "Epoch [4337/6000], Train Loss: 0.0127, Test Loss: 0.0195\n",
            "Epoch [4338/6000], Train Loss: 0.0127, Test Loss: 0.0195\n",
            "Epoch [4339/6000], Train Loss: 0.0126, Test Loss: 0.0194\n",
            "Epoch [4340/6000], Train Loss: 0.0126, Test Loss: 0.0194\n",
            "Epoch [4341/6000], Train Loss: 0.0126, Test Loss: 0.0194\n",
            "Epoch [4342/6000], Train Loss: 0.0126, Test Loss: 0.0194\n",
            "Epoch [4343/6000], Train Loss: 0.0126, Test Loss: 0.0194\n",
            "Epoch [4344/6000], Train Loss: 0.0126, Test Loss: 0.0193\n",
            "Epoch [4345/6000], Train Loss: 0.0126, Test Loss: 0.0193\n",
            "Epoch [4346/6000], Train Loss: 0.0126, Test Loss: 0.0193\n",
            "Epoch [4347/6000], Train Loss: 0.0126, Test Loss: 0.0193\n",
            "Epoch [4348/6000], Train Loss: 0.0126, Test Loss: 0.0193\n",
            "Epoch [4349/6000], Train Loss: 0.0125, Test Loss: 0.0193\n",
            "Epoch [4350/6000], Train Loss: 0.0125, Test Loss: 0.0192\n",
            "Epoch [4351/6000], Train Loss: 0.0125, Test Loss: 0.0192\n",
            "Epoch [4352/6000], Train Loss: 0.0125, Test Loss: 0.0192\n",
            "Epoch [4353/6000], Train Loss: 0.0125, Test Loss: 0.0193\n",
            "Epoch [4354/6000], Train Loss: 0.0125, Test Loss: 0.0193\n",
            "Epoch [4355/6000], Train Loss: 0.0125, Test Loss: 0.0192\n",
            "Epoch [4356/6000], Train Loss: 0.0125, Test Loss: 0.0192\n",
            "Epoch [4357/6000], Train Loss: 0.0125, Test Loss: 0.0192\n",
            "Epoch [4358/6000], Train Loss: 0.0124, Test Loss: 0.0191\n",
            "Epoch [4359/6000], Train Loss: 0.0124, Test Loss: 0.0191\n",
            "Epoch [4360/6000], Train Loss: 0.0124, Test Loss: 0.0191\n",
            "Epoch [4361/6000], Train Loss: 0.0124, Test Loss: 0.0191\n",
            "Epoch [4362/6000], Train Loss: 0.0124, Test Loss: 0.0191\n",
            "Epoch [4363/6000], Train Loss: 0.0124, Test Loss: 0.0191\n",
            "Epoch [4364/6000], Train Loss: 0.0124, Test Loss: 0.0191\n",
            "Epoch [4365/6000], Train Loss: 0.0124, Test Loss: 0.0191\n",
            "Epoch [4366/6000], Train Loss: 0.0124, Test Loss: 0.0190\n",
            "Epoch [4367/6000], Train Loss: 0.0124, Test Loss: 0.0190\n",
            "Epoch [4368/6000], Train Loss: 0.0123, Test Loss: 0.0190\n",
            "Epoch [4369/6000], Train Loss: 0.0123, Test Loss: 0.0190\n",
            "Epoch [4370/6000], Train Loss: 0.0123, Test Loss: 0.0190\n",
            "Epoch [4371/6000], Train Loss: 0.0123, Test Loss: 0.0190\n",
            "Epoch [4372/6000], Train Loss: 0.0123, Test Loss: 0.0190\n",
            "Epoch [4373/6000], Train Loss: 0.0123, Test Loss: 0.0189\n",
            "Epoch [4374/6000], Train Loss: 0.0123, Test Loss: 0.0190\n",
            "Epoch [4375/6000], Train Loss: 0.0123, Test Loss: 0.0190\n",
            "Epoch [4376/6000], Train Loss: 0.0123, Test Loss: 0.0189\n",
            "Epoch [4377/6000], Train Loss: 0.0122, Test Loss: 0.0189\n",
            "Epoch [4378/6000], Train Loss: 0.0122, Test Loss: 0.0189\n",
            "Epoch [4379/6000], Train Loss: 0.0122, Test Loss: 0.0188\n",
            "Epoch [4380/6000], Train Loss: 0.0122, Test Loss: 0.0189\n",
            "Epoch [4381/6000], Train Loss: 0.0122, Test Loss: 0.0189\n",
            "Epoch [4382/6000], Train Loss: 0.0122, Test Loss: 0.0188\n",
            "Epoch [4383/6000], Train Loss: 0.0122, Test Loss: 0.0188\n",
            "Epoch [4384/6000], Train Loss: 0.0122, Test Loss: 0.0188\n",
            "Epoch [4385/6000], Train Loss: 0.0122, Test Loss: 0.0188\n",
            "Epoch [4386/6000], Train Loss: 0.0122, Test Loss: 0.0188\n",
            "Epoch [4387/6000], Train Loss: 0.0121, Test Loss: 0.0187\n",
            "Epoch [4388/6000], Train Loss: 0.0121, Test Loss: 0.0188\n",
            "Epoch [4389/6000], Train Loss: 0.0121, Test Loss: 0.0187\n",
            "Epoch [4390/6000], Train Loss: 0.0121, Test Loss: 0.0187\n",
            "Epoch [4391/6000], Train Loss: 0.0121, Test Loss: 0.0187\n",
            "Epoch [4392/6000], Train Loss: 0.0121, Test Loss: 0.0187\n",
            "Epoch [4393/6000], Train Loss: 0.0121, Test Loss: 0.0187\n",
            "Epoch [4394/6000], Train Loss: 0.0121, Test Loss: 0.0187\n",
            "Epoch [4395/6000], Train Loss: 0.0121, Test Loss: 0.0186\n",
            "Epoch [4396/6000], Train Loss: 0.0121, Test Loss: 0.0186\n",
            "Epoch [4397/6000], Train Loss: 0.0120, Test Loss: 0.0186\n",
            "Epoch [4398/6000], Train Loss: 0.0120, Test Loss: 0.0186\n",
            "Epoch [4399/6000], Train Loss: 0.0120, Test Loss: 0.0186\n",
            "Epoch [4400/6000], Train Loss: 0.0120, Test Loss: 0.0186\n",
            "Epoch [4401/6000], Train Loss: 0.0120, Test Loss: 0.0186\n",
            "Epoch [4402/6000], Train Loss: 0.0120, Test Loss: 0.0185\n",
            "Epoch [4403/6000], Train Loss: 0.0120, Test Loss: 0.0185\n",
            "Epoch [4404/6000], Train Loss: 0.0120, Test Loss: 0.0185\n",
            "Epoch [4405/6000], Train Loss: 0.0120, Test Loss: 0.0185\n",
            "Epoch [4406/6000], Train Loss: 0.0120, Test Loss: 0.0185\n",
            "Epoch [4407/6000], Train Loss: 0.0119, Test Loss: 0.0185\n",
            "Epoch [4408/6000], Train Loss: 0.0119, Test Loss: 0.0185\n",
            "Epoch [4409/6000], Train Loss: 0.0119, Test Loss: 0.0185\n",
            "Epoch [4410/6000], Train Loss: 0.0119, Test Loss: 0.0184\n",
            "Epoch [4411/6000], Train Loss: 0.0119, Test Loss: 0.0184\n",
            "Epoch [4412/6000], Train Loss: 0.0119, Test Loss: 0.0184\n",
            "Epoch [4413/6000], Train Loss: 0.0119, Test Loss: 0.0184\n",
            "Epoch [4414/6000], Train Loss: 0.0119, Test Loss: 0.0184\n",
            "Epoch [4415/6000], Train Loss: 0.0119, Test Loss: 0.0184\n",
            "Epoch [4416/6000], Train Loss: 0.0119, Test Loss: 0.0184\n",
            "Epoch [4417/6000], Train Loss: 0.0118, Test Loss: 0.0184\n",
            "Epoch [4418/6000], Train Loss: 0.0118, Test Loss: 0.0183\n",
            "Epoch [4419/6000], Train Loss: 0.0118, Test Loss: 0.0183\n",
            "Epoch [4420/6000], Train Loss: 0.0118, Test Loss: 0.0183\n",
            "Epoch [4421/6000], Train Loss: 0.0118, Test Loss: 0.0183\n",
            "Epoch [4422/6000], Train Loss: 0.0118, Test Loss: 0.0183\n",
            "Epoch [4423/6000], Train Loss: 0.0118, Test Loss: 0.0182\n",
            "Epoch [4424/6000], Train Loss: 0.0118, Test Loss: 0.0182\n",
            "Epoch [4425/6000], Train Loss: 0.0118, Test Loss: 0.0183\n",
            "Epoch [4426/6000], Train Loss: 0.0118, Test Loss: 0.0183\n",
            "Epoch [4427/6000], Train Loss: 0.0117, Test Loss: 0.0182\n",
            "Epoch [4428/6000], Train Loss: 0.0117, Test Loss: 0.0182\n",
            "Epoch [4429/6000], Train Loss: 0.0117, Test Loss: 0.0182\n",
            "Epoch [4430/6000], Train Loss: 0.0117, Test Loss: 0.0182\n",
            "Epoch [4431/6000], Train Loss: 0.0117, Test Loss: 0.0181\n",
            "Epoch [4432/6000], Train Loss: 0.0117, Test Loss: 0.0181\n",
            "Epoch [4433/6000], Train Loss: 0.0117, Test Loss: 0.0181\n",
            "Epoch [4434/6000], Train Loss: 0.0117, Test Loss: 0.0181\n",
            "Epoch [4435/6000], Train Loss: 0.0117, Test Loss: 0.0181\n",
            "Epoch [4436/6000], Train Loss: 0.0117, Test Loss: 0.0181\n",
            "Epoch [4437/6000], Train Loss: 0.0116, Test Loss: 0.0181\n",
            "Epoch [4438/6000], Train Loss: 0.0116, Test Loss: 0.0181\n",
            "Epoch [4439/6000], Train Loss: 0.0116, Test Loss: 0.0180\n",
            "Epoch [4440/6000], Train Loss: 0.0116, Test Loss: 0.0180\n",
            "Epoch [4441/6000], Train Loss: 0.0116, Test Loss: 0.0180\n",
            "Epoch [4442/6000], Train Loss: 0.0116, Test Loss: 0.0180\n",
            "Epoch [4443/6000], Train Loss: 0.0116, Test Loss: 0.0180\n",
            "Epoch [4444/6000], Train Loss: 0.0116, Test Loss: 0.0180\n",
            "Epoch [4445/6000], Train Loss: 0.0116, Test Loss: 0.0180\n",
            "Epoch [4446/6000], Train Loss: 0.0116, Test Loss: 0.0180\n",
            "Epoch [4447/6000], Train Loss: 0.0116, Test Loss: 0.0179\n",
            "Epoch [4448/6000], Train Loss: 0.0115, Test Loss: 0.0179\n",
            "Epoch [4449/6000], Train Loss: 0.0115, Test Loss: 0.0180\n",
            "Epoch [4450/6000], Train Loss: 0.0115, Test Loss: 0.0179\n",
            "Epoch [4451/6000], Train Loss: 0.0115, Test Loss: 0.0179\n",
            "Epoch [4452/6000], Train Loss: 0.0115, Test Loss: 0.0179\n",
            "Epoch [4453/6000], Train Loss: 0.0115, Test Loss: 0.0179\n",
            "Epoch [4454/6000], Train Loss: 0.0115, Test Loss: 0.0179\n",
            "Epoch [4455/6000], Train Loss: 0.0115, Test Loss: 0.0178\n",
            "Epoch [4456/6000], Train Loss: 0.0115, Test Loss: 0.0178\n",
            "Epoch [4457/6000], Train Loss: 0.0115, Test Loss: 0.0178\n",
            "Epoch [4458/6000], Train Loss: 0.0114, Test Loss: 0.0178\n",
            "Epoch [4459/6000], Train Loss: 0.0114, Test Loss: 0.0178\n",
            "Epoch [4460/6000], Train Loss: 0.0114, Test Loss: 0.0178\n",
            "Epoch [4461/6000], Train Loss: 0.0114, Test Loss: 0.0178\n",
            "Epoch [4462/6000], Train Loss: 0.0114, Test Loss: 0.0178\n",
            "Epoch [4463/6000], Train Loss: 0.0114, Test Loss: 0.0178\n",
            "Epoch [4464/6000], Train Loss: 0.0114, Test Loss: 0.0178\n",
            "Epoch [4465/6000], Train Loss: 0.0114, Test Loss: 0.0177\n",
            "Epoch [4466/6000], Train Loss: 0.0114, Test Loss: 0.0177\n",
            "Epoch [4467/6000], Train Loss: 0.0114, Test Loss: 0.0177\n",
            "Epoch [4468/6000], Train Loss: 0.0113, Test Loss: 0.0177\n",
            "Epoch [4469/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4470/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4471/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4472/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4473/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4474/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4475/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4476/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4477/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4478/6000], Train Loss: 0.0113, Test Loss: 0.0176\n",
            "Epoch [4479/6000], Train Loss: 0.0112, Test Loss: 0.0176\n",
            "Epoch [4480/6000], Train Loss: 0.0112, Test Loss: 0.0176\n",
            "Epoch [4481/6000], Train Loss: 0.0112, Test Loss: 0.0175\n",
            "Epoch [4482/6000], Train Loss: 0.0112, Test Loss: 0.0175\n",
            "Epoch [4483/6000], Train Loss: 0.0112, Test Loss: 0.0175\n",
            "Epoch [4484/6000], Train Loss: 0.0112, Test Loss: 0.0175\n",
            "Epoch [4485/6000], Train Loss: 0.0112, Test Loss: 0.0174\n",
            "Epoch [4486/6000], Train Loss: 0.0112, Test Loss: 0.0175\n",
            "Epoch [4487/6000], Train Loss: 0.0112, Test Loss: 0.0175\n",
            "Epoch [4488/6000], Train Loss: 0.0112, Test Loss: 0.0175\n",
            "Epoch [4489/6000], Train Loss: 0.0112, Test Loss: 0.0174\n",
            "Epoch [4490/6000], Train Loss: 0.0111, Test Loss: 0.0174\n",
            "Epoch [4491/6000], Train Loss: 0.0111, Test Loss: 0.0174\n",
            "Epoch [4492/6000], Train Loss: 0.0111, Test Loss: 0.0173\n",
            "Epoch [4493/6000], Train Loss: 0.0111, Test Loss: 0.0173\n",
            "Epoch [4494/6000], Train Loss: 0.0111, Test Loss: 0.0173\n",
            "Epoch [4495/6000], Train Loss: 0.0111, Test Loss: 0.0173\n",
            "Epoch [4496/6000], Train Loss: 0.0111, Test Loss: 0.0174\n",
            "Epoch [4497/6000], Train Loss: 0.0111, Test Loss: 0.0173\n",
            "Epoch [4498/6000], Train Loss: 0.0111, Test Loss: 0.0173\n",
            "Epoch [4499/6000], Train Loss: 0.0111, Test Loss: 0.0173\n",
            "Epoch [4500/6000], Train Loss: 0.0111, Test Loss: 0.0173\n",
            "Epoch [4501/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4502/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4503/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4504/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4505/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4506/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4507/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4508/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4509/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4510/6000], Train Loss: 0.0110, Test Loss: 0.0172\n",
            "Epoch [4511/6000], Train Loss: 0.0110, Test Loss: 0.0171\n",
            "Epoch [4512/6000], Train Loss: 0.0109, Test Loss: 0.0171\n",
            "Epoch [4513/6000], Train Loss: 0.0109, Test Loss: 0.0171\n",
            "Epoch [4514/6000], Train Loss: 0.0109, Test Loss: 0.0171\n",
            "Epoch [4515/6000], Train Loss: 0.0109, Test Loss: 0.0171\n",
            "Epoch [4516/6000], Train Loss: 0.0109, Test Loss: 0.0171\n",
            "Epoch [4517/6000], Train Loss: 0.0109, Test Loss: 0.0171\n",
            "Epoch [4518/6000], Train Loss: 0.0109, Test Loss: 0.0170\n",
            "Epoch [4519/6000], Train Loss: 0.0109, Test Loss: 0.0170\n",
            "Epoch [4520/6000], Train Loss: 0.0109, Test Loss: 0.0170\n",
            "Epoch [4521/6000], Train Loss: 0.0109, Test Loss: 0.0170\n",
            "Epoch [4522/6000], Train Loss: 0.0108, Test Loss: 0.0170\n",
            "Epoch [4523/6000], Train Loss: 0.0108, Test Loss: 0.0170\n",
            "Epoch [4524/6000], Train Loss: 0.0108, Test Loss: 0.0170\n",
            "Epoch [4525/6000], Train Loss: 0.0108, Test Loss: 0.0170\n",
            "Epoch [4526/6000], Train Loss: 0.0108, Test Loss: 0.0170\n",
            "Epoch [4527/6000], Train Loss: 0.0108, Test Loss: 0.0169\n",
            "Epoch [4528/6000], Train Loss: 0.0108, Test Loss: 0.0169\n",
            "Epoch [4529/6000], Train Loss: 0.0108, Test Loss: 0.0169\n",
            "Epoch [4530/6000], Train Loss: 0.0108, Test Loss: 0.0169\n",
            "Epoch [4531/6000], Train Loss: 0.0108, Test Loss: 0.0169\n",
            "Epoch [4532/6000], Train Loss: 0.0108, Test Loss: 0.0169\n",
            "Epoch [4533/6000], Train Loss: 0.0108, Test Loss: 0.0168\n",
            "Epoch [4534/6000], Train Loss: 0.0107, Test Loss: 0.0168\n",
            "Epoch [4535/6000], Train Loss: 0.0107, Test Loss: 0.0169\n",
            "Epoch [4536/6000], Train Loss: 0.0107, Test Loss: 0.0169\n",
            "Epoch [4537/6000], Train Loss: 0.0107, Test Loss: 0.0168\n",
            "Epoch [4538/6000], Train Loss: 0.0107, Test Loss: 0.0168\n",
            "Epoch [4539/6000], Train Loss: 0.0107, Test Loss: 0.0168\n",
            "Epoch [4540/6000], Train Loss: 0.0107, Test Loss: 0.0168\n",
            "Epoch [4541/6000], Train Loss: 0.0107, Test Loss: 0.0168\n",
            "Epoch [4542/6000], Train Loss: 0.0107, Test Loss: 0.0168\n",
            "Epoch [4543/6000], Train Loss: 0.0107, Test Loss: 0.0168\n",
            "Epoch [4544/6000], Train Loss: 0.0107, Test Loss: 0.0167\n",
            "Epoch [4545/6000], Train Loss: 0.0106, Test Loss: 0.0167\n",
            "Epoch [4546/6000], Train Loss: 0.0106, Test Loss: 0.0167\n",
            "Epoch [4547/6000], Train Loss: 0.0106, Test Loss: 0.0167\n",
            "Epoch [4548/6000], Train Loss: 0.0106, Test Loss: 0.0167\n",
            "Epoch [4549/6000], Train Loss: 0.0106, Test Loss: 0.0167\n",
            "Epoch [4550/6000], Train Loss: 0.0106, Test Loss: 0.0167\n",
            "Epoch [4551/6000], Train Loss: 0.0106, Test Loss: 0.0167\n",
            "Epoch [4552/6000], Train Loss: 0.0106, Test Loss: 0.0166\n",
            "Epoch [4553/6000], Train Loss: 0.0106, Test Loss: 0.0166\n",
            "Epoch [4554/6000], Train Loss: 0.0106, Test Loss: 0.0166\n",
            "Epoch [4555/6000], Train Loss: 0.0106, Test Loss: 0.0166\n",
            "Epoch [4556/6000], Train Loss: 0.0105, Test Loss: 0.0166\n",
            "Epoch [4557/6000], Train Loss: 0.0105, Test Loss: 0.0166\n",
            "Epoch [4558/6000], Train Loss: 0.0105, Test Loss: 0.0165\n",
            "Epoch [4559/6000], Train Loss: 0.0105, Test Loss: 0.0165\n",
            "Epoch [4560/6000], Train Loss: 0.0105, Test Loss: 0.0165\n",
            "Epoch [4561/6000], Train Loss: 0.0105, Test Loss: 0.0165\n",
            "Epoch [4562/6000], Train Loss: 0.0105, Test Loss: 0.0165\n",
            "Epoch [4563/6000], Train Loss: 0.0105, Test Loss: 0.0165\n",
            "Epoch [4564/6000], Train Loss: 0.0105, Test Loss: 0.0165\n",
            "Epoch [4565/6000], Train Loss: 0.0105, Test Loss: 0.0165\n",
            "Epoch [4566/6000], Train Loss: 0.0105, Test Loss: 0.0164\n",
            "Epoch [4567/6000], Train Loss: 0.0105, Test Loss: 0.0164\n",
            "Epoch [4568/6000], Train Loss: 0.0104, Test Loss: 0.0164\n",
            "Epoch [4569/6000], Train Loss: 0.0104, Test Loss: 0.0164\n",
            "Epoch [4570/6000], Train Loss: 0.0104, Test Loss: 0.0164\n",
            "Epoch [4571/6000], Train Loss: 0.0104, Test Loss: 0.0164\n",
            "Epoch [4572/6000], Train Loss: 0.0104, Test Loss: 0.0164\n",
            "Epoch [4573/6000], Train Loss: 0.0104, Test Loss: 0.0164\n",
            "Epoch [4574/6000], Train Loss: 0.0104, Test Loss: 0.0164\n",
            "Epoch [4575/6000], Train Loss: 0.0104, Test Loss: 0.0164\n",
            "Epoch [4576/6000], Train Loss: 0.0104, Test Loss: 0.0163\n",
            "Epoch [4577/6000], Train Loss: 0.0104, Test Loss: 0.0163\n",
            "Epoch [4578/6000], Train Loss: 0.0104, Test Loss: 0.0163\n",
            "Epoch [4579/6000], Train Loss: 0.0104, Test Loss: 0.0163\n",
            "Epoch [4580/6000], Train Loss: 0.0103, Test Loss: 0.0163\n",
            "Epoch [4581/6000], Train Loss: 0.0103, Test Loss: 0.0163\n",
            "Epoch [4582/6000], Train Loss: 0.0103, Test Loss: 0.0163\n",
            "Epoch [4583/6000], Train Loss: 0.0103, Test Loss: 0.0163\n",
            "Epoch [4584/6000], Train Loss: 0.0103, Test Loss: 0.0163\n",
            "Epoch [4585/6000], Train Loss: 0.0103, Test Loss: 0.0162\n",
            "Epoch [4586/6000], Train Loss: 0.0103, Test Loss: 0.0162\n",
            "Epoch [4587/6000], Train Loss: 0.0103, Test Loss: 0.0162\n",
            "Epoch [4588/6000], Train Loss: 0.0103, Test Loss: 0.0162\n",
            "Epoch [4589/6000], Train Loss: 0.0103, Test Loss: 0.0162\n",
            "Epoch [4590/6000], Train Loss: 0.0103, Test Loss: 0.0162\n",
            "Epoch [4591/6000], Train Loss: 0.0102, Test Loss: 0.0161\n",
            "Epoch [4592/6000], Train Loss: 0.0102, Test Loss: 0.0161\n",
            "Epoch [4593/6000], Train Loss: 0.0102, Test Loss: 0.0161\n",
            "Epoch [4594/6000], Train Loss: 0.0102, Test Loss: 0.0161\n",
            "Epoch [4595/6000], Train Loss: 0.0102, Test Loss: 0.0161\n",
            "Epoch [4596/6000], Train Loss: 0.0102, Test Loss: 0.0161\n",
            "Epoch [4597/6000], Train Loss: 0.0102, Test Loss: 0.0161\n",
            "Epoch [4598/6000], Train Loss: 0.0102, Test Loss: 0.0161\n",
            "Epoch [4599/6000], Train Loss: 0.0102, Test Loss: 0.0160\n",
            "Epoch [4600/6000], Train Loss: 0.0102, Test Loss: 0.0160\n",
            "Epoch [4601/6000], Train Loss: 0.0102, Test Loss: 0.0160\n",
            "Epoch [4602/6000], Train Loss: 0.0102, Test Loss: 0.0160\n",
            "Epoch [4603/6000], Train Loss: 0.0101, Test Loss: 0.0160\n",
            "Epoch [4604/6000], Train Loss: 0.0101, Test Loss: 0.0160\n",
            "Epoch [4605/6000], Train Loss: 0.0101, Test Loss: 0.0160\n",
            "Epoch [4606/6000], Train Loss: 0.0101, Test Loss: 0.0160\n",
            "Epoch [4607/6000], Train Loss: 0.0101, Test Loss: 0.0160\n",
            "Epoch [4608/6000], Train Loss: 0.0101, Test Loss: 0.0160\n",
            "Epoch [4609/6000], Train Loss: 0.0101, Test Loss: 0.0159\n",
            "Epoch [4610/6000], Train Loss: 0.0101, Test Loss: 0.0160\n",
            "Epoch [4611/6000], Train Loss: 0.0101, Test Loss: 0.0159\n",
            "Epoch [4612/6000], Train Loss: 0.0101, Test Loss: 0.0159\n",
            "Epoch [4613/6000], Train Loss: 0.0101, Test Loss: 0.0159\n",
            "Epoch [4614/6000], Train Loss: 0.0100, Test Loss: 0.0159\n",
            "Epoch [4615/6000], Train Loss: 0.0100, Test Loss: 0.0159\n",
            "Epoch [4616/6000], Train Loss: 0.0100, Test Loss: 0.0159\n",
            "Epoch [4617/6000], Train Loss: 0.0100, Test Loss: 0.0159\n",
            "Epoch [4618/6000], Train Loss: 0.0100, Test Loss: 0.0159\n",
            "Epoch [4619/6000], Train Loss: 0.0100, Test Loss: 0.0158\n",
            "Epoch [4620/6000], Train Loss: 0.0100, Test Loss: 0.0158\n",
            "Epoch [4621/6000], Train Loss: 0.0100, Test Loss: 0.0158\n",
            "Epoch [4622/6000], Train Loss: 0.0100, Test Loss: 0.0158\n",
            "Epoch [4623/6000], Train Loss: 0.0100, Test Loss: 0.0158\n",
            "Epoch [4624/6000], Train Loss: 0.0100, Test Loss: 0.0157\n",
            "Epoch [4625/6000], Train Loss: 0.0100, Test Loss: 0.0157\n",
            "Epoch [4626/6000], Train Loss: 0.0099, Test Loss: 0.0157\n",
            "Epoch [4627/6000], Train Loss: 0.0099, Test Loss: 0.0158\n",
            "Epoch [4628/6000], Train Loss: 0.0099, Test Loss: 0.0158\n",
            "Epoch [4629/6000], Train Loss: 0.0099, Test Loss: 0.0157\n",
            "Epoch [4630/6000], Train Loss: 0.0099, Test Loss: 0.0157\n",
            "Epoch [4631/6000], Train Loss: 0.0099, Test Loss: 0.0157\n",
            "Epoch [4632/6000], Train Loss: 0.0099, Test Loss: 0.0156\n",
            "Epoch [4633/6000], Train Loss: 0.0099, Test Loss: 0.0156\n",
            "Epoch [4634/6000], Train Loss: 0.0099, Test Loss: 0.0156\n",
            "Epoch [4635/6000], Train Loss: 0.0099, Test Loss: 0.0156\n",
            "Epoch [4636/6000], Train Loss: 0.0099, Test Loss: 0.0157\n",
            "Epoch [4637/6000], Train Loss: 0.0099, Test Loss: 0.0157\n",
            "Epoch [4638/6000], Train Loss: 0.0099, Test Loss: 0.0156\n",
            "Epoch [4639/6000], Train Loss: 0.0098, Test Loss: 0.0156\n",
            "Epoch [4640/6000], Train Loss: 0.0098, Test Loss: 0.0156\n",
            "Epoch [4641/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4642/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4643/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4644/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4645/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4646/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4647/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4648/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4649/6000], Train Loss: 0.0098, Test Loss: 0.0155\n",
            "Epoch [4650/6000], Train Loss: 0.0098, Test Loss: 0.0154\n",
            "Epoch [4651/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4652/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4653/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4654/6000], Train Loss: 0.0097, Test Loss: 0.0155\n",
            "Epoch [4655/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4656/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4657/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4658/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4659/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4660/6000], Train Loss: 0.0097, Test Loss: 0.0154\n",
            "Epoch [4661/6000], Train Loss: 0.0097, Test Loss: 0.0153\n",
            "Epoch [4662/6000], Train Loss: 0.0097, Test Loss: 0.0153\n",
            "Epoch [4663/6000], Train Loss: 0.0096, Test Loss: 0.0153\n",
            "Epoch [4664/6000], Train Loss: 0.0096, Test Loss: 0.0153\n",
            "Epoch [4665/6000], Train Loss: 0.0096, Test Loss: 0.0153\n",
            "Epoch [4666/6000], Train Loss: 0.0096, Test Loss: 0.0153\n",
            "Epoch [4667/6000], Train Loss: 0.0096, Test Loss: 0.0153\n",
            "Epoch [4668/6000], Train Loss: 0.0096, Test Loss: 0.0153\n",
            "Epoch [4669/6000], Train Loss: 0.0096, Test Loss: 0.0153\n",
            "Epoch [4670/6000], Train Loss: 0.0096, Test Loss: 0.0152\n",
            "Epoch [4671/6000], Train Loss: 0.0096, Test Loss: 0.0152\n",
            "Epoch [4672/6000], Train Loss: 0.0096, Test Loss: 0.0152\n",
            "Epoch [4673/6000], Train Loss: 0.0096, Test Loss: 0.0152\n",
            "Epoch [4674/6000], Train Loss: 0.0096, Test Loss: 0.0152\n",
            "Epoch [4675/6000], Train Loss: 0.0096, Test Loss: 0.0152\n",
            "Epoch [4676/6000], Train Loss: 0.0095, Test Loss: 0.0152\n",
            "Epoch [4677/6000], Train Loss: 0.0095, Test Loss: 0.0152\n",
            "Epoch [4678/6000], Train Loss: 0.0095, Test Loss: 0.0152\n",
            "Epoch [4679/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4680/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4681/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4682/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4683/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4684/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4685/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4686/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4687/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4688/6000], Train Loss: 0.0095, Test Loss: 0.0151\n",
            "Epoch [4689/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4690/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4691/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4692/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4693/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4694/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4695/6000], Train Loss: 0.0094, Test Loss: 0.0149\n",
            "Epoch [4696/6000], Train Loss: 0.0094, Test Loss: 0.0149\n",
            "Epoch [4697/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4698/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4699/6000], Train Loss: 0.0094, Test Loss: 0.0150\n",
            "Epoch [4700/6000], Train Loss: 0.0094, Test Loss: 0.0149\n",
            "Epoch [4701/6000], Train Loss: 0.0094, Test Loss: 0.0149\n",
            "Epoch [4702/6000], Train Loss: 0.0093, Test Loss: 0.0149\n",
            "Epoch [4703/6000], Train Loss: 0.0093, Test Loss: 0.0149\n",
            "Epoch [4704/6000], Train Loss: 0.0093, Test Loss: 0.0149\n",
            "Epoch [4705/6000], Train Loss: 0.0093, Test Loss: 0.0149\n",
            "Epoch [4706/6000], Train Loss: 0.0093, Test Loss: 0.0149\n",
            "Epoch [4707/6000], Train Loss: 0.0093, Test Loss: 0.0148\n",
            "Epoch [4708/6000], Train Loss: 0.0093, Test Loss: 0.0148\n",
            "Epoch [4709/6000], Train Loss: 0.0093, Test Loss: 0.0148\n",
            "Epoch [4710/6000], Train Loss: 0.0093, Test Loss: 0.0148\n",
            "Epoch [4711/6000], Train Loss: 0.0093, Test Loss: 0.0148\n",
            "Epoch [4712/6000], Train Loss: 0.0093, Test Loss: 0.0148\n",
            "Epoch [4713/6000], Train Loss: 0.0093, Test Loss: 0.0148\n",
            "Epoch [4714/6000], Train Loss: 0.0092, Test Loss: 0.0148\n",
            "Epoch [4715/6000], Train Loss: 0.0092, Test Loss: 0.0148\n",
            "Epoch [4716/6000], Train Loss: 0.0092, Test Loss: 0.0147\n",
            "Epoch [4717/6000], Train Loss: 0.0092, Test Loss: 0.0147\n",
            "Epoch [4718/6000], Train Loss: 0.0092, Test Loss: 0.0147\n",
            "Epoch [4719/6000], Train Loss: 0.0092, Test Loss: 0.0147\n",
            "Epoch [4720/6000], Train Loss: 0.0092, Test Loss: 0.0147\n",
            "Epoch [4721/6000], Train Loss: 0.0092, Test Loss: 0.0147\n",
            "Epoch [4722/6000], Train Loss: 0.0092, Test Loss: 0.0147\n",
            "Epoch [4723/6000], Train Loss: 0.0092, Test Loss: 0.0147\n",
            "Epoch [4724/6000], Train Loss: 0.0092, Test Loss: 0.0146\n",
            "Epoch [4725/6000], Train Loss: 0.0092, Test Loss: 0.0146\n",
            "Epoch [4726/6000], Train Loss: 0.0092, Test Loss: 0.0146\n",
            "Epoch [4727/6000], Train Loss: 0.0092, Test Loss: 0.0146\n",
            "Epoch [4728/6000], Train Loss: 0.0091, Test Loss: 0.0146\n",
            "Epoch [4729/6000], Train Loss: 0.0091, Test Loss: 0.0146\n",
            "Epoch [4730/6000], Train Loss: 0.0091, Test Loss: 0.0146\n",
            "Epoch [4731/6000], Train Loss: 0.0091, Test Loss: 0.0146\n",
            "Epoch [4732/6000], Train Loss: 0.0091, Test Loss: 0.0146\n",
            "Epoch [4733/6000], Train Loss: 0.0091, Test Loss: 0.0146\n",
            "Epoch [4734/6000], Train Loss: 0.0091, Test Loss: 0.0145\n",
            "Epoch [4735/6000], Train Loss: 0.0091, Test Loss: 0.0145\n",
            "Epoch [4736/6000], Train Loss: 0.0091, Test Loss: 0.0145\n",
            "Epoch [4737/6000], Train Loss: 0.0091, Test Loss: 0.0145\n",
            "Epoch [4738/6000], Train Loss: 0.0091, Test Loss: 0.0145\n",
            "Epoch [4739/6000], Train Loss: 0.0091, Test Loss: 0.0145\n",
            "Epoch [4740/6000], Train Loss: 0.0091, Test Loss: 0.0145\n",
            "Epoch [4741/6000], Train Loss: 0.0090, Test Loss: 0.0145\n",
            "Epoch [4742/6000], Train Loss: 0.0090, Test Loss: 0.0145\n",
            "Epoch [4743/6000], Train Loss: 0.0090, Test Loss: 0.0145\n",
            "Epoch [4744/6000], Train Loss: 0.0090, Test Loss: 0.0144\n",
            "Epoch [4745/6000], Train Loss: 0.0090, Test Loss: 0.0144\n",
            "Epoch [4746/6000], Train Loss: 0.0090, Test Loss: 0.0144\n",
            "Epoch [4747/6000], Train Loss: 0.0090, Test Loss: 0.0144\n",
            "Epoch [4748/6000], Train Loss: 0.0090, Test Loss: 0.0144\n",
            "Epoch [4749/6000], Train Loss: 0.0090, Test Loss: 0.0144\n",
            "Epoch [4750/6000], Train Loss: 0.0090, Test Loss: 0.0144\n",
            "Epoch [4751/6000], Train Loss: 0.0090, Test Loss: 0.0143\n",
            "Epoch [4752/6000], Train Loss: 0.0090, Test Loss: 0.0144\n",
            "Epoch [4753/6000], Train Loss: 0.0090, Test Loss: 0.0143\n",
            "Epoch [4754/6000], Train Loss: 0.0090, Test Loss: 0.0143\n",
            "Epoch [4755/6000], Train Loss: 0.0089, Test Loss: 0.0143\n",
            "Epoch [4756/6000], Train Loss: 0.0089, Test Loss: 0.0143\n",
            "Epoch [4757/6000], Train Loss: 0.0089, Test Loss: 0.0143\n",
            "Epoch [4758/6000], Train Loss: 0.0089, Test Loss: 0.0143\n",
            "Epoch [4759/6000], Train Loss: 0.0089, Test Loss: 0.0143\n",
            "Epoch [4760/6000], Train Loss: 0.0089, Test Loss: 0.0143\n",
            "Epoch [4761/6000], Train Loss: 0.0089, Test Loss: 0.0143\n",
            "Epoch [4762/6000], Train Loss: 0.0089, Test Loss: 0.0142\n",
            "Epoch [4763/6000], Train Loss: 0.0089, Test Loss: 0.0142\n",
            "Epoch [4764/6000], Train Loss: 0.0089, Test Loss: 0.0142\n",
            "Epoch [4765/6000], Train Loss: 0.0089, Test Loss: 0.0142\n",
            "Epoch [4766/6000], Train Loss: 0.0089, Test Loss: 0.0142\n",
            "Epoch [4767/6000], Train Loss: 0.0089, Test Loss: 0.0142\n",
            "Epoch [4768/6000], Train Loss: 0.0088, Test Loss: 0.0142\n",
            "Epoch [4769/6000], Train Loss: 0.0088, Test Loss: 0.0142\n",
            "Epoch [4770/6000], Train Loss: 0.0088, Test Loss: 0.0142\n",
            "Epoch [4771/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4772/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4773/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4774/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4775/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4776/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4777/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4778/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4779/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4780/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4781/6000], Train Loss: 0.0088, Test Loss: 0.0141\n",
            "Epoch [4782/6000], Train Loss: 0.0087, Test Loss: 0.0141\n",
            "Epoch [4783/6000], Train Loss: 0.0087, Test Loss: 0.0140\n",
            "Epoch [4784/6000], Train Loss: 0.0087, Test Loss: 0.0140\n",
            "Epoch [4785/6000], Train Loss: 0.0087, Test Loss: 0.0140\n",
            "Epoch [4786/6000], Train Loss: 0.0087, Test Loss: 0.0140\n",
            "Epoch [4787/6000], Train Loss: 0.0087, Test Loss: 0.0140\n",
            "Epoch [4788/6000], Train Loss: 0.0087, Test Loss: 0.0140\n",
            "Epoch [4789/6000], Train Loss: 0.0087, Test Loss: 0.0140\n",
            "Epoch [4790/6000], Train Loss: 0.0087, Test Loss: 0.0140\n",
            "Epoch [4791/6000], Train Loss: 0.0087, Test Loss: 0.0139\n",
            "Epoch [4792/6000], Train Loss: 0.0087, Test Loss: 0.0139\n",
            "Epoch [4793/6000], Train Loss: 0.0087, Test Loss: 0.0139\n",
            "Epoch [4794/6000], Train Loss: 0.0087, Test Loss: 0.0139\n",
            "Epoch [4795/6000], Train Loss: 0.0087, Test Loss: 0.0139\n",
            "Epoch [4796/6000], Train Loss: 0.0086, Test Loss: 0.0139\n",
            "Epoch [4797/6000], Train Loss: 0.0086, Test Loss: 0.0139\n",
            "Epoch [4798/6000], Train Loss: 0.0086, Test Loss: 0.0139\n",
            "Epoch [4799/6000], Train Loss: 0.0086, Test Loss: 0.0139\n",
            "Epoch [4800/6000], Train Loss: 0.0086, Test Loss: 0.0139\n",
            "Epoch [4801/6000], Train Loss: 0.0086, Test Loss: 0.0139\n",
            "Epoch [4802/6000], Train Loss: 0.0086, Test Loss: 0.0138\n",
            "Epoch [4803/6000], Train Loss: 0.0086, Test Loss: 0.0138\n",
            "Epoch [4804/6000], Train Loss: 0.0086, Test Loss: 0.0138\n",
            "Epoch [4805/6000], Train Loss: 0.0086, Test Loss: 0.0138\n",
            "Epoch [4806/6000], Train Loss: 0.0086, Test Loss: 0.0138\n",
            "Epoch [4807/6000], Train Loss: 0.0086, Test Loss: 0.0138\n",
            "Epoch [4808/6000], Train Loss: 0.0086, Test Loss: 0.0138\n",
            "Epoch [4809/6000], Train Loss: 0.0086, Test Loss: 0.0137\n",
            "Epoch [4810/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4811/6000], Train Loss: 0.0085, Test Loss: 0.0138\n",
            "Epoch [4812/6000], Train Loss: 0.0085, Test Loss: 0.0138\n",
            "Epoch [4813/6000], Train Loss: 0.0085, Test Loss: 0.0138\n",
            "Epoch [4814/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4815/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4816/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4817/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4818/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4819/6000], Train Loss: 0.0085, Test Loss: 0.0136\n",
            "Epoch [4820/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4821/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4822/6000], Train Loss: 0.0085, Test Loss: 0.0137\n",
            "Epoch [4823/6000], Train Loss: 0.0085, Test Loss: 0.0136\n",
            "Epoch [4824/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4825/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4826/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4827/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4828/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4829/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4830/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4831/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4832/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4833/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4834/6000], Train Loss: 0.0084, Test Loss: 0.0136\n",
            "Epoch [4835/6000], Train Loss: 0.0084, Test Loss: 0.0135\n",
            "Epoch [4836/6000], Train Loss: 0.0084, Test Loss: 0.0135\n",
            "Epoch [4837/6000], Train Loss: 0.0084, Test Loss: 0.0135\n",
            "Epoch [4838/6000], Train Loss: 0.0084, Test Loss: 0.0135\n",
            "Epoch [4839/6000], Train Loss: 0.0083, Test Loss: 0.0135\n",
            "Epoch [4840/6000], Train Loss: 0.0083, Test Loss: 0.0135\n",
            "Epoch [4841/6000], Train Loss: 0.0083, Test Loss: 0.0135\n",
            "Epoch [4842/6000], Train Loss: 0.0083, Test Loss: 0.0135\n",
            "Epoch [4843/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4844/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4845/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4846/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4847/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4848/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4849/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4850/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4851/6000], Train Loss: 0.0083, Test Loss: 0.0134\n",
            "Epoch [4852/6000], Train Loss: 0.0083, Test Loss: 0.0133\n",
            "Epoch [4853/6000], Train Loss: 0.0082, Test Loss: 0.0133\n",
            "Epoch [4854/6000], Train Loss: 0.0082, Test Loss: 0.0133\n",
            "Epoch [4855/6000], Train Loss: 0.0082, Test Loss: 0.0133\n",
            "Epoch [4856/6000], Train Loss: 0.0082, Test Loss: 0.0133\n",
            "Epoch [4857/6000], Train Loss: 0.0082, Test Loss: 0.0133\n",
            "Epoch [4858/6000], Train Loss: 0.0082, Test Loss: 0.0133\n",
            "Epoch [4859/6000], Train Loss: 0.0082, Test Loss: 0.0133\n",
            "Epoch [4860/6000], Train Loss: 0.0082, Test Loss: 0.0132\n",
            "Epoch [4861/6000], Train Loss: 0.0082, Test Loss: 0.0132\n",
            "Epoch [4862/6000], Train Loss: 0.0082, Test Loss: 0.0132\n",
            "Epoch [4863/6000], Train Loss: 0.0082, Test Loss: 0.0132\n",
            "Epoch [4864/6000], Train Loss: 0.0082, Test Loss: 0.0132\n",
            "Epoch [4865/6000], Train Loss: 0.0082, Test Loss: 0.0132\n",
            "Epoch [4866/6000], Train Loss: 0.0082, Test Loss: 0.0132\n",
            "Epoch [4867/6000], Train Loss: 0.0082, Test Loss: 0.0132\n",
            "Epoch [4868/6000], Train Loss: 0.0081, Test Loss: 0.0132\n",
            "Epoch [4869/6000], Train Loss: 0.0081, Test Loss: 0.0132\n",
            "Epoch [4870/6000], Train Loss: 0.0081, Test Loss: 0.0132\n",
            "Epoch [4871/6000], Train Loss: 0.0081, Test Loss: 0.0132\n",
            "Epoch [4872/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4873/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4874/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4875/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4876/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4877/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4878/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4879/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4880/6000], Train Loss: 0.0081, Test Loss: 0.0131\n",
            "Epoch [4881/6000], Train Loss: 0.0081, Test Loss: 0.0130\n",
            "Epoch [4882/6000], Train Loss: 0.0081, Test Loss: 0.0130\n",
            "Epoch [4883/6000], Train Loss: 0.0080, Test Loss: 0.0130\n",
            "Epoch [4884/6000], Train Loss: 0.0080, Test Loss: 0.0130\n",
            "Epoch [4885/6000], Train Loss: 0.0080, Test Loss: 0.0130\n",
            "Epoch [4886/6000], Train Loss: 0.0080, Test Loss: 0.0130\n",
            "Epoch [4887/6000], Train Loss: 0.0080, Test Loss: 0.0130\n",
            "Epoch [4888/6000], Train Loss: 0.0080, Test Loss: 0.0130\n",
            "Epoch [4889/6000], Train Loss: 0.0080, Test Loss: 0.0129\n",
            "Epoch [4890/6000], Train Loss: 0.0080, Test Loss: 0.0129\n",
            "Epoch [4891/6000], Train Loss: 0.0080, Test Loss: 0.0129\n",
            "Epoch [4892/6000], Train Loss: 0.0080, Test Loss: 0.0130\n",
            "Epoch [4893/6000], Train Loss: 0.0080, Test Loss: 0.0129\n",
            "Epoch [4894/6000], Train Loss: 0.0080, Test Loss: 0.0129\n",
            "Epoch [4895/6000], Train Loss: 0.0080, Test Loss: 0.0129\n",
            "Epoch [4896/6000], Train Loss: 0.0080, Test Loss: 0.0129\n",
            "Epoch [4897/6000], Train Loss: 0.0080, Test Loss: 0.0129\n",
            "Epoch [4898/6000], Train Loss: 0.0079, Test Loss: 0.0129\n",
            "Epoch [4899/6000], Train Loss: 0.0079, Test Loss: 0.0129\n",
            "Epoch [4900/6000], Train Loss: 0.0079, Test Loss: 0.0129\n",
            "Epoch [4901/6000], Train Loss: 0.0079, Test Loss: 0.0129\n",
            "Epoch [4902/6000], Train Loss: 0.0079, Test Loss: 0.0129\n",
            "Epoch [4903/6000], Train Loss: 0.0079, Test Loss: 0.0129\n",
            "Epoch [4904/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4905/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4906/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4907/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4908/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4909/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4910/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4911/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4912/6000], Train Loss: 0.0079, Test Loss: 0.0128\n",
            "Epoch [4913/6000], Train Loss: 0.0078, Test Loss: 0.0128\n",
            "Epoch [4914/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4915/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4916/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4917/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4918/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4919/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4920/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4921/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4922/6000], Train Loss: 0.0078, Test Loss: 0.0127\n",
            "Epoch [4923/6000], Train Loss: 0.0078, Test Loss: 0.0126\n",
            "Epoch [4924/6000], Train Loss: 0.0078, Test Loss: 0.0126\n",
            "Epoch [4925/6000], Train Loss: 0.0078, Test Loss: 0.0126\n",
            "Epoch [4926/6000], Train Loss: 0.0078, Test Loss: 0.0126\n",
            "Epoch [4927/6000], Train Loss: 0.0078, Test Loss: 0.0126\n",
            "Epoch [4928/6000], Train Loss: 0.0078, Test Loss: 0.0126\n",
            "Epoch [4929/6000], Train Loss: 0.0077, Test Loss: 0.0126\n",
            "Epoch [4930/6000], Train Loss: 0.0077, Test Loss: 0.0126\n",
            "Epoch [4931/6000], Train Loss: 0.0077, Test Loss: 0.0126\n",
            "Epoch [4932/6000], Train Loss: 0.0077, Test Loss: 0.0126\n",
            "Epoch [4933/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4934/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4935/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4936/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4937/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4938/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4939/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4940/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4941/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4942/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4943/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4944/6000], Train Loss: 0.0077, Test Loss: 0.0125\n",
            "Epoch [4945/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4946/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4947/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4948/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4949/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4950/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4951/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4952/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4953/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4954/6000], Train Loss: 0.0076, Test Loss: 0.0124\n",
            "Epoch [4955/6000], Train Loss: 0.0076, Test Loss: 0.0123\n",
            "Epoch [4956/6000], Train Loss: 0.0076, Test Loss: 0.0123\n",
            "Epoch [4957/6000], Train Loss: 0.0076, Test Loss: 0.0123\n",
            "Epoch [4958/6000], Train Loss: 0.0076, Test Loss: 0.0123\n",
            "Epoch [4959/6000], Train Loss: 0.0076, Test Loss: 0.0123\n",
            "Epoch [4960/6000], Train Loss: 0.0076, Test Loss: 0.0123\n",
            "Epoch [4961/6000], Train Loss: 0.0075, Test Loss: 0.0123\n",
            "Epoch [4962/6000], Train Loss: 0.0075, Test Loss: 0.0123\n",
            "Epoch [4963/6000], Train Loss: 0.0075, Test Loss: 0.0123\n",
            "Epoch [4964/6000], Train Loss: 0.0075, Test Loss: 0.0123\n",
            "Epoch [4965/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4966/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4967/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4968/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4969/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4970/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4971/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4972/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4973/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4974/6000], Train Loss: 0.0075, Test Loss: 0.0122\n",
            "Epoch [4975/6000], Train Loss: 0.0075, Test Loss: 0.0121\n",
            "Epoch [4976/6000], Train Loss: 0.0075, Test Loss: 0.0121\n",
            "Epoch [4977/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4978/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4979/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4980/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4981/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4982/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4983/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4984/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4985/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4986/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4987/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4988/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4989/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4990/6000], Train Loss: 0.0074, Test Loss: 0.0121\n",
            "Epoch [4991/6000], Train Loss: 0.0074, Test Loss: 0.0120\n",
            "Epoch [4992/6000], Train Loss: 0.0074, Test Loss: 0.0120\n",
            "Epoch [4993/6000], Train Loss: 0.0073, Test Loss: 0.0120\n",
            "Epoch [4994/6000], Train Loss: 0.0073, Test Loss: 0.0120\n",
            "Epoch [4995/6000], Train Loss: 0.0073, Test Loss: 0.0120\n",
            "Epoch [4996/6000], Train Loss: 0.0073, Test Loss: 0.0120\n",
            "Epoch [4997/6000], Train Loss: 0.0073, Test Loss: 0.0120\n",
            "Epoch [4998/6000], Train Loss: 0.0073, Test Loss: 0.0120\n",
            "Epoch [4999/6000], Train Loss: 0.0073, Test Loss: 0.0120\n",
            "Epoch [5000/6000], Train Loss: 0.0073, Test Loss: 0.0120\n",
            "Epoch [5001/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5002/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5003/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5004/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5005/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5006/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5007/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5008/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5009/6000], Train Loss: 0.0073, Test Loss: 0.0119\n",
            "Epoch [5010/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5011/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5012/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5013/6000], Train Loss: 0.0072, Test Loss: 0.0119\n",
            "Epoch [5014/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5015/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5016/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5017/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5018/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5019/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5020/6000], Train Loss: 0.0072, Test Loss: 0.0118\n",
            "Epoch [5021/6000], Train Loss: 0.0072, Test Loss: 0.0117\n",
            "Epoch [5022/6000], Train Loss: 0.0072, Test Loss: 0.0117\n",
            "Epoch [5023/6000], Train Loss: 0.0072, Test Loss: 0.0117\n",
            "Epoch [5024/6000], Train Loss: 0.0072, Test Loss: 0.0117\n",
            "Epoch [5025/6000], Train Loss: 0.0072, Test Loss: 0.0117\n",
            "Epoch [5026/6000], Train Loss: 0.0072, Test Loss: 0.0117\n",
            "Epoch [5027/6000], Train Loss: 0.0071, Test Loss: 0.0117\n",
            "Epoch [5028/6000], Train Loss: 0.0071, Test Loss: 0.0117\n",
            "Epoch [5029/6000], Train Loss: 0.0071, Test Loss: 0.0117\n",
            "Epoch [5030/6000], Train Loss: 0.0071, Test Loss: 0.0117\n",
            "Epoch [5031/6000], Train Loss: 0.0071, Test Loss: 0.0117\n",
            "Epoch [5032/6000], Train Loss: 0.0071, Test Loss: 0.0117\n",
            "Epoch [5033/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5034/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5035/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5036/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5037/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5038/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5039/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5040/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5041/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5042/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5043/6000], Train Loss: 0.0071, Test Loss: 0.0116\n",
            "Epoch [5044/6000], Train Loss: 0.0070, Test Loss: 0.0116\n",
            "Epoch [5045/6000], Train Loss: 0.0070, Test Loss: 0.0116\n",
            "Epoch [5046/6000], Train Loss: 0.0070, Test Loss: 0.0116\n",
            "Epoch [5047/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5048/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5049/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5050/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5051/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5052/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5053/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5054/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5055/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5056/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5057/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5058/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5059/6000], Train Loss: 0.0070, Test Loss: 0.0115\n",
            "Epoch [5060/6000], Train Loss: 0.0070, Test Loss: 0.0114\n",
            "Epoch [5061/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5062/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5063/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5064/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5065/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5066/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5067/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5068/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5069/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5070/6000], Train Loss: 0.0069, Test Loss: 0.0114\n",
            "Epoch [5071/6000], Train Loss: 0.0069, Test Loss: 0.0113\n",
            "Epoch [5072/6000], Train Loss: 0.0069, Test Loss: 0.0113\n",
            "Epoch [5073/6000], Train Loss: 0.0069, Test Loss: 0.0113\n",
            "Epoch [5074/6000], Train Loss: 0.0069, Test Loss: 0.0113\n",
            "Epoch [5075/6000], Train Loss: 0.0069, Test Loss: 0.0113\n",
            "Epoch [5076/6000], Train Loss: 0.0069, Test Loss: 0.0113\n",
            "Epoch [5077/6000], Train Loss: 0.0069, Test Loss: 0.0113\n",
            "Epoch [5078/6000], Train Loss: 0.0069, Test Loss: 0.0113\n",
            "Epoch [5079/6000], Train Loss: 0.0068, Test Loss: 0.0113\n",
            "Epoch [5080/6000], Train Loss: 0.0068, Test Loss: 0.0113\n",
            "Epoch [5081/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5082/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5083/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5084/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5085/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5086/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5087/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5088/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5089/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5090/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5091/6000], Train Loss: 0.0068, Test Loss: 0.0111\n",
            "Epoch [5092/6000], Train Loss: 0.0068, Test Loss: 0.0112\n",
            "Epoch [5093/6000], Train Loss: 0.0068, Test Loss: 0.0111\n",
            "Epoch [5094/6000], Train Loss: 0.0068, Test Loss: 0.0111\n",
            "Epoch [5095/6000], Train Loss: 0.0068, Test Loss: 0.0111\n",
            "Epoch [5096/6000], Train Loss: 0.0068, Test Loss: 0.0111\n",
            "Epoch [5097/6000], Train Loss: 0.0068, Test Loss: 0.0111\n",
            "Epoch [5098/6000], Train Loss: 0.0067, Test Loss: 0.0111\n",
            "Epoch [5099/6000], Train Loss: 0.0067, Test Loss: 0.0111\n",
            "Epoch [5100/6000], Train Loss: 0.0067, Test Loss: 0.0111\n",
            "Epoch [5101/6000], Train Loss: 0.0067, Test Loss: 0.0111\n",
            "Epoch [5102/6000], Train Loss: 0.0067, Test Loss: 0.0111\n",
            "Epoch [5103/6000], Train Loss: 0.0067, Test Loss: 0.0111\n",
            "Epoch [5104/6000], Train Loss: 0.0067, Test Loss: 0.0111\n",
            "Epoch [5105/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5106/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5107/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5108/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5109/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5110/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5111/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5112/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5113/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5114/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5115/6000], Train Loss: 0.0067, Test Loss: 0.0110\n",
            "Epoch [5116/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5117/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5118/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5119/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5120/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5121/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5122/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5123/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5124/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5125/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5126/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5127/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5128/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5129/6000], Train Loss: 0.0066, Test Loss: 0.0109\n",
            "Epoch [5130/6000], Train Loss: 0.0066, Test Loss: 0.0108\n",
            "Epoch [5131/6000], Train Loss: 0.0066, Test Loss: 0.0108\n",
            "Epoch [5132/6000], Train Loss: 0.0066, Test Loss: 0.0108\n",
            "Epoch [5133/6000], Train Loss: 0.0066, Test Loss: 0.0108\n",
            "Epoch [5134/6000], Train Loss: 0.0065, Test Loss: 0.0108\n",
            "Epoch [5135/6000], Train Loss: 0.0065, Test Loss: 0.0108\n",
            "Epoch [5136/6000], Train Loss: 0.0065, Test Loss: 0.0108\n",
            "Epoch [5137/6000], Train Loss: 0.0065, Test Loss: 0.0108\n",
            "Epoch [5138/6000], Train Loss: 0.0065, Test Loss: 0.0108\n",
            "Epoch [5139/6000], Train Loss: 0.0065, Test Loss: 0.0108\n",
            "Epoch [5140/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5141/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5142/6000], Train Loss: 0.0065, Test Loss: 0.0108\n",
            "Epoch [5143/6000], Train Loss: 0.0065, Test Loss: 0.0108\n",
            "Epoch [5144/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5145/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5146/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5147/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5148/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5149/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5150/6000], Train Loss: 0.0065, Test Loss: 0.0106\n",
            "Epoch [5151/6000], Train Loss: 0.0065, Test Loss: 0.0106\n",
            "Epoch [5152/6000], Train Loss: 0.0065, Test Loss: 0.0107\n",
            "Epoch [5153/6000], Train Loss: 0.0064, Test Loss: 0.0107\n",
            "Epoch [5154/6000], Train Loss: 0.0064, Test Loss: 0.0107\n",
            "Epoch [5155/6000], Train Loss: 0.0064, Test Loss: 0.0107\n",
            "Epoch [5156/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5157/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5158/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5159/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5160/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5161/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5162/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5163/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5164/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5165/6000], Train Loss: 0.0064, Test Loss: 0.0105\n",
            "Epoch [5166/6000], Train Loss: 0.0064, Test Loss: 0.0105\n",
            "Epoch [5167/6000], Train Loss: 0.0064, Test Loss: 0.0106\n",
            "Epoch [5168/6000], Train Loss: 0.0064, Test Loss: 0.0105\n",
            "Epoch [5169/6000], Train Loss: 0.0064, Test Loss: 0.0105\n",
            "Epoch [5170/6000], Train Loss: 0.0064, Test Loss: 0.0105\n",
            "Epoch [5171/6000], Train Loss: 0.0064, Test Loss: 0.0105\n",
            "Epoch [5172/6000], Train Loss: 0.0063, Test Loss: 0.0105\n",
            "Epoch [5173/6000], Train Loss: 0.0063, Test Loss: 0.0105\n",
            "Epoch [5174/6000], Train Loss: 0.0063, Test Loss: 0.0105\n",
            "Epoch [5175/6000], Train Loss: 0.0063, Test Loss: 0.0105\n",
            "Epoch [5176/6000], Train Loss: 0.0063, Test Loss: 0.0105\n",
            "Epoch [5177/6000], Train Loss: 0.0063, Test Loss: 0.0105\n",
            "Epoch [5178/6000], Train Loss: 0.0063, Test Loss: 0.0105\n",
            "Epoch [5179/6000], Train Loss: 0.0063, Test Loss: 0.0105\n",
            "Epoch [5180/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5181/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5182/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5183/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5184/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5185/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5186/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5187/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5188/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5189/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5190/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5191/6000], Train Loss: 0.0063, Test Loss: 0.0104\n",
            "Epoch [5192/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5193/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5194/6000], Train Loss: 0.0062, Test Loss: 0.0104\n",
            "Epoch [5195/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5196/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5197/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5198/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5199/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5200/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5201/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5202/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5203/6000], Train Loss: 0.0062, Test Loss: 0.0103\n",
            "Epoch [5204/6000], Train Loss: 0.0062, Test Loss: 0.0102\n",
            "Epoch [5205/6000], Train Loss: 0.0062, Test Loss: 0.0102\n",
            "Epoch [5206/6000], Train Loss: 0.0062, Test Loss: 0.0102\n",
            "Epoch [5207/6000], Train Loss: 0.0062, Test Loss: 0.0102\n",
            "Epoch [5208/6000], Train Loss: 0.0062, Test Loss: 0.0102\n",
            "Epoch [5209/6000], Train Loss: 0.0062, Test Loss: 0.0102\n",
            "Epoch [5210/6000], Train Loss: 0.0062, Test Loss: 0.0102\n",
            "Epoch [5211/6000], Train Loss: 0.0062, Test Loss: 0.0102\n",
            "Epoch [5212/6000], Train Loss: 0.0061, Test Loss: 0.0102\n",
            "Epoch [5213/6000], Train Loss: 0.0061, Test Loss: 0.0102\n",
            "Epoch [5214/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5215/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5216/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5217/6000], Train Loss: 0.0061, Test Loss: 0.0102\n",
            "Epoch [5218/6000], Train Loss: 0.0061, Test Loss: 0.0102\n",
            "Epoch [5219/6000], Train Loss: 0.0061, Test Loss: 0.0102\n",
            "Epoch [5220/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5221/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5222/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5223/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5224/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5225/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5226/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5227/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5228/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5229/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5230/6000], Train Loss: 0.0061, Test Loss: 0.0101\n",
            "Epoch [5231/6000], Train Loss: 0.0060, Test Loss: 0.0101\n",
            "Epoch [5232/6000], Train Loss: 0.0060, Test Loss: 0.0101\n",
            "Epoch [5233/6000], Train Loss: 0.0060, Test Loss: 0.0101\n",
            "Epoch [5234/6000], Train Loss: 0.0060, Test Loss: 0.0101\n",
            "Epoch [5235/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5236/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5237/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5238/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5239/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5240/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5241/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5242/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5243/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5244/6000], Train Loss: 0.0060, Test Loss: 0.0100\n",
            "Epoch [5245/6000], Train Loss: 0.0060, Test Loss: 0.0099\n",
            "Epoch [5246/6000], Train Loss: 0.0060, Test Loss: 0.0099\n",
            "Epoch [5247/6000], Train Loss: 0.0060, Test Loss: 0.0099\n",
            "Epoch [5248/6000], Train Loss: 0.0060, Test Loss: 0.0099\n",
            "Epoch [5249/6000], Train Loss: 0.0060, Test Loss: 0.0099\n",
            "Epoch [5250/6000], Train Loss: 0.0060, Test Loss: 0.0099\n",
            "Epoch [5251/6000], Train Loss: 0.0060, Test Loss: 0.0099\n",
            "Epoch [5252/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5253/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5254/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5255/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5256/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5257/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5258/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5259/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5260/6000], Train Loss: 0.0059, Test Loss: 0.0099\n",
            "Epoch [5261/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5262/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5263/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5264/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5265/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5266/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5267/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5268/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5269/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5270/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5271/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5272/6000], Train Loss: 0.0059, Test Loss: 0.0098\n",
            "Epoch [5273/6000], Train Loss: 0.0058, Test Loss: 0.0098\n",
            "Epoch [5274/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5275/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5276/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5277/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5278/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5279/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5280/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5281/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5282/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5283/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5284/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5285/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5286/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5287/6000], Train Loss: 0.0058, Test Loss: 0.0097\n",
            "Epoch [5288/6000], Train Loss: 0.0058, Test Loss: 0.0096\n",
            "Epoch [5289/6000], Train Loss: 0.0058, Test Loss: 0.0096\n",
            "Epoch [5290/6000], Train Loss: 0.0058, Test Loss: 0.0096\n",
            "Epoch [5291/6000], Train Loss: 0.0058, Test Loss: 0.0096\n",
            "Epoch [5292/6000], Train Loss: 0.0058, Test Loss: 0.0096\n",
            "Epoch [5293/6000], Train Loss: 0.0058, Test Loss: 0.0096\n",
            "Epoch [5294/6000], Train Loss: 0.0057, Test Loss: 0.0096\n",
            "Epoch [5295/6000], Train Loss: 0.0057, Test Loss: 0.0096\n",
            "Epoch [5296/6000], Train Loss: 0.0057, Test Loss: 0.0096\n",
            "Epoch [5297/6000], Train Loss: 0.0057, Test Loss: 0.0096\n",
            "Epoch [5298/6000], Train Loss: 0.0057, Test Loss: 0.0096\n",
            "Epoch [5299/6000], Train Loss: 0.0057, Test Loss: 0.0096\n",
            "Epoch [5300/6000], Train Loss: 0.0057, Test Loss: 0.0096\n",
            "Epoch [5301/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5302/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5303/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5304/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5305/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5306/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5307/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5308/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5309/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5310/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5311/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5312/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5313/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5314/6000], Train Loss: 0.0057, Test Loss: 0.0095\n",
            "Epoch [5315/6000], Train Loss: 0.0057, Test Loss: 0.0094\n",
            "Epoch [5316/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5317/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5318/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5319/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5320/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5321/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5322/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5323/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5324/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5325/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5326/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5327/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5328/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5329/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5330/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5331/6000], Train Loss: 0.0056, Test Loss: 0.0094\n",
            "Epoch [5332/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5333/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5334/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5335/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5336/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5337/6000], Train Loss: 0.0056, Test Loss: 0.0093\n",
            "Epoch [5338/6000], Train Loss: 0.0055, Test Loss: 0.0093\n",
            "Epoch [5339/6000], Train Loss: 0.0055, Test Loss: 0.0093\n",
            "Epoch [5340/6000], Train Loss: 0.0055, Test Loss: 0.0093\n",
            "Epoch [5341/6000], Train Loss: 0.0055, Test Loss: 0.0093\n",
            "Epoch [5342/6000], Train Loss: 0.0055, Test Loss: 0.0093\n",
            "Epoch [5343/6000], Train Loss: 0.0055, Test Loss: 0.0093\n",
            "Epoch [5344/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5345/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5346/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5347/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5348/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5349/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5350/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5351/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5352/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5353/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5354/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5355/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5356/6000], Train Loss: 0.0055, Test Loss: 0.0092\n",
            "Epoch [5357/6000], Train Loss: 0.0055, Test Loss: 0.0091\n",
            "Epoch [5358/6000], Train Loss: 0.0055, Test Loss: 0.0091\n",
            "Epoch [5359/6000], Train Loss: 0.0055, Test Loss: 0.0091\n",
            "Epoch [5360/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5361/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5362/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5363/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5364/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5365/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5366/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5367/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5368/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5369/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5370/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5371/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5372/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5373/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5374/6000], Train Loss: 0.0054, Test Loss: 0.0091\n",
            "Epoch [5375/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5376/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5377/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5378/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5379/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5380/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5381/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5382/6000], Train Loss: 0.0054, Test Loss: 0.0090\n",
            "Epoch [5383/6000], Train Loss: 0.0053, Test Loss: 0.0090\n",
            "Epoch [5384/6000], Train Loss: 0.0053, Test Loss: 0.0090\n",
            "Epoch [5385/6000], Train Loss: 0.0053, Test Loss: 0.0090\n",
            "Epoch [5386/6000], Train Loss: 0.0053, Test Loss: 0.0090\n",
            "Epoch [5387/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5388/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5389/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5390/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5391/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5392/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5393/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5394/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5395/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5396/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5397/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5398/6000], Train Loss: 0.0053, Test Loss: 0.0088\n",
            "Epoch [5399/6000], Train Loss: 0.0053, Test Loss: 0.0088\n",
            "Epoch [5400/6000], Train Loss: 0.0053, Test Loss: 0.0088\n",
            "Epoch [5401/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5402/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5403/6000], Train Loss: 0.0053, Test Loss: 0.0089\n",
            "Epoch [5404/6000], Train Loss: 0.0053, Test Loss: 0.0088\n",
            "Epoch [5405/6000], Train Loss: 0.0053, Test Loss: 0.0088\n",
            "Epoch [5406/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5407/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5408/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5409/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5410/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5411/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5412/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5413/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5414/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5415/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5416/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5417/6000], Train Loss: 0.0052, Test Loss: 0.0088\n",
            "Epoch [5418/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5419/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5420/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5421/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5422/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5423/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5424/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5425/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5426/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5427/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5428/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5429/6000], Train Loss: 0.0052, Test Loss: 0.0087\n",
            "Epoch [5430/6000], Train Loss: 0.0051, Test Loss: 0.0087\n",
            "Epoch [5431/6000], Train Loss: 0.0051, Test Loss: 0.0087\n",
            "Epoch [5432/6000], Train Loss: 0.0051, Test Loss: 0.0087\n",
            "Epoch [5433/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5434/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5435/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5436/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5437/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5438/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5439/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5440/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5441/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5442/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5443/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5444/6000], Train Loss: 0.0051, Test Loss: 0.0085\n",
            "Epoch [5445/6000], Train Loss: 0.0051, Test Loss: 0.0085\n",
            "Epoch [5446/6000], Train Loss: 0.0051, Test Loss: 0.0085\n",
            "Epoch [5447/6000], Train Loss: 0.0051, Test Loss: 0.0085\n",
            "Epoch [5448/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5449/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5450/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5451/6000], Train Loss: 0.0051, Test Loss: 0.0086\n",
            "Epoch [5452/6000], Train Loss: 0.0051, Test Loss: 0.0085\n",
            "Epoch [5453/6000], Train Loss: 0.0051, Test Loss: 0.0085\n",
            "Epoch [5454/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5455/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5456/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5457/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5458/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5459/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5460/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5461/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5462/6000], Train Loss: 0.0050, Test Loss: 0.0085\n",
            "Epoch [5463/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5464/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5465/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5466/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5467/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5468/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5469/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5470/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5471/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5472/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5473/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5474/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5475/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5476/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5477/6000], Train Loss: 0.0050, Test Loss: 0.0084\n",
            "Epoch [5478/6000], Train Loss: 0.0049, Test Loss: 0.0084\n",
            "Epoch [5479/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5480/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5481/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5482/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5483/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5484/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5485/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5486/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5487/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5488/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5489/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5490/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5491/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5492/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5493/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5494/6000], Train Loss: 0.0049, Test Loss: 0.0083\n",
            "Epoch [5495/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5496/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5497/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5498/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5499/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5500/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5501/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5502/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5503/6000], Train Loss: 0.0049, Test Loss: 0.0082\n",
            "Epoch [5504/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5505/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5506/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5507/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5508/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5509/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5510/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5511/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5512/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5513/6000], Train Loss: 0.0048, Test Loss: 0.0082\n",
            "Epoch [5514/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5515/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5516/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5517/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5518/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5519/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5520/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5521/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5522/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5523/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5524/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5525/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5526/6000], Train Loss: 0.0048, Test Loss: 0.0080\n",
            "Epoch [5527/6000], Train Loss: 0.0048, Test Loss: 0.0080\n",
            "Epoch [5528/6000], Train Loss: 0.0048, Test Loss: 0.0081\n",
            "Epoch [5529/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5530/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5531/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5532/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5533/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5534/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5535/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5536/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5537/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5538/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5539/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5540/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5541/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5542/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5543/6000], Train Loss: 0.0047, Test Loss: 0.0080\n",
            "Epoch [5544/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5545/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5546/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5547/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5548/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5549/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5550/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5551/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5552/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5553/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5554/6000], Train Loss: 0.0047, Test Loss: 0.0079\n",
            "Epoch [5555/6000], Train Loss: 0.0046, Test Loss: 0.0079\n",
            "Epoch [5556/6000], Train Loss: 0.0046, Test Loss: 0.0079\n",
            "Epoch [5557/6000], Train Loss: 0.0046, Test Loss: 0.0079\n",
            "Epoch [5558/6000], Train Loss: 0.0046, Test Loss: 0.0079\n",
            "Epoch [5559/6000], Train Loss: 0.0046, Test Loss: 0.0079\n",
            "Epoch [5560/6000], Train Loss: 0.0046, Test Loss: 0.0079\n",
            "Epoch [5561/6000], Train Loss: 0.0046, Test Loss: 0.0079\n",
            "Epoch [5562/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5563/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5564/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5565/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5566/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5567/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5568/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5569/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5570/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5571/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5572/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5573/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5574/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5575/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5576/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5577/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5578/6000], Train Loss: 0.0046, Test Loss: 0.0078\n",
            "Epoch [5579/6000], Train Loss: 0.0046, Test Loss: 0.0077\n",
            "Epoch [5580/6000], Train Loss: 0.0046, Test Loss: 0.0077\n",
            "Epoch [5581/6000], Train Loss: 0.0046, Test Loss: 0.0077\n",
            "Epoch [5582/6000], Train Loss: 0.0046, Test Loss: 0.0077\n",
            "Epoch [5583/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5584/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5585/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5586/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5587/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5588/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5589/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5590/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5591/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5592/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5593/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5594/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5595/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5596/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5597/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5598/6000], Train Loss: 0.0045, Test Loss: 0.0077\n",
            "Epoch [5599/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5600/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5601/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5602/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5603/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5604/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5605/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5606/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5607/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5608/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5609/6000], Train Loss: 0.0045, Test Loss: 0.0076\n",
            "Epoch [5610/6000], Train Loss: 0.0044, Test Loss: 0.0076\n",
            "Epoch [5611/6000], Train Loss: 0.0044, Test Loss: 0.0076\n",
            "Epoch [5612/6000], Train Loss: 0.0044, Test Loss: 0.0076\n",
            "Epoch [5613/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5614/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5615/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5616/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5617/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5618/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5619/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5620/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5621/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5622/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5623/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5624/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5625/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5626/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5627/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5628/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5629/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5630/6000], Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch [5631/6000], Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch [5632/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5633/6000], Train Loss: 0.0044, Test Loss: 0.0075\n",
            "Epoch [5634/6000], Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch [5635/6000], Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch [5636/6000], Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch [5637/6000], Train Loss: 0.0044, Test Loss: 0.0074\n",
            "Epoch [5638/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5639/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5640/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5641/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5642/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5643/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5644/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5645/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5646/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5647/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5648/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5649/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5650/6000], Train Loss: 0.0043, Test Loss: 0.0074\n",
            "Epoch [5651/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5652/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5653/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5654/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5655/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5656/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5657/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5658/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5659/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5660/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5661/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5662/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5663/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5664/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5665/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5666/6000], Train Loss: 0.0043, Test Loss: 0.0073\n",
            "Epoch [5667/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5668/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5669/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5670/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5671/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5672/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5673/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5674/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5675/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5676/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5677/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5678/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5679/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5680/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5681/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5682/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5683/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5684/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5685/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5686/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5687/6000], Train Loss: 0.0042, Test Loss: 0.0072\n",
            "Epoch [5688/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5689/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5690/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5691/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5692/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5693/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5694/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5695/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5696/6000], Train Loss: 0.0042, Test Loss: 0.0071\n",
            "Epoch [5697/6000], Train Loss: 0.0041, Test Loss: 0.0071\n",
            "Epoch [5698/6000], Train Loss: 0.0041, Test Loss: 0.0071\n",
            "Epoch [5699/6000], Train Loss: 0.0041, Test Loss: 0.0071\n",
            "Epoch [5700/6000], Train Loss: 0.0041, Test Loss: 0.0071\n",
            "Epoch [5701/6000], Train Loss: 0.0041, Test Loss: 0.0071\n",
            "Epoch [5702/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5703/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5704/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5705/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5706/6000], Train Loss: 0.0041, Test Loss: 0.0071\n",
            "Epoch [5707/6000], Train Loss: 0.0041, Test Loss: 0.0071\n",
            "Epoch [5708/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5709/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5710/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5711/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5712/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5713/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5714/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5715/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5716/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5717/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5718/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5719/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5720/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5721/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5722/6000], Train Loss: 0.0041, Test Loss: 0.0070\n",
            "Epoch [5723/6000], Train Loss: 0.0041, Test Loss: 0.0069\n",
            "Epoch [5724/6000], Train Loss: 0.0041, Test Loss: 0.0069\n",
            "Epoch [5725/6000], Train Loss: 0.0041, Test Loss: 0.0069\n",
            "Epoch [5726/6000], Train Loss: 0.0041, Test Loss: 0.0069\n",
            "Epoch [5727/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5728/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5729/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5730/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5731/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5732/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5733/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5734/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5735/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5736/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5737/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5738/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5739/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5740/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5741/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5742/6000], Train Loss: 0.0040, Test Loss: 0.0069\n",
            "Epoch [5743/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5744/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5745/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5746/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5747/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5748/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5749/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5750/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5751/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5752/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5753/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5754/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5755/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5756/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5757/6000], Train Loss: 0.0040, Test Loss: 0.0068\n",
            "Epoch [5758/6000], Train Loss: 0.0039, Test Loss: 0.0068\n",
            "Epoch [5759/6000], Train Loss: 0.0039, Test Loss: 0.0068\n",
            "Epoch [5760/6000], Train Loss: 0.0039, Test Loss: 0.0068\n",
            "Epoch [5761/6000], Train Loss: 0.0039, Test Loss: 0.0068\n",
            "Epoch [5762/6000], Train Loss: 0.0039, Test Loss: 0.0068\n",
            "Epoch [5763/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5764/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5765/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5766/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5767/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5768/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5769/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5770/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5771/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5772/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5773/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5774/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5775/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5776/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5777/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5778/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5779/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5780/6000], Train Loss: 0.0039, Test Loss: 0.0067\n",
            "Epoch [5781/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5782/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5783/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5784/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5785/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5786/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5787/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5788/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5789/6000], Train Loss: 0.0039, Test Loss: 0.0066\n",
            "Epoch [5790/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5791/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5792/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5793/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5794/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5795/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5796/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5797/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5798/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5799/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5800/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5801/6000], Train Loss: 0.0038, Test Loss: 0.0066\n",
            "Epoch [5802/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5803/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5804/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5805/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5806/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5807/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5808/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5809/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5810/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5811/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5812/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5813/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5814/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5815/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5816/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5817/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5818/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5819/6000], Train Loss: 0.0038, Test Loss: 0.0065\n",
            "Epoch [5820/6000], Train Loss: 0.0038, Test Loss: 0.0064\n",
            "Epoch [5821/6000], Train Loss: 0.0038, Test Loss: 0.0064\n",
            "Epoch [5822/6000], Train Loss: 0.0038, Test Loss: 0.0064\n",
            "Epoch [5823/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5824/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5825/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5826/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5827/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5828/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5829/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5830/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5831/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5832/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5833/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5834/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5835/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5836/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5837/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5838/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5839/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5840/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5841/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5842/6000], Train Loss: 0.0037, Test Loss: 0.0064\n",
            "Epoch [5843/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5844/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5845/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5846/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5847/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5848/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5849/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5850/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5851/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5852/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5853/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5854/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5855/6000], Train Loss: 0.0037, Test Loss: 0.0063\n",
            "Epoch [5856/6000], Train Loss: 0.0036, Test Loss: 0.0063\n",
            "Epoch [5857/6000], Train Loss: 0.0036, Test Loss: 0.0063\n",
            "Epoch [5858/6000], Train Loss: 0.0036, Test Loss: 0.0063\n",
            "Epoch [5859/6000], Train Loss: 0.0036, Test Loss: 0.0063\n",
            "Epoch [5860/6000], Train Loss: 0.0036, Test Loss: 0.0063\n",
            "Epoch [5861/6000], Train Loss: 0.0036, Test Loss: 0.0063\n",
            "Epoch [5862/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5863/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5864/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5865/6000], Train Loss: 0.0036, Test Loss: 0.0063\n",
            "Epoch [5866/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5867/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5868/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5869/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5870/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5871/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5872/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5873/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5874/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5875/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5876/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5877/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5878/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5879/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5880/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5881/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5882/6000], Train Loss: 0.0036, Test Loss: 0.0061\n",
            "Epoch [5883/6000], Train Loss: 0.0036, Test Loss: 0.0061\n",
            "Epoch [5884/6000], Train Loss: 0.0036, Test Loss: 0.0062\n",
            "Epoch [5885/6000], Train Loss: 0.0036, Test Loss: 0.0061\n",
            "Epoch [5886/6000], Train Loss: 0.0036, Test Loss: 0.0061\n",
            "Epoch [5887/6000], Train Loss: 0.0036, Test Loss: 0.0061\n",
            "Epoch [5888/6000], Train Loss: 0.0036, Test Loss: 0.0061\n",
            "Epoch [5889/6000], Train Loss: 0.0036, Test Loss: 0.0061\n",
            "Epoch [5890/6000], Train Loss: 0.0036, Test Loss: 0.0061\n",
            "Epoch [5891/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5892/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5893/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5894/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5895/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5896/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5897/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5898/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5899/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5900/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5901/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5902/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5903/6000], Train Loss: 0.0035, Test Loss: 0.0061\n",
            "Epoch [5904/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5905/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5906/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5907/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5908/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5909/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5910/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5911/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5912/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5913/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5914/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5915/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5916/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5917/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5918/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5919/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5920/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5921/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5922/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5923/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5924/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5925/6000], Train Loss: 0.0035, Test Loss: 0.0060\n",
            "Epoch [5926/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5927/6000], Train Loss: 0.0034, Test Loss: 0.0060\n",
            "Epoch [5928/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5929/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5930/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5931/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5932/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5933/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5934/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5935/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5936/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5937/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5938/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5939/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5940/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5941/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5942/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5943/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5944/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5945/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5946/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5947/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5948/6000], Train Loss: 0.0034, Test Loss: 0.0059\n",
            "Epoch [5949/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5950/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5951/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5952/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5953/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5954/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5955/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5956/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5957/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5958/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5959/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5960/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5961/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5962/6000], Train Loss: 0.0034, Test Loss: 0.0058\n",
            "Epoch [5963/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5964/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5965/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5966/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5967/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5968/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5969/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5970/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5971/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5972/6000], Train Loss: 0.0033, Test Loss: 0.0058\n",
            "Epoch [5973/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5974/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5975/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5976/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5977/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5978/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5979/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5980/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5981/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5982/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5983/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5984/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5985/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5986/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5987/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5988/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5989/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5990/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5991/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5992/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5993/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5994/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5995/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5996/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5997/6000], Train Loss: 0.0033, Test Loss: 0.0057\n",
            "Epoch [5998/6000], Train Loss: 0.0033, Test Loss: 0.0056\n",
            "Epoch [5999/6000], Train Loss: 0.0033, Test Loss: 0.0056\n",
            "Epoch [6000/6000], Train Loss: 0.0033, Test Loss: 0.0056\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAACANklEQVR4nO3dd3hUVf7H8c9k0glJQDChSWhSpBcRG6xUsWFFQEHWH64ICsaKBQK4oi4iawPFRVwFQVdFVESawVVRFESRRQTpHYQkhJA69/fHOCOTemcykyl5v55nHsmdc+89M2eC8+Hc+z0WwzAMAQAAAADKFObvDgAAAABAoCM4AQAAAEAFCE4AAAAAUAGCEwAAAABUgOAEAAAAABUgOAEAAABABQhOAAAAAFABghMAAAAAVIDgBAAAAAAVIDgBCAm33XabUlJSPNo3LS1NFovFux1CuSozXtVJdna2zj77bM2fP7/Sx9q1a5csFovmzZvn0f4Wi0VpaWmV7kdVc7zu6dOn+7sr5Xr44YfVvXt3f3cDQDkITgB8ymKxmHqkp6f7u6shz/EF0sxj165dlT7fgQMHlJaWpo0bN1b6WN7Uq1cvtW3b1t/dMOWf//ynatasqZtvvtm5zRH0w8LCtHfv3hL7ZGVlKSYmRhaLRWPHjq3K7pb6GYuPj1fHjh314osvqqioyKPjLliwQDNnzvRuZ9105MgRPfzww2rXrp3i4uIUHR2t5s2ba+TIkfryyy9d2s6bN8/lPYiOjlb9+vXVv39/Pf/88zp58mSJ448fP14//vijlixZUlUvCYCbwv3dAQCh7c0333T5+d///rdWrFhRYnvr1q0rdZ45c+bIZrN5tO9jjz2mhx9+uFLnDwZ169Yt8b4/++yz2rdvn5577rkSbSvrwIEDmjx5slJSUtSxY0eX5yozXtVFQUGB/vnPf+ree++V1Wot8XxUVJTefvttPfjggy7b33///VKP17hxY50+fVoREREe9ef06dMKDzf3tWHIkCEaOHCgJCkzM1NLly7V3Xffrd27d+sf//iH2+desGCBfv75Z40fP97tfb1h3bp1uuKKK3Ty5EndfPPNuvPOOxUVFaWdO3dq8eLFmjdvntasWaNLL73UZb8pU6aoSZMmKigo0KFDh5Senq7x48drxowZWrJkidq3b+9sm5ycrGuuuUbTp0/X1VdfXdUvEYAJBCcAPnXLLbe4/PzNN99oxYoVJbYXl5OTo9jYWNPn8fTLoCSFh4eb/kIYzGrUqFHifV+4cKFOnDhR4Xh4W2XGq7r4+OOPdfToUd10002lPj9w4MBSg9OCBQt0xRVX6L333nPZ7pj58JQ7+3bu3NnlM3XXXXepe/fuWrBggUfByZ9OnDihQYMGKTw8XBs3blSrVq1cnn/iiSe0cOFCxcTElNj38ssvV9euXZ0/T5gwQatXr9aVV16pq6++Wlu2bHHZ76abbtKNN96oHTt2qGnTpr57UQA8wqV6APzOcenU+vXrdemllyo2NlaPPPKIJOnDDz/UFVdcofr16ysqKkrNmjXT1KlTS1zyU/yemTPva3j11VfVrFkzRUVFqVu3bvruu+9c9i3tHifHZU6LFy9W27ZtFRUVpfPOO0/Lli0r0f/09HR17dpV0dHRatasmV555RVT902NHTtWcXFxysnJKfHckCFDlJyc7Hyd33//vfr37686deooJiZGTZo00V//+tdyj++pvLw8TZo0Sc2bN1dUVJQaNWqkBx98UHl5eS7tVqxYoYsvvliJiYmKi4tTy5YtneOWnp6ubt26SZJGjhzpvGTJcX9NZcZLkt599121adNG0dHRatu2rT744AOv3zf18ssv67zzzlNUVJTq16+vMWPGKCMjw6XNtm3bdP311ys5OVnR0dFq2LChbr75ZmVmZpp6n8qzePFipaSkqFmzZqU+P3ToUG3cuFG//PKLc9uhQ4e0evVqDR06tET70u5xuu222xQXF6f9+/dr0KBBiouLU926dXX//feX+B2rzD1OFotFSUlJJf6Bwszvd69evfTJJ59o9+7dzs/RmeOcm5urtLQ0nXvuuYqOjla9evV03XXX6bfffivRDzOfreJmz56tgwcPaubMmSVCk+O1DRkyxPl5r8hll12mxx9/XLt379Zbb73l8lyfPn2c7wuAwBP6/8QKICj8/vvvuvzyy3XzzTfrlltuUVJSkiT7vQJxcXFKTU1VXFycVq9erYkTJyorK8vUv1wvWLBAJ0+e1N/+9jdZLBY988wzuu6667Rjx44KZz2+/PJLvf/++7rrrrtUs2ZNPf/887r++uu1Z88enXXWWZKkH374QQMGDFC9evU0efJkFRUVacqUKaYudRs8eLBeeuklffLJJ7rxxhud23NycvTRRx/ptttuk9Vq1ZEjR9SvXz/VrVtXDz/8sBITE7Vr164yL8mqDJvNpquvvlpffvml7rjjDrVu3VqbNm3Sc889p19//VWLFy+WJG3evFlXXnml2rdvrylTpigqKkrbt2/XV199Jcl+6eWUKVM0ceJE3XHHHbrkkkskSRdeeGG55zczXp988okGDx6sdu3aadq0aTpx4oRuv/12NWjQwGvvQ1pamiZPnqw+ffpo9OjR2rp1q2bNmqXvvvtOX331lSIiIpSfn6/+/fsrLy9Pd999t5KTk7V//359/PHHysjIUEJCQoXvU3m+/vprde7cucznL730UjVs2FALFizQlClTJEmLFi1SXFycrrjiCtOvtaioSP3791f37t01ffp0rVy5Us8++6yaNWum0aNHmz7OmXJycnTs2DFJ9nuuPv30Uy1btkwTJkxwaWfm9/vRRx9VZmamyyWlcXFxzr5feeWVWrVqlW6++WaNGzdOJ0+e1IoVK/Tzzz+7hE5P/y746KOPFBMTo+uuu86j96I0t956qx555BEtX75co0aNcm5PSEhQs2bN9NVXX+nee+/12vkAeIkBAFVozJgxRvG/enr27GlIMmbPnl2ifU5OToltf/vb34zY2FgjNzfXuW3EiBFG48aNnT/v3LnTkGScddZZxvHjx53bP/zwQ0OS8dFHHzm3TZo0qUSfJBmRkZHG9u3bndt+/PFHQ5LxwgsvOLddddVVRmxsrLF//37ntm3bthnh4eEljlmczWYzGjRoYFx//fUu29955x1DkvHFF18YhmEYH3zwgSHJ+O6778o9nieuuOIKl/ftzTffNMLCwoz//ve/Lu1mz55tSDK++uorwzAM47nnnjMkGUePHi3z2N99950hyXj99ddLPFeZ8WrXrp3RsGFD4+TJk85t6enphiSXY5alZ8+exnnnnVfm80eOHDEiIyONfv36GUVFRc7tL774oiHJmDt3rmEYhvHDDz8Ykox33323zGOZeZ9KU1BQYFgsFuO+++4r8Zzj83r06FHj/vvvN5o3b+58rlu3bsbIkSMNw7B/hseMGeN8zvEenzkeI0aMMCQZU6ZMcTlHp06djC5durhsk2RMmjSp3H47zlHaY/To0YbNZnNpb/b3u/jn1GHu3LmGJGPGjBklnnOcy53PVmlq1apldOzYscT2rKws4+jRo85Hdna287nXX3+9wt/ZhIQEo1OnTiW29+vXz2jdunW5fQLgH1yqByAgREVFaeTIkSW2n3n9/8mTJ3Xs2DFdcsklysnJcblEqSyDBw9WrVq1nD87Zj527NhR4b59+vRx+Rfr9u3bKz4+3rlvUVGRVq5cqUGDBql+/frOds2bN9fll19e4fEtFotuvPFGLV26VNnZ2c7tixYtUoMGDXTxxRdLkhITEyXZ73kpKCio8LiV8e6776p169Zq1aqVjh075nxcdtllkqTPP//cpU8ffvihV4s8VDReBw4c0KZNmzR8+HDnrIMk9ezZU+3atfNKH1auXKn8/HyNHz9eYWF//m9y1KhRio+P1yeffCLJPjsgSZ999lmpl1tKnr9Px48fl2EYLu9FaYYOHart27fru+++c/63tMv0KnLnnXe6/HzJJZeY+h0pyx133KEVK1ZoxYoVeu+99zRmzBi98sorSk1NdWlX2d/v9957T3Xq1NHdd99d4rnil8p6+ndBVlaWy2fN4dZbb1XdunWdj4ceeqjC/p4pLi6u1Op6tWrVcs7WAQgsBCcAAaFBgwaKjIwssX3z5s269tprlZCQoPj4eNWtW9d50/mZ95GU5ZxzznH52fHF6cSJE27v69jfse+RI0d0+vRpNW/evES70raVZvDgwTp9+rSzBHF2draWLl2qG2+80fnFr2fPnrr++us1efJk1alTR9dcc41ef/31EvccecO2bdu0efNmly+EdevW1bnnnivJ/pod/b7ooov0f//3f0pKStLNN9+sd955p9IhqqLx2r17t6TS31+z73lFHOdo2bKly/bIyEg1bdrU+XyTJk2Umpqq1157TXXq1FH//v310ksvuXwuK/s+GYZR7vOdOnVSq1attGDBAs2fP1/JycnOkGtWdHR0iUtLz/yclyY/P1+HDh1yeZx5X1KLFi3Up08f9enTR9ddd51efPFF3XXXXZo5c6Y2bdrkbFfZ3+/ffvtNLVu2NFXcxdO/C2rWrOnyDxsOU6ZMcYZDT2RnZ6tmzZolthuGwbpyQIDiHicAAaG0ilQZGRnq2bOn4uPjNWXKFDVr1kzR0dHasGGDHnroIVNfPksr4yxV/IW0svuadcEFFyglJUXvvPOOhg4dqo8++kinT5/W4MGDnW0sFov+85//6JtvvtFHH32kzz77TH/961/17LPP6ptvvin1X8M9ZbPZ1K5dO82YMaPU5xs1aiTJPl5ffPGFPv/8c33yySdatmyZFi1apMsuu0zLly8v872rSFW859707LPP6rbbbtOHH36o5cuX65577tG0adP0zTffqGHDhh6/T7Vr15bFYjEV8IcOHapZs2apZs2aGjx4sMssmRmejNXXX3+tv/zlLy7bdu7cWe4+vXv31osvvqgvvvhC7dq188rvtzs8/Wy1atVKP/74owoKClzuhTqzlLi79u3bp8zMzFLD/okTJ1SnTh2Pjw3Ad5hxAhCw0tPT9fvvv2vevHkaN26crrzySvXp06fCy5eqytlnn63o6Ght3769xHOlbSvLTTfdpGXLlikrK0uLFi1SSkqKLrjgghLtLrjgAv3973/X999/r/nz52vz5s1auHBhpV5Dcc2aNdPx48fVu3dv54zBmY8zZ2HCwsLUu3dvzZgxQ//73//097//XatXr3ZezueLfzVv3LixpNLfX3feczPn2Lp1q8v2/Px87dy50/m8Q7t27fTYY4/piy++0H//+1/t379fs2fPdj5f0ftUmvDwcDVr1qzCMCLZg9PBgwf166+/enSZnic6dOjgnG1xPJKTk8vdp7CwUJKcszfu/H6X9Vlq1qyZtm7d6tNLWK+88kqdPn1aH3zwgdeO6VhPrX///iWe27lzZ6XXtQPgGwQnAAHL8S/EZ/6LcH5+vl5++WV/dcmF1WpVnz59tHjxYh04cMC5ffv27fr0009NH2fw4MHKy8vTG2+8oWXLlpVYt+fEiRMl/lXcsaDsmZfr/fbbb6WWYHbHTTfdpP3792vOnDklnjt9+rROnTolyX4PTnHF+1SjRg1JKlHCuzLq16+vtm3b6t///rfL5VNr1qxxuQSsMvr06aPIyEg9//zzLu/7v/71L2VmZjor1mVlZTnDgEO7du0UFhbmfA/MvE9l6dGjh77//vsK+9usWTPNnDlT06ZN0/nnn19he2+oVatWiVBd0TpPH330kSR76JLc+/2uUaNGqZfuXX/99Tp27JhefPHFEs95a5Zy9OjRSkpK0r333qtff/210udZvXq1pk6dqiZNmmjYsGEuz2VmZuq3336rsPokAP/gUj0AAevCCy9UrVq1NGLECN1zzz2yWCx68803A+qyrbS0NC1fvlwXXXSRRo8eraKiIr344otq27atNm7caOoYnTt3VvPmzfXoo48qLy/P5TI9SXrjjTf08ssv69prr1WzZs108uRJzZkzR/Hx8Ro4cKCzXe/evSXZ1+vx1K233qp33nlHd955pz7//HNddNFFKioq0i+//KJ33nlHn332mbp27aopU6boiy++0BVXXKHGjRvryJEjevnll9WwYUNnUYtmzZopMTFRs2fPVs2aNVWjRg11795dTZo08bh/kvTkk0/qmmuu0UUXXaSRI0fqxIkTzve8tHtRSnP06FE98cQTJbY7vsxOmDBBkydP1oABA3T11Vdr69atevnll9WtWzfnPTirV6/W2LFjdeONN+rcc89VYWGh3nzzTVmtVl1//fWSZOp9Kss111yjN998U7/++qvzHrOyjBs3ztTrriobNmxwrlF08uRJrVq1Su+9954uvPBC9evXT5J7v99dunTRokWLlJqaqm7duikuLk5XXXWVhg8frn//+99KTU3VunXrdMkll+jUqVNauXKl7rrrLl1zzTWVfi21a9fWBx98oKuuukodOnTQzTffrG7duikiIkJ79+7Vu+++K6n0eyI//fRT/fLLLyosLNThw4e1evVqrVixQo0bN9aSJUtKhM2VK1fKMAyv9BuAD1R9IT8A1VlZ5cjLKg/91VdfGRdccIERExNj1K9f33jwwQeNzz77zJBkfP755852ZZW3/sc//lHimCpWVrmscuRnlnJ2aNy4sTFixAiXbatWrTI6depkREZGGs2aNTNee+0147777jOio6PLeBdKevTRRw1JLqWlHTZs2GAMGTLEOOecc4yoqCjj7LPPNq688krj+++/L9E3M+W4z1Ramef8/Hzj6aefNs477zwjKirKqFWrltGlSxdj8uTJRmZmpvM1X3PNNUb9+vWNyMhIo379+saQIUOMX3/91eVYH374odGmTRtneXZHKezKjJdhGMbChQuNVq1aGVFRUUbbtm2NJUuWGNdff73RqlWrCl+zo/x9aY/evXs727344otGq1atjIiICCMpKckYPXq0ceLECefzO3bsMP76178azZo1M6Kjo43atWsbf/nLX4yVK1c625h9n0qTl5dn1KlTx5g6darL9jPLkZen+Ge4rHLkNWrUKLFvWb8TnpQjDw8PN5o2bWo88MADLiXkDcP873d2drYxdOhQIzExsUTZ+ZycHOPRRx81mjRpYkRERBjJycnGDTfcYPz2228ufTL72SrLwYMHjQceeMBo06aNERMTY0RFRRlNmzY1hg8f7lw6wMFRjtzxiIyMNJKTk42+ffsa//znP42srKxSzzF48GDj4osvNtUfAFXPYhgB9E+3ABAiBg0apM2bN2vbtm3+7kq10bFjR9WtW9fjKmeBaOrUqXr99de1bds2jwtuIDgcOnRITZo00cKFC5lxAgIU9zgBQCWdPn3a5edt27Zp6dKl6tWrl386FOIKCgpK3FuUnp6uH3/8MeTe83vvvVfZ2dleLwKCwDNz5ky1a9eO0AQEMGacAKCS6tWrp9tuu825xs+sWbOUl5enH374QS1atPB390LOrl271KdPH91yyy2qX7++fvnlF82ePVsJCQn6+eefddZZZ/m7iwCAEERxCACopAEDBujtt9/WoUOHFBUVpR49eujJJ58kNPlIrVq11KVLF7322ms6evSoatSooSuuuEJPPfUUoQkA4DPMOAEAAABABbjHCQAAAAAqQHACAAAAgApUu3ucbDabDhw4oJo1a8pisfi7OwAAAAD8xDAMnTx5UvXr11dYWPlzStUuOB04cECNGjXydzcAAAAABIi9e/eqYcOG5bapdsGpZs2akuxvTnx8vF/7UlBQoOXLl6tfv36KiIjwa1/gPYxr6GFMQxPjGnoY09DEuIaeQBrTrKwsNWrUyJkRylPtgpPj8rz4+PiACE6xsbGKj4/3+4cG3sO4hh7GNDQxrqGHMQ1NjGvoCcQxNXMLD8UhAAAAAKACBCcAAAAAqADBCQAAAAAqUO3ucQIAAEDoKCoqUkFBgb+7ATcUFBQoPDxcubm5Kioq8vn5IiIiZLVaK30cghMAAACCUnZ2tvbt2yfDMPzdFbjBMAwlJydr7969VbKuqsViUcOGDRUXF1ep4xCcAAAAEHSKioq0b98+xcbGqm7dulXyBRzeYbPZlJ2drbi4uAoXna0swzB09OhR7du3Ty1atKjUzBPBCQAAAEGnoKBAhmGobt26iomJ8Xd34Aabzab8/HxFR0f7PDhJUt26dbVr1y4VFBRUKjhRHAIAAABBi5kmVMRbnxGCEwAAAABUgOAEAAAAABUgOAEAAKDaKiqS0tOlt9+2/7cKqmN7XUpKimbOnGm6fXp6uiwWizIyMnzWp1BEcAIAAEC19P77UkqK9Je/SEOH2v+bkmLf7gsWi6XcR1pamkfH/e6773THHXeYbn/hhRfq4MGDSkhI8Oh8ZoVaQKOqHgAAAKqd99+XbrhBKr4E1P799u3/+Y903XXePefBgwedf160aJEmTpyorVu3Oreduc6QYRgqKipSeHjFX9fr1q3rVj8iIyOVnJzs1j5gxsmv8vOlJUuaaty4MM2caf8ZAAAA7jMM6dQpc4+sLOmee0qGJsdxJGncOHs7M8czu/5ucnKy85GQkCCLxeL8+ZdfflHNmjX16aefqkuXLoqKitKXX36p3377Tddcc42SkpIUFxenbt26aeXKlS7HLX6pnsVi0WuvvaZrr71WsbGxatGihZYsWeJ8vvhM0Lx585SYmKjPPvtMrVu3VlxcnAYMGOAS9AoLC3XPPfcoMTFRZ511lh566CGNGDFCgwYNMvfiS3HixAkNHz5ctWrVUmxsrC6//HJt27bN+fzu3bt11VVXqVatWqpRo4bOO+88LV261LnvsGHDnOXoW7Rooddff93jvphBcPKTBx+U4uLCNXduO82aZdW990rR0fbtAAAAcE9OjhQXZ+6RkGCfWSqLYUj79tnbmTleTo73XsfDDz+sp556Slu2bFH79u2VnZ2tgQMHatWqVfrhhx80YMAAXXXVVdqzZ0+5x5k8ebJuuukm/fTTTxo4cKCGDRum48ePl9k+JydH06dP15tvvqkvvvhCe/bs0f333+98/umnn9b8+fP1+uuv66uvvlJWVpYWL15cqdc6cuRIff/991qyZInWrl0rwzA0cOBAFRQUSJLGjBmjvLw8ffHFF9q0aZOefvpp56zc448/rv/973/69NNPtWXLFs2aNUt16tSpVH8qwqV6fvDgg9I//lFyu2H8uf2ZZ6q2TwAAAPC/KVOmqG/fvs6fa9eurQ4dOjh/njp1qj744AMtWbJEY8eOLfM4t912m4YMGSJJevLJJ/X8889r3bp1GjBgQKntCwoKNHv2bDVr1kySNHbsWE2ZMsX5/AsvvKAJEybo2muvlSS9+OKLztkfT/z222/66KOP9NVXX+nCCy+UJM2fP1+NGjXS4sWLdeONN2rPnj26/vrr1a5dO0lS06ZNnfvv2bNHnTp1UteuXSXZZ918jRmnKpafL02f7vip9MW4pk/nsj0AAAB3xMZK2dnmHma/7y9dau54sbHeex2OIOCQnZ2t+++/X61bt1ZiYqLi4uK0ZcuWCmec2rdv7/xzjRo1FB8fryNHjpTZPjY21hmaJKlevXrO9pmZmTp8+LDOP/985/NWq1VdunRx67WdaevWrQoPD1f37t2d28466yy1bNlSW7ZskSTdc889euKJJ3TRRRdp0qRJ+umnn5xtR48erYULF6pjx4568MEH9fXXX3vcF7MITlXs+ecrvg7WMOztAAAAYI7FItWoYe7Rr5/UsKF9n7KO1aiRvZ2Z45V1HE/UqFHD5ef7779fH3zwgZ588kn997//1caNG9WuXTvlV/Cv7BEREcVek0U2m82t9obZm7d85P/+7/+0Y8cO3Xrrrdq0aZO6du2qF154QZJ0+eWXa/fu3br33nt14MAB9e7d2+XSQl8gOFWx997zbjsAAAC4x2qV/vlP+5+Lhx7HzzNn2tv521dffaXbbrtN1157rdq1a6fk5GTt2rWrSvuQkJCgpKQkfffdd85tRUVF2rBhg8fHbNmypQoLC/Xtt986t/3+++/aunWr2rRp49zWqFEj3XnnnXr//fd13333ac6cOc7n6tatqxEjRuitt97SzJkz9eqrr3rcHzO4x6mK/fabuXaV+BwCAACgAtddZy85Pm6cvRCEQ8OG9tDk7VLknmrRooXef/99XXXVVbJYLHr88cfLnTnylbvvvlvTpk1T8+bN1apVK73wwgs6ceKELCam2zZt2qSaNWs6fzYMQ82aNdPVV1+tUaNG6ZVXXlHNmjX18MMPq0GDBrrmmmskSePHj9fll1+uc889VydOnNDnn3+u1q1bS5ImTpyoLl266LzzzlNeXp4+/vhj53O+QnCqYmZnPPPzpdOnpZgY3/YHAACgurruOumaa6T//lc6eFCqV0+65JLAmGlymDFjhv7617/qwgsvVJ06dfTQQw8pKyuryvvx0EMP6dChQxo+fLisVqvuuOMO9e/fX1YTb9all17q8rPVatWxY8c0d+5c3XvvvbryyiuVn5+vSy+9VEuXLnVeNlhUVKQxY8Zo3759io+P14ABA/Tcc89Jsq9FNWHCBO3atUsxMTG65JJLtHDhQu+/8DNYDH9fvFjFsrKylJCQoMzMTMXHx1f5+du2lTZvNtf2b3+TZs/2bX/gfQUFBVq6dKkGDhxY4nphBCfGNDQxrqGHMQ1NZY1rbm6udu7cqSZNmig6OtqPPayebDabWrdurZtuuklTp051e9+srCzFx8crLMz3dw6V91lxJxtwj1MVGz7cfNs33/RdPwAAAACzdu/erTlz5ujXX3/Vpk2bNHr0aO3cuVNDhw71d9eqDMGpio0fb75tTo79cj0AAADAn8LCwjRv3jx169ZNF110kTZt2qSVK1f6/L6iQMI9TlUsMlJKTJQyMsy1v/deLtcDAACAfzVq1EhfffWVv7vhV8w4+cHNN5tv6+N73AAAAACYQHDygxkzzLfNzLRX2AMAAADgPwQnP4iJkaKizLefOdNnXQEAAABgAsHJTy6+2Hzbp5/2XT8AAAAAVIzg5CcPPGC+7fHjXK4HAAAA+BPByU/69JEk82sPc7keAAAA4D9+D04vvfSSUlJSFB0dre7du2vdunXlts/IyNCYMWNUr149RUVF6dxzz9XSpUurqLfeY7VKbduaD05uLsgMAACAai4tLU0dO3b0dzdChl+D06JFi5SamqpJkyZpw4YN6tChg/r3768jR46U2j4/P199+/bVrl279J///Edbt27VnDlz1KBBgyruuXc89ZTNdNvsbBbDBQAA8Jq0tLL/ZXrqVPvzXmaxWMp9pFXinBaLRYsXL3bZdv/992vVqlWV67QJ1SWg+XUB3BkzZmjUqFEaOXKkJGn27Nn65JNPNHfuXD388MMl2s+dO1fHjx/X119/rYiICElSSkpKVXbZq3r3NiTZZDa/Xn21tGKFT7sEAABQPVit0sSJ9j8//vif26dOtW+fMsXrpzx48KDzz4sWLdLEiRO1detW57a4uDivni8uLs7rx6zO/Bac8vPztX79ek2YMMG5LSwsTH369NHatWtL3WfJkiXq0aOHxowZow8//FB169bV0KFD9dBDD8lqtZa6T15envLy8pw/Z2VlSZIKCgpUUFDgxVfkPputQO3b/66ffko21X7lSkO5uYUq46UiQDg+V/7+fMF7GNPQxLiGHsY0NJU1rgUFBTIMQzabTTabTTIMKSfH/IHHj5fy8hQ2caJseXnSQw9JTz+tsL//XbZHH7U/f/KkuWPFxkoWS4XNzj77bOefa9asKYvF4rLttdde03PPPaedO3cqJSVFd999t0aPHi3J/t35vvvu0/vvv68TJ04oKSlJf/vb3/Twww+radOmkqRrr71WktS4cWPt2LFDkydP1ocffqgNGzZIkkaOHKmMjAxdfPHFmjFjhvLz8zV48GA999xzzkmJgwcPatSoUfr888+VnJysqVOn6rHHHtO4ceM0bty4Ul+XYdhvP7HZSr+aatOmTbr33nu1du1axcbG6rrrrtOkSZNUs2ZN2Ww2paen6+GHH9bmzZsVERGh8847T2+99ZYaN26sH3/8Uampqfr+++9lsVjUokULzZo1S127dq3w/Xaw2WwyDEMFBQUlMoM7f1/4LTgdO3ZMRUVFSkpKctmelJSkX375pdR9duzYodWrV2vYsGFaunSptm/frrvuuksFBQWaNGlSqftMmzZNkydPLrF9+fLlio2NrfwLqaRHHpFuvvlqSRX/skkWPfnkt+rS5XdfdwtesILpwZDDmIYmxjX0MKahqfi4hoeHKzk5WdnZ2crPz5dOnVJiw4YeHTvs73+X/v73Mn+uSMa+fVKNGm6dMzc3V4ZhOP9R/5133tGkSZP0zDPPqH379vrpp580btw4hYWFaciQIXrhhRf04Ycf6l//+pcaNmyo/fv3a//+/crKytLKlSvVokULvfTSS+rdu7esVquysrKUl5enoqIil4mDzz//XGeddZY+/PBD7dixQ7fffrtatmypESNGSJJuueUW/f777/roo48UERGhRx99VEeOHFFubq7zOMUVP8+ZTp06pQEDBqhbt25atWqVjh07pnvuuUenTp3Syy+/rMLCQl177bUaPny4XnnlFeXn52vDhg3Kzs5WVlaWhg4dqvbt22vVqlWyWq3atGmT8vLyyuxLafLz83X69Gl98cUXKiwsdHkux42w7ddL9dxls9l09tln69VXX5XValWXLl20f/9+/eMf/ygzOE2YMEGpqanOn7OystSoUSP169dP8fHxVdX1UhUUFGjFihWKijKUl2cmOEkvv9xDBw+avzcKVc8xrn379nX+6w2CG2MamhjX0MOYhqayxjU3N1d79+5VXFycoqOj5c9LcuLj490OTtHR0bJYLM7vo88884ymT5+uIUOGSJLatWunXbt26c0339Tf/vY3HTlyRC1btlT//v1lsVjUtm1b1/NLSk5OVosWLZzbo6KiZLVanc9HRESodu3aeuWVV2S1WtW1a1e99957+vrrr3X33Xfrl19+UXp6ur799lvnjM7cuXPVsmVLRUdHl/ndufh5zrRo0SLl5eVp/vz5qvHHe2SxWDRo0CBNnz5dkZGRysrK0nXXXacOHTpIkrp16+bcf//+/XrwwQed/enUqZNb77Nk/6zExMTo0ksvtX9WzuBOAPNbcKpTp46sVqsOHz7ssv3w4cNKTi790rV69eopIiLCZYqtdevWOnTokPLz8xUZGVlin6ioKEVFRZXYHhERETB/qd56q6HXXjPX9vffrSostComxrd9QuUF0mcM3sGYhibGNfQwpqGp+LgWFRXJYrEoLCxMYWFhUlycvZqWu556SnriCSky0r5w5mOPSaXca1+eMJOX6rnsExbm/O+pU6f022+/adSoUfrb3/7mbFNYWKiEhASFhYVp5MiR6tu3r1q3bq0BAwboyiuvVL9+/Uoc03FcyR5QzjyXxWLReeed5/I+1q9fX5s2bVJYWJi2bdum8PBwde3a1bnPueeeq1q1ajnf69IUP8+Ztm7dqg4dOqhmzZrObRdffLFsNpt+/fVX9erVS7fddpsuv/xy9e3bV3369NFNN92kevXqSZJSU1N1xx13aP78+erTp49uvPFGNWvWzOS7/Of7YrFYSv27wZ2/K/xWVS8yMlJdunRxqfRhs9m0atUq9ejRo9R9LrroIm3fvt3l+slff/1V9erVKzU0BYtnn3VvBqmMy0sBAACqL4vFPuvjzmPGDHtomjJFysuz//eJJ+zb3TmOm6GpuOw/At+cOXO0ceNG5+Pnn3/WN998I0nq3Lmzdu7cqalTp+r06dO66aabdMMNN7h9ruJBwWKxlHlvUlV5/fXXtXbtWl144YVatGiRzj33XOfrTktL0+bNm3XFFVdo9erVatOmjT744AO/9NOv5chTU1M1Z84cvfHGG9qyZYtGjx6tU6dOOavsDR8+3KV4xOjRo3X8+HGNGzdOv/76qz755BM9+eSTGjNmjL9eglfExEhnnWW+/euv+64vAAAA1cKZ1fMcVfUef9z+88SJVbqIZlJSkurXr68dO3aoefPmLo8mTZo428XHx2vw4MGaM2eOFi1apPfee0/Hjx+XZA9ERUVFlepHy5YtVVhYqB9++MG5bfv27Tpx4oTHx2zdurV+/PFHnTp1yrntq6++UlhYmFq2bOnc1qlTJ02YMEFff/212rZtqwULFjifO/fcc3Xvvfdq+fLluu666/S6n74M+/Uep8GDB+vo0aOaOHGiDh06pI4dO2rZsmXOghF79uxxmfJr1KiRPvvsM917771q3769GjRooHHjxumhhx7y10vwmvnzpQEDzLUtLLTPRFNdEgAAwENFRa6hycHxcyVDiLsmT56se+65RwkJCRowYIDy8vL0/fff68SJE0pNTdWMGTNUr149derUSWFhYXr33XeVnJysxMRESfYlelatWqWLLrpIUVFRqlWrltt9aNWqlfr06aM77rhDs2bNUkREhO677z7FxMQ4L8cry+nTp7Vx40aXbTVr1tSwYcM0adIkjRgxQmlpaTp69KjGjRunwYMHKykpSTt37tSrr76qq6++WvXr19fWrVu1bds2DR8+XKdPn9YDDzygG264QU2aNNG+ffv03Xff6frrr3f7tXmD34tDjB07VmPHji31ufT09BLbevTo4Zy6CyV9+rjXvnlz6dAh3/QFAAAg5JW32GzxMFUF/u///k+xsbH6xz/+oQceeEA1atRQu3btNH78eEn2EPLMM89o27Ztslqt6tatm5YuXeqcZHj22WedV3M1aNBAu3bt8qgf//73v3X77bfr0ksvVXJysqZNm6bNmzeXKKpQ3K+//lqicEPv3r21cuVKffbZZxo3bpy6devmUo5ckmJjY/XLL7/ojTfe0O+//6569eppzJgx+tvf/qbCwkL9/vvvGj58uA4fPqw6derouuuuK7VidlWwGI7C69VEVlaWEhISlJmZGRBV9ZYuXaqBAwcqIiJCfftKK1ea3z8nRxSJCEDFxxXBjzENTYxr6GFMQ1NZ45qbm6udO3eqSZMmFX6ph+f27dunRo0aaeXKlerdu7dXjmmz2ZSVlaX4+PgyC054U3mfFXeygV/vcYKrJUvca0+RCAAAAHjT6tWrtWTJEu3cuVNff/21br75ZqWkpOjSSy/1d9f8juAUQCgSAQAAAH8qKCjQI488ovPOO0/XXnut6tatq/T0dGZxFQD3OMEVRSIAAADgL/3791f//v393Y2AxIxTgPGkSAQAAAAA3yI4BRir1b3wdPiwdPq07/oDAAAQyKpZnTN4wFufEYJTAKJIBAAAQPmsVqskKT8/3889QaBzfEYcnxlPcY9TAHIUifj9d3Pt33hDevVV3/YJAAAgkISHhys2NlZHjx5VRERElZS1hnfYbDbl5+crNzfX5+Nms9l09OhRxcbGKjy8ctGH4BSg3CkSkZ9vv1yPNZ0AAEB1YbFYVK9ePe3cuVO7d+/2d3fgBsMwdPr0acXExMhisfj8fGFhYTrnnHMqfS6CU4Byt0hE9+7STz/5pi8AAACBKDIyUi1atOByvSBTUFCgL774QpdeemmVlDmPjIz0yswWwSlAOYpErFxprv2mTfaZp8hI3/YLAAAgkISFhSk6Otrf3YAbrFarCgsLFR0dHVTrQ3ExaABzt0jEzJk+6QYAAABQ7RGcAlhMjBQfb77900/7ri8AAABAdUZwCnALF5pve/y4/XI9AAAAAN5FcApw/fq5157L9QAAAADvIzgFOKtVatvWfPupU33XFwAAAKC6IjgFgenTzbfNzrav6QQAAADAewhOQcDdNZ3GjfNNPwAAAIDqiuAUBBxrOpn1xhu+6wsAAABQHRGcgoQ7azrl53O5HgAAAOBNBKcgERMjRUWZb3/vvb7rCwAAAFDdEJyCyIgR5tu++abv+gEAAABUNwSnIOLOGk05OVyuBwAAAHgLwSmIxMRIsbHm23O5HgAAAOAdBKcgM3y4+barV/uuHwAAAEB1QnAKMjNmmG974oTv+gEAAABUJwSnIBMTI9Wsaa7t8eNSUZFv+wMAAABUBwSnINSunbl2Npu0apVv+wIAAABUBwSnINSkifm2b7zhu34AAAAA1QXBKQi5s57Tzp2+6wcAAABQXRCcgtBll5lvu3277/oBAAAAVBcEpyBktUoNG5pre/SolJ/v2/4AAAAAoY7gFKQaNzbf9sUXfdcPAAAAoDogOAWpQYPMt12zxmfdAAAAAKoFglOQuuce82337vVdPwAAAIDqgOAUpCIjpVq1zLXdvdu3fQEAAABCHcEpiCUkmGt3/DgFIgAAAIDKIDgFsfr1zbelQAQAAADgOYJTELv2WvNtKRABAAAAeI7gFMTcKRCxaZPv+gEAAACEOoJTEIuMlGrXNtd21y6pqMin3QEAAABCFsEpyJm9z8kwpFWrfNsXAAAAIFQRnIJc+/bm277+uu/6AQAAAIQyglOQu+02821/+sln3QAAAABCGsEpyF12mfm2e/f6rh8AAABAKCM4BTmrVapb11zbkydZCBcAAADwBMEpBLRoYb7t88/7rh8AAABAqCI4hQB3FsJ9/33f9QMAAAAIVQSnEODOQrjbt/uuHwAAAECoIjiFAHcWwj12jIVwAQAAAHcRnEJEp07m2rEQLgAAAOA+gpM/pKVJU6eW/tzUqfbn3XT++ebbshAuAAAA4J5wf3egWlqzRkpPV1hRketU0dSp0sSJUq9ebh+yd29p2jRzbVkIFwAAAHAPM07+sHOnJMk6ebLOXbTIfv2cIzSd8bw73MlaLIQLAAAAuIcZJ3/IyHD+sfXbb8t45x3Xig1nPG+WYyHco0crbutYCDcy0u3TAAAAANUSM04BwOKlMncshAsAAAD4BsHJH2rVKv95i8WjAhEshAsAAAD4RkAEp5deekkpKSmKjo5W9+7dtW7dujLbzps3TxaLxeURHR1dhb31gr/+tfznMzLsBSTc5M5CuD//7PbhAQAAgGrL78Fp0aJFSk1N1aRJk7RhwwZ16NBB/fv315EjR8rcJz4+XgcPHnQ+du/eXYU9riIeFIiIjJTi4821ddznBAAAAKBifg9OM2bM0KhRozRy5Ei1adNGs2fPVmxsrObOnVvmPhaLRcnJyc5HUlJSFfbYC1avrrjNoUMeHbptW/Ntuc8JAAAAMMevVfXy8/O1fv16TZgwwbktLCxMffr00dq1a8vcLzs7W40bN5bNZlPnzp315JNP6rzzziu1bV5envLy8pw/Z2VlSZIKCgpUUFDgpVfiHuuuXRUmVkOS7fHHZXOUKDfp6qvD9PXXVlNt33uvSOPG2dw6Pirm+Fz56/MF72NMQxPjGnoY09DEuIaeQBpTd/rg1+B07NgxFRUVlZgxSkpK0i+//FLqPi1bttTcuXPVvn17ZWZmavr06brwwgu1efNmNWzYsET7adOmafLkySW2L1++XLGxsd55IW66/MgRVVQJ3JKXp9xXXtHKrl3dOnbTppJ0tSRLhW03b87X0qXL3To+zFuxYoW/uwAvY0xDE+MaehjT0MS4hp5AGNOcnBzTbS2GYRg+7Eu5Dhw4oAYNGujrr79Wjx49nNsffPBBrVmzRt9++22FxygoKFDr1q01ZMgQTZ06tcTzpc04NWrUSMeOHVO82RuCvMxat67CMjMrbGckJKjQzMJMxdSubVV2dsVXYVoshnJyCmU1N0EFkwoKCrRixQr17dtXERER/u4OvIAxDU2Ma+hhTEMT4xp6AmlMs7KyVKdOHWVmZlaYDfw641SnTh1ZrVYdPnzYZfvhw4eVnJxs6hgRERHq1KmTtm/fXurzUVFRioqKKnU/vw1UrVqSieBkkTzqY+PG0ubNFbczDIu++CJC/fq5fQqY4NfPGHyCMQ1NjGvoYUxDE+MaegJhTN05v1+LQ0RGRqpLly5atWqVc5vNZtOqVatcZqDKU1RUpE2bNqlevXq+6qb3jRghpaT47PAdOphv+/rrPusGAAAAEDL8XlUvNTVVc+bM0RtvvKEtW7Zo9OjROnXqlEaOHClJGj58uEvxiClTpmj58uXasWOHNmzYoFtuuUW7d+/W//3f//nrJbjP7OK2eXkeBazbbjPf9ssv3T48AAAAUO349VI9SRo8eLCOHj2qiRMn6tChQ+rYsaOWLVvmLBixZ88ehYX9me9OnDihUaNG6dChQ6pVq5a6dOmir7/+Wm3atPHXS/CMmXLjubkelSW/7DLzbffvl4qKxH1OAAAAQDn8HpwkaezYsRo7dmypz6Wnp7v8/Nxzz+m5556rgl75WFSUPRhVJDra7UNbrVKjRtLevRW3NQxp1SpxnxMAAABQDr9fqldtjR8vW2Kizw5/ySXm23KfEwAAAFA+gpO/pKXZp3sqUgX3Of30k9uHBwAAAKoVglOgq4L7nHbscPvwAAAAQLVCcAoAFc47eXifU9265trm5kqnT7t9CgAAAKDaIDj5U/v2KoyIkKWidhaL+RLmZ2jRwnzb++5z+/AAAABAtUFw8qOiMxb+LVdGhjRzptvHv/Za822XLXP78AAAAEC1QXDyM1tEhLmG2dluH/uee8y33bXLvp4TAAAAgJIITsEi3P0ltyIjJbMVzx3rOQEAAAAoieDkZ1lNmshmpviDxeJRWfIuXcy3/de/3D48AAAAUC0QnPzsq7//XRYz6znl5kr79rl9/AceMN/200/dPjwAAABQLRCcAoDhTrlxN6vr9eljvu3Jk1J+vluHBwAAAKoFglMgMHsjUlGR29X1rFapTh3z7Z9/3q3DAwAAANUCwSkAGLfean6R29xct4/ft6/5tnPmuH14AAAAIOQRnAKAbeJEqaDAXGN3Luv7w8iR5ttu20ZZcgAAAKA4glOgMFtuPC9P6tXLrUNfdpn5tpQlBwAAAEoiOAWKCy4wN5uUmytt3OjWoa1WqWVL8+0pSw4AAAC4IjgFivR085freXCf0//9n/m2lCUHAAAAXBGcAonZy/UsFrfLkt9zj/m2lCUHAAAAXBGcAskFF5gLT7m50rx5bh06MlKqWdN8e8qSAwAAAH8iOAWS9HT7DUlmHDrk9uGvvNJ8W8qSAwAAAH8iOAWamBhz7QoL3T60O2XJf/2VsuQAAACAA8Ep0CQmmmsXEeHTsuSStHy5e+0BAACAUEVwCjQjRkhRURW3y82VvvnGrUNbrVLbtubbT5/u1uEBAACAkEVwCjRpafaqeT7iThhyM5cBAAAAIYvgFIjMzDhJ9vuh3CxL3qeP+bY5OZQlBwAAACSCU2AaP97cvU4ZGdLMmW4d2mqVkpPNt3fz8AAAAEBIIjgForQ0+z1MZmRnu334QYPMt33mGbcPDwAAAIQcglOgKigw187MgrnFzJhhvu3vv3O5HgAAAEBwClQNG0rR0RW3s1iklBS3Dh0TY365KInL9QAAAACCU6Datctcu9xc6dAhtw/vzuV6L7zg9uEBAACAkEJwCmSGYa6dmZmpYkaONN923z6pqMjtUwAAAAAhg+AUyOrVM9fOYnG7LPlll7nXleXL3WsPAAAAhBKCUyAbMcLcbJKHZcnPPdd8e3cWzgUAAABCDcEpkKWlma+uZ7Z8+RlGjTLf9ptv3D48AAAAEDIIToHObLlxD+5zuuce821zcihLDgAAgOqL4BToLrjAfFlyN+9zioyUEhPNt6csOQAAAKorglOgS083V10vI0OaN8/tw998s/m2Tz/t9uEBAACAkEBwCgZmV6v1YD2nGTPMtz1+nMv1AAAAUD0RnIKBO9fTuSkmxr3bo7hcDwAAANURwSkYmC1LHhMj9erl9uGHDzff9vnn3T48AAAAEPQITsEgLc38fU4bN7p9eHdmkfbvl4qK3D4FAAAAENQITsHC7H1OtWp5dOiICPPtly93+xQAAABAUCM4BQuz9zmZmZkqxSWXmG/74IMenQIAAAAIWgSnYDFihLnwlJHh0eHdCUM//8zlegAAAKheCE7BIi1Nys2tuJ3F4lGBiD593GvP5XoAAACoTghOwcSHBSKsVqltW/PtuVwPAAAA1QnBKZhccIFP13SaPt18Wy7XAwAAQHVCcAom6ek+Pby7l+utWuWbfgAAAACBhuAUbMzMOHl4n5PVKjVsaL795MlunwIAAAAISgSnUBMWZr/Pafduj3a/+27zbdeu5XI9AAAAVA8Ep2DjKDdusZT+vM1m/6+H6zmNH2++rWFwuR4AAACqB4JTsOnQwX65XkXByMP1nCIjpdq1zbf/1788Og0AAAAQVAhOwcbHBSIk6aGHzLddssR3/QAAAAACBcEpVNWubV801wPuXK6XmyudPu3RaQAAAICgQXAKVTt3SvPmebRrZKRUo4b59uPGeXQaAAAAIGgQnIKR4z6ninh4n5MkDRtmvu2//+3xaQAAAICgEBDB6aWXXlJKSoqio6PVvXt3rVu3ztR+CxculMVi0aBBg3zbwUCTnm4uONWq5fEpZs403zYvj8v1AAAAENr8HpwWLVqk1NRUTZo0SRs2bFCHDh3Uv39/HTlypNz9du3apfvvv1+XXHJJFfU0CHlYklySYmKkuDjz7blcDwAAAKHM78FpxowZGjVqlEaOHKk2bdpo9uzZio2N1dy5c8vcp6ioSMOGDdPkyZPVtGnTKuxtADFzGV5YmMcFIiTp8cfNt33jDY9PAwAAAAS8cH+ePD8/X+vXr9eECROc28LCwtSnTx+tXbu2zP2mTJmis88+W7fffrv++9//lnuOvLw85eXlOX/OysqSJBUUFKigoKCSr6ByHOf3pB9Ww1CYJMNikaWUmSWbpLCdO2VLT1eRh69zzBjpoYfCJZWx2O4Z8vMNZWUVKibGo1OFlMqMKwITYxqaGNfQw5iGJsY19ATSmLrTB78Gp2PHjqmoqEhJSUku25OSkvTLL7+Uus+XX36pf/3rX9q4caOpc0ybNk2TJ08usX358uWKjY11u8++sGLFCrf3uahhQ8VERalGGZc0OqYST//vf1q5dKnHfYuOvly5uZEmWlp04417ddddmzw+V6jxZFwR2BjT0MS4hh7GNDQxrqEnEMY0JyfHdFu/Bid3nTx5UrfeeqvmzJmjOnXqmNpnwoQJSk1Ndf6clZWlRo0aqV+/foqPj/dVV00pKCjQihUr1LdvX0VERLi388CBsp57boXNYvPzNXDgQA97KN1yS5hee81c2/T0FH38cSOPzxUqKjWuCEiMaWhiXEMPYxqaGNfQE0hj6rgazQy/Bqc6derIarXq8OHDLtsPHz6s5OTkEu1/++037dq1S1dddZVzm81mkySFh4dr69atatasmcs+UVFRioqKKnGsiIgIvw+Ug8d9MXGfk6VWrUq9zuefl+nglJ8fpsLCMC7X+0MgfcbgHYxpaGJcQw9jGpoY19ATCGPqzvn9WhwiMjJSXbp00apVq5zbbDabVq1apR49epRo36pVK23atEkbN250Pq6++mr95S9/0caNG9WoUTWb7TBTkrwSlfUkqusBAAAAUgBU1UtNTdWcOXP0xhtvaMuWLRo9erROnTqlkSNHSpKGDx/uLB4RHR2ttm3bujwSExNVs2ZNtW3bVpGRZu7FqWYqWVlPoroeAAAA4PfgNHjwYE2fPl0TJ05Ux44dtXHjRi1btsxZMGLPnj06ePCgn3sZoBo3lpo0Kb/Nzp3SvHmVOs348ebb5uezGC4AAABCT0AUhxg7dqzGjh1b6nPp6enl7juvkqEgqKWnS3/5iz0clcdScTnx8kRGSjVqSKdOmWt/9dVSABRJAQAAALzG7zNOqKRduypuk5JS6dMMG2a+7cqVUlFRpU8JAAAABAyCU7AzUVmvwhkpE2bOdK/98uWVPiUAAAAQMAhOwc5MZb3MzEoXiHC3ut6DD1bqdAAAAEBAIThVBxkZ0po1lT6MO9X1fv6Zy/UAAAAQOghOwc5MZT3JK5fruVNdT+JyPQAAAIQOglOwS083t8htJSvrSfbqerVrm2//t79V+pQAAABAQCA4VRdmwpUJDz1kvu3evfZ1nQAAAIBgR3AKBWbKjXthxkly/3I9d6vxAQAAAIGI4BQKzKzlJFW6sp5kv1yvUSPz7Z9+utKnBAAAAPyO4FRd7Nrllcp6kjRnjvm2x49zuR4AAACCH8EpFIwYYe5yPS9U1pOkPn3ca8/legAAAAh2BKdQYPYSPC/d52S1Sm3bmm8/dapXTgsAAAD4DcGpOvFSZT1Jmj7dfNvsbOn0aa+dGgAAAKhyBKfqxEszTpL7l+uNG+e1UwMAAABVjuBUnVgsUq9eXjmU1epeeHrjDa+cFgAAAPALglOoMFscYvdur51yyRLzbfPzuVwPAAAAwYvgFCp69jQXnrx4n1NMjBQVZb599+5eOzUAAABQpQhOoaKKK+s5jBhhvu2mTazpBAAAgOBEcKpuvDjjJLm/RhNrOgEAACAYEZyqGy/POMXESPHx5ts/9phXTw8AAABUCYJTdePFynoOCxeab1tQIM2f79XTAwAAAD5HcAolI0ZITZqU38bLlfUkqV8/99rfeqtUVOTVLgAAAAA+RXAKJWlp5u5h8vJ9Tlar1Lmz+faGIX36qVe7AAAAAPgUwak6ysjw+iHXrHGv/bXXer0LAAAAgM8QnKqjWrW8fsi4OPeKRBQWSm++6fVuAAAAAD5BcKqOmjY1v+6TG/bvd6/98OHc6wQAAIDgQHAKVWWVHQ8Lk1avlubN8/op3Z11kqRHH/V6NwAAAACvIziFGkdlvbIKQNhs9v96eT0nB3dnnZ5+mlknAAAABD6CU6jxU2U9h7g46ayz3Nvnhht80hUAAADAawhO1ZUPKus57N3rXvvFi6X8fJ90BQAAAPAKglMoMhOKfFBZzyEmRjrnHPf26djRJ10BAAAAvILgFIoSEytuY7FIvXr5rAu//OJe+y1bpNOnfdMXAAAAoLIITqFoxAjpssvKb7Nzp7R7t8+6EBMjtW7t3j7uzlIBAAAAVYXgFIrS0qQdOypu56MCEQ4bN7rX/tgxKTvbJ10BAAAAKoXgVJ35qCS5Q2SkdNFF7u3jw1uvAAAAAI8RnKozH884Sfa1dt1RWCjdc49v+gIAAAB4iuAUqsxU1svM9GmBCMk+63Ttte7t88ILlCcHAABAYCE4hSozlfUyMnxaIMLh3Xfd34fy5AAAAAgkBKdQNWKE1KRJxe2q4HI9q1X697/d24fy5AAAAAgkHgWnvXv3at++fc6f161bp/Hjx+vVV1/1WsdQSWlp5kKRmUv6vODWW+2X7bkjIcE3fQEAAADc5VFwGjp0qD7//HNJ0qFDh9S3b1+tW7dOjz76qKZMmeLVDsLHqrCMnbsZraCAQhEAAAAIDB4Fp59//lnnn3++JOmdd95R27Zt9fXXX2v+/PmaN2+eN/uHqpCWViWn8WRRXApFAAAAIBB4FJwKCgoUFRUlSVq5cqWuvvpqSVKrVq108OBB7/UOvrdrl7RmTZWdzt1FcSWpXj2vdwMAAABwi0fB6bzzztPs2bP13//+VytWrNCAAQMkSQcOHNBZZ53l1Q6iEho3Nlddb+dOn3fFITJSGjPGvX2OH5fmz/dNfwAAAAAzPApOTz/9tF555RX16tVLQ4YMUYcOHSRJS5YscV7ChwCQnm6+LHkVevFFKczNT94tt0hFRb7pDwAAAFCRcE926tWrl44dO6asrCzVOqO4wB133KHY2FivdQ6hKzNTqlnTvX1atZK2bfNNfwAAAIDyeDTjdPr0aeXl5TlD0+7duzVz5kxt3bpVZ599tlc7iEoyM5tksUi9evm6Jy7i4qRmzdzbZ/t2KTvbN/0BAAAAyuNRcLrmmmv07z9WNM3IyFD37t317LPPatCgQZo1a5ZXO4hKquhSPYvFHq48qdpQSVu3ur8PazsBAADAHzwKThs2bNAll1wiSfrPf/6jpKQk7d69W//+97/1/PPPe7WDqKSUlPKfN7NIro9YrdIf+ds0m026+27f9AcAAAAoi0fBKScnRzX/uEFl+fLluu666xQWFqYLLrhAu3fv9moHUUk9e1Ycnvzo1lvt6zu548UXWdsJAAAAVcuj4NS8eXMtXrxYe/fu1WeffaZ+/fpJko4cOaL4+HivdhCVZHZxW4ulyhbCLc6Ton5xcV7vBgAAAFAmj4LTxIkTdf/99yslJUXnn3++evToIck++9SpUyevdhBeYCaZZGRIM2f6uCOl82Rtp4ICLtkDAABA1fEoON1www3as2ePvv/+e3322WfO7b1799Zzzz3ntc6hiuXm+u3UnqztxCV7AAAAqCoereMkScnJyUpOTta+ffskSQ0bNmTx20DVoYP0448VzzxFR1dJd8qSnS25uwxYTAwL4wIAAMD3PJpxstlsmjJlihISEtS4cWM1btxYiYmJmjp1qmw2m7f7iMpKTzfXzg/rOZ0pJka6/HL39rHZpM6dfdMfAAAAwMGjGadHH31U//rXv/TUU0/poosukiR9+eWXSktLU25urv7+9797tZOoAn5cz+lMS5fau+KOH36Q3n5bGjLEN30CAAAAPApOb7zxhl577TVdffXVzm3t27dXgwYNdNdddxGcgpEf13MqLifH/Uv2hg6VbrrJvjYUAAAA4G0eXap3/PhxtWrVqsT2Vq1a6fjx424f76WXXlJKSoqio6PVvXt3rVu3rsy277//vrp27arExETVqFFDHTt21Jtvvun2OaudxER/98A0Ty7Zk6RSPpIAAACAV3gUnDp06KAXX3yxxPYXX3xR7du3d+tYixYtUmpqqiZNmqQNGzaoQ4cO6t+/v44cOVJq+9q1a+vRRx/V2rVr9dNPP2nkyJEaOXKkS3U/lGLECHPFH/Ly/Hqfk8PSpe7vs327/ZI9AAAAwNs8Ck7PPPOM5s6dqzZt2uj222/X7bffrjZt2mjevHmaPn26W8eaMWOGRo0apZEjR6pNmzaaPXu2YmNjNXfu3FLb9+rVS9dee61at26tZs2aady4cWrfvr2+/PJLT15K9ZGWZu5yvNxcv9/n5JCT4/4+Q4dSZQ8AAADe59E9Tj179tSvv/6ql156Sb/88osk6brrrtMdd9yhJ554Qpdccomp4+Tn52v9+vWaMGGCc1tYWJj69OmjtWvXVri/YRhavXq1tm7dqqeffrrUNnl5ecrLy3P+nJWVJUkqKChQQUGBqX76iuP8VdUPa3S0wvLyZEgqr/6CIanQz++NJIWHSwMGhGnZsjCV32NXsbFFys72X3XHqh5X+B5jGpoY19DDmIYmxjX0BNKYutMHi2F4ryrAjz/+qM6dO6vI5D/5HzhwQA0aNNDXX3+tHj16OLc/+OCDWrNmjb799ttS98vMzFSDBg2Ul5cnq9Wql19+WX/9619LbZuWlqbJkyeX2L5gwQLFuluBIMhd9Oijit+1S5GnTpXbLj82Vp8uWFBFvarYoEFXSnKn6oOhzp0PauLE73zVJQAAAISAnJwcDR06VJmZmYqPjy+3rccL4PpTzZo1tXHjRmVnZ2vVqlVKTU1V06ZN1auUe3MmTJig1NRU589ZWVlq1KiR+vXrV+Gb42sFBQVasWKF+vbtq4iICN+fcOBAWevWrbBZuM2mq++5R4Xbt/u+TyZkZtqUkODOrJNFGzbU01/+MlAxMb7sWemqfFzhc4xpaGJcQw9jGpoY19ATSGPquBrNDL8Gpzp16shqterw4cMu2w8fPqzk5OQy9wsLC1Pz5s0lSR07dtSWLVs0bdq0UoNTVFSUoqKiSmyPiIjw+0A5VGlfTCySFJabKx0+HEDvjzRwoLsFIyxKSIjwa5X1QPqMwTsY09DEuIYexjQ0Ma6hJxDG1J3ze1QcwlsiIyPVpUsXrVq1yrnNZrNp1apVLpfuVcRms7ncxwQvMFOBrwp98on7C+NKUtOm3u8LAAAAqh+3Zpyuu+66cp/PyMhwuwOpqakaMWKEunbtqvPPP18zZ87UqVOnNHLkSEnS8OHD1aBBA02bNk2SNG3aNHXt2lXNmjVTXl6eli5dqjfffFOzZs1y+9zVUmKiZGaccnPtlfjS0nzbHzfk5kqlTB6Wa+dOaf58adgw3/QJAAAA1YNbwSkhIaHC54cPH+5WBwYPHqyjR49q4sSJOnTokDp27Khly5YpKSlJkrRnzx6Fhf05MXbq1Cnddddd2rdvn2JiYtSqVSu99dZbGjx4sFvnrbZGjJCeesq+XlN58vKkefMCKjhFRkrjxkn//Kd7+91yi3TzzZLVnfoSAAAAwBncCk6vv/66TzoxduxYjR07ttTn0tPTXX5+4okn9MQTT/ikH9VCWpo9eZi5tPHQIZ93x10zZ0rvvy/t3evefuHh5paxAgAAAErj13ucAE/s2ePZfn9MYgIAAABuIzhVRx06mCv+YLFIKSk+744nPKkFcuSINH6817sCAACAaoDgVB2lp5u7bi03NyAv15Ps9zuNGeP+fv/8p5Sf7/3+AAAAILQRnKorsyvDBlhZ8jO9+KJn3XO3Mh8AAABAcEL5HGXJA9Tp057tFx/v3X4AAAAgtBGcqiuz9zk5ypIHsJwc9/c5eVIaOND7fQEAAEBoIjhVV8XKvJcrQO9zcoiJka64wv39Pv1Ueucd7/cHAAAAoYfgVJ2ZXdiosNC3/fCCjz/27H6nwYOloiLv9wcAAAChheBUnV1wQdCXJT+Tp/c7hbu1DDQAAACqI4JTdWa2LHlhYcBfrufgyfpOEsUiAAAAUD6CU3Vntix5YWFAV9dziIyU7r7b/f0oFgEAAIDyEJxgTlGRNHOmv3thyvPPS7Vqub8fxSIAAABQFoJTdWe2LLlkX9MpSBw/7tl+FIsAAABAaQhO1Z07ZcmDjKfFACkWAQAAgOIITpAKCsy1KyoKiup6DlartHChZ/t6UtocAAAAoYvgBKlhQ3PTLIWF0r59vu+PFw0e7NniuHl5UtOm3u8PAAAAghPBCdKuXfbpGTOC8Dq2jz/2rFjEzp3Svfd6vz8AAAAIPgQn2Fks5tsF0eV6Dp4Wi5g5U8rP92pXAAAAEIQITrBLSpLCTHwccnOD7nI9B0+LRURFebcfAAAACD4EJ9jt2iVFRJhra7MFxWK4xVWmWITZCTkAAACEJoIT/mQ2HRiGNG+eT7viK54Wi5CC8vYuAAAAeAnBCX9KSjKfDoL0cj3JXiyidm339ysqkuLjvd8fAAAABD6CE/60a5d9NskMm03q1cuXvfGp33/3bL+TJ6UuXbzbFwAAAAQ+ghNcOWacKpp5Mgzpm2983x8fMpsRi9uwQXr7be/2BQAAAIGN4ARXF1wgRUebK0EXAnW6PQ1PQ4faL90DAABA9UBwgqv0dKmgwFzbIF3Tqbi8PM/2o1gEAABA9UFwgmcsFvt9TkFcJMIhMlIaN86zfSlTDgAAUD0QnFBSw4bm7nGSgr5IhMPMmdI553i2L+EJAAAg9BGcUNKuXebbhkCRCIfduz2//C462rt9AQAAQGAhOKF0Vqv5tmYKSQQJs7d3FZeXJ9Wt692+AAAAIHAQnFC6hx82H54MIySKRDh4Wmnv2DGpc2fv9gUAAACBgeCE0qWl2e9fkiq+icdmk/bs8XmXqpKnk2g//CBdey2/VgAAAKGGb3goW3y8FBZmfgomBIpEOFit0sKFnu37ySdhmju3jXc7BAAAAL8iOKFsGRnmS8YZhvTllz7tTlUbPFi68kpP9rRoyZLmeu89yu0BAACECoITyudOmTnHpX0h5KOPpMaNPdnToiFDrCoq8naPAAAA4A8EJ5TvggvMzzpZLFJiok+74w+7dkmxsZ7safG4vDkAAAACC8EJ5UtPt9/nVJGwMPuMU2amz7vkD6dOeb4vC+QCAAAEP4ITKhYXV3GbMy/TC9HVYD0tUy4RngAAAIIdwQkVGz/evfZ5eT7pRiAgPAEAAFRPBCdULC1Niooy3z5E73VyIDwBAABUPwQnmPPww+6VJs/K8m1//IzwBAAAUL0QnGBOWtqffzZTLMJikVJSfNWbgEB4AgAAqD4ITjDvnHP+rJ5XEZtN2r3b933ys8JCz/clPAEAAAQPghPM27XL/UVuQ3zWyWqV3nnH8/0JTwAAAMGB4AT3JCS4174azDrdeKN0772e7094AgAACHwEJ7gnI8P9fUJ0XaczzZghXXml5/sTngAAAAIbwQnuc3fWKYTXdTrTRx9JnTtLkmdVIwhPAAAAgYvgBPe5uyCuVC1mnSRp/XqpUyebCE8AAAChheAE96WleTbr1KuXL3oTcL791qaUlBMiPAEAAIQOghM848ms05dfer0bgWrmzP8qJcXNCoRnIDwBAAAEFoITPJOWZq/F7Y6iIteFdEPcr7/aKlWNnfAEAAAQOAhO8Nxjj7m/z1NPeb8fAWznzsotZUV4AgAACAwEJ3guLU2KinJvn/x8n3QlkBGeAAAAgh/BCZWTm+tee6u1cikiSBGeAAAAghvBCZVntsJeeLhUWCjt2ePb/gSonTulJk0835/wBAAA4D8EJ1ReRoa5doWF9v8aRrUpTV7cjh1Sp06e72+x2GtsAAAAoGoFRHB66aWXlJKSoujoaHXv3l3r1q0rs+2cOXN0ySWXqFatWqpVq5b69OlTbntUkcaN3Wv/zTe+6UcQ2LBB6tzZ8/3Dw6V33/VefwAAAFAxvwenRYsWKTU1VZMmTdKGDRvUoUMH9e/fX0eOHCm1fXp6uoYMGaLPP/9ca9euVaNGjdSvXz/t37+/insOF7t2uXctWX5+tSpNXtz69VKXLp7vf9NN0v33e68/AAAAKJ/fg9OMGTM0atQojRw5Um3atNHs2bMVGxuruXPnltp+/vz5uuuuu9SxY0e1atVKr732mmw2m1atWlXFPUcJ55xjPjwZhvTEE77tT4D7/nvpyis93//ZZ6Vx47zXHwAAAJQt3J8nz8/P1/r16zVhwgTntrCwMPXp00dr1641dYycnBwVFBSodu3apT6fl5envLw8589ZWVmSpIKCAhUUFFSi95XnOL+/++E127YpPCpKFkm28HCFOe5pKoNRVKTCUHntZ3BnXN9/X7r//jA9/3yYJPerPzz/vKEvvrBp3Tqb2/vCvJD7XYUkxjUUMaahiXENPYE0pu70wWIYhuHDvpTrwIEDatCggb7++mv16NHDuf3BBx/UmjVr9O2331Z4jLvuukufffaZNm/erOjo6BLPp6WlafLkySW2L1iwQLGxsZV7ASih76hRivr9d1ltFX+Rt1ksKoqJ0dIFC6qgZ4Ft7tw2WrKkuTwJT5KhGjVyNX/+cm93CwAAIKTl5ORo6NChyszMVHx8fLlt/TrjVFlPPfWUFi5cqPT09FJDkyRNmDBBqampzp+zsrKc90VV9Ob4WkFBgVasWKG+ffsqIiLCr33xmv37FWZiUVxbWJjCbDZZcnI0cODAKuhY1fFkXAcOlB56yKbnnvNk5smiU6eiNXTolcrIYObJF0LydxWMawhiTEMT4xp6AmlMHVejmeHX4FSnTh1ZrVYdPnzYZfvhw4eVnJxc7r7Tp0/XU089pZUrV6p9+/ZltouKilJUKV/kIyIi/D5QDoHUF68IC6uwZnbYHzNSFkkRNWu6v5BuEHB3XGfMkC68ULrxRk/OZlFOjlWxsVYFwKx3yAq531VIYlxDEWMamhjX0BMIY+rO+f1aHCIyMlJdunRxKezgKPRw5qV7xT3zzDOaOnWqli1bpq5du1ZFV+GOiy92r8LeGfegVXc33PDncleeKCxkoVwAAABf8PuleqmpqRoxYoS6du2q888/XzNnztSpU6c0cuRISdLw4cPVoEEDTZs2TZL09NNPa+LEiVqwYIFSUlJ06NAhSVJcXJzi4uL89jpwhvR0+2JD7qzUGh0dkrNOnrBa7UUHKxOALBb7MQAAAOAdfg9OgwcP1tGjRzVx4kQdOnRIHTt21LJly5SUlCRJ2rNnj8LC/pwYmzVrlvLz83XDDTe4HGfSpElKq8brAgWchg2l3bvNt8/Lk1JS7OtBQRLhCQAAIJD4PThJ0tixYzV27NhSn0tPT3f5eRdfrIPDmQvihoVJJqrsac8en3YpGHkjPBUW2mexAAAA4Dm/L4CLENa4sfnQJNlTQkqKT7sUjCo7axQeLi1a5J2+AAAAVFcEJ/jOrl3mQ5ODO5f3VSOVDU833yxddZV3+gIAAFAdEZzgWwkJ7u9Txppc1V1lw9PHHzOhBwAA4CmCE3wrI8P9ffLypF69vN2TkGAYlbtfafduqUYN7/UHAACguiA4wfc8mXX64gvv9yNEFBZKlam8n5PDWk8AAADuIjjB9zyZdTIMe1UDlOrkycpfdkd4AgAAMI/ghKrRuLH7+xQVSYmJXu9KqNi5U7ryysodw2Jxb51iAACA6orghKqxa5dnN+dkZnK/Uzk++qjypcbDw6W33/ZOfwAAAEIVwQlV5+KLPdtvzRopLc2rXQklN91kv++pMoYOlTp39k5/AAAAQhHBCVUnPV2KivJs38mTvdqVUGO1Vr5c+Q8/SLGx3ukPAABAqCE4oWrl5nq+L9UMKlTZ8HT6NG8zAABAaQhOqHqelCd3COMjWxHDqHz4ITwBAAC44lsoql5GhueruFKm3BSbzfOrIh0sFik/3zv9AQAACHYEJ/jHY495vm9REVMiJuTmSk2aVO4YUVHSPfd4pz8AAADBjOAE/0hL82xtpzMRniq0Y4c0fnzljvHCC1Lt2l7pDgAAQNAiOMF/du2q3P1OEuHJhOeek/LyKneMEyd4qwEAQPVGcIJ/ZWR4p5IBi+SWKzKy8hX3JPtbXVRU+eMAAAAEG4IT/M9mq3x4WrOG8GSCNyruhYdLb7/tnf4AAAAEC4ITAoPNVvljrFlDxT0TbDapZs3KHWPoUKlTJ+/0BwAAIBgQnBA4Jk2q/DGouGdKVpbUuXPljrFxIzkVAABUHwQnBI60NKlnT+8cy2KRoqO9c6wQtX69tGBB5Y5BTgUAANUFwQmBJT3de+EpL49v9RUYMkQqLKz8cSwWKTu78scBAAAIVAQnBJ70dO9ctudgsUhhfNTLYrV6p+JezZrc9wQAAEIX3yYRmNLSvPNt3sFRTi4lxXvHDDGGUfmiERs3MskHAABCE8EJgc2b4UmSdu+2f7NPTPTucUOEN4pGSPa3OD+/8scBAAAIFAQnBD5vLD5UXGYmM1Bl8EbRCEmKipLuuafyxwEAAAgEBCcEB5vNfjOOtzlmoFg814W3ika88IJUq1bljwMAAOBvBCcEj8JCqXFj3xx7zRoCVDGOohGVzasZGfa3tqjIK90CAADwC4ITgsuuXd6/7+lMjgDFGlBOhYVSkyaVP054uPT225U/DgAAgD8QnBCcDMN3s0/Sn2tAUSJOkrRjh/TWW5U/ztChUocOlT8OAABAVSM4IXjt2uXd9Z7K4ghQaWm+P1cAGzbMO/c9/fQTl+4BAIDgQ3BCcHOs99Szp+/PNXmy/Rt/eLjvzxWgHPc9eeNKxvBwaf78yh8HAACgKhCcEBrS071TycCMoqI/Z6Gq6UzU6dNSp06VP84tt0gNG1b+OAAAAL5GcEJoKSy0B6ioqKo7p2MmKqx6/Tpt2OCd9Z7277e/fadPV/5YAAAAvlK9vumh+sjNrboZKIc/FuoNj4xU31Gjqu68fuSt9Z4kKTZWuvxy7xwLAADA2whOCG2OGagqrI5nkRRz9KjCIyOrxeV8jvue6tat/LGWLWP2CQAABCaCE6oHm61KA5Tlj4eT43I+xyMlpUr6UZWOHJHGjfPOsWJjpf79vXMsAAAAbyA4oXpxBKiEBP/2Y/du1yAVIrNSM2fal8DyhuXL7W/L8ePeOR4AAEBlEJxQPWVk2AOUYfi7J386c1bKG/W+/SQy0v62xsd753hnnSXFxEj5+d45HgAAgCcIToAjQFXhfVAVyssrOSMVZLNSmZnS3Xd751i5ufZCiXfe6Z3jAQAAuIvgBDg4LuNr3NjfPSlb8XulEhP93aNyPf+89y7dk6RXXrG/7Oxs7x0TAADADIITUNyuXX/OQgXSpXylycwM+Fkpx6V73qyHUbOmlJzM5XsAAKDqEJyAijgCVM+e/u6JOcVnpQKkkt/OndJbb3nveIcP2y/fu+MO7x0TAACgLAQnwKz09D9DVFSUv3vjvuKV/Pxwmd+wYfaltWJjvXfMOXPsLycz03vHBAAAKI7gBHgiN9f1cr4zCksYfzwCXmmX+VVBgQyrVTp1Spo3z7vHTUyUatRg8VwAAOAbBCfAGxyFJQxDhfn5soWHB0d4Kk0VhakRI+yzT0lJ3jtmTo59NqtbN6moyHvHBQAAIDgBPvDxf/6jwvz84CkyUREfhSmrVTp0yL6sljd9/70UHi7961/ePS4AAKi+CE5AVTkzRAVyyXOzigepMM//OklIsL8t/ft7sX+S/u//7F07fty7xwUAANUPwQnwh+IlzwNxEV53OfpfiWp+y5bZL7ezWr3btbPOsh+TAhIAAMBTBCcgkJxxr1TQVu8rrng1vwrWmoqJsd/7NHeud7ths1FAAgAAeI7gBASy4tX7gn1WyqGstabOMHKkPUA1berdUzsKSLRqxQK6AADAPIITEGyKz0qF0uxUsfBktUq//SadPGkv9uBNW7fa37Irr6QCHwAAqBjBCQgVxWenEhL83SPPlHJfVFycVFDg/bWfJOmTT6jABwAAKkZwAkJVRkbpM1PBYPfuUqeYHGs/deni/VM6KvAdOuT9YwMAgOBHcAKqm2AJU0VF9iRTLEBZrfZ1mnJy7DNR3lavnv20R496/9gAACB4EZwABHaYcgSo6GiXzTEx9nufMjJ8Uy/j7LMJUAAA4E8EJwClKx6kevb0b3/y8koNUAkJ9noZ3i5f7nD22VJkZJgyMnxzfAAAEBz8HpxeeuklpaSkKDo6Wt27d9e6devKbLt582Zdf/31SklJkcVi0cyZM6uuo0B1l55e+syUt1errYgjQFksUq9ezs2O8uVdu/ripFbddtvViowM4x4oAACqKb8Gp0WLFik1NVWTJk3Shg0b1KFDB/Xv319HjhwptX1OTo6aNm2qp556SsnJyVXcWwClKiz031pTa9a4hCirVfruO/v9T7Vre/tkFklW5z1Q779PGXMAAKoTL6+M4p4ZM2Zo1KhRGjlypCRp9uzZ+uSTTzR37lw9/PDDJdp369ZN3bp1k6RSny9NXl6e8vLynD9nZWVJkgoKClRQUFDZl1ApjvP7ux/wLsZV9lmhYsKmTFHYE0/IkG/+xcZwhChJkY89pkOHJio7W0pODlN+fpjswcd7rr9ekgylptr097/bqnziDZXH72roYUxDE+MaegJpTN3pg8Uw/HMXeH5+vmJjY/Wf//xHgwYNcm4fMWKEMjIy9OGHH5a7f0pKisaPH6/x48eX2y4tLU2TJ08usX3BggWKjY31pOsAKuHqP37ffTkndeZfaobFovlvfKDhw6+UPbL54syGLrhgj1JTNyoy0geHBwAAPpGTk6OhQ4cqMzNT8fHx5bb124zTsWPHVFRUpKSkJJftSUlJ+uWXX7x2ngkTJig1NdX5c1ZWlho1aqR+/fpV+Ob4WkFBgVasWKG+ffsqIiLCr32B9zCu5SvMz1d4XJyM/HyfhSeX4xqGbhk+SLf88WOeIhWj3OKtKn3Gb75prJtuOkeNGtn04482n5RKh3fxuxp6GNPQxLiGnkAaU8fVaGb49VK9qhAVFaWoqKgS2yMiIvw+UA6B1Bd4D+Najrw8KSXFvtCtjxWPR9HKl+2MiwUNSVM0SZOV5pWz7d1rVe3aVkVH2xfTTUjwwmHhU/yuhh7GNDQxrqEnEMbUnfP7rThEnTp1ZLVadfjwYZfthw8fpvADUB3s2mUvJNG4cZWf2nLGI0zSJE2WTRbZZFGhwjTJCyEqN1dKTLQXHWQtKAAAgp/fglNkZKS6dOmiVatWObfZbDatWrVKPXr08Fe3AFQ1R4Dy49TMmUHKKsMlSNlk0efq5fGxbbY/F9OlEh8AAMHLr+XIU1NTNWfOHL3xxhvasmWLRo8erVOnTjmr7A0fPlwTJkxwts/Pz9fGjRu1ceNG5efna//+/dq4caO2b9/ur5cAwFsyMvweoBwsxR49tcYrQer666XwcKljRyk722vdBQAAVcCv9zgNHjxYR48e1cSJE3Xo0CF17NhRy5YtcxaM2LNnj8LC/sx2Bw4cUKdOnZw/T58+XdOnT1fPnj2Vnp5e1d0H4AsZGX/+OTw8IKZoit8n5QhSZwqT+QKlP/4o1axpv4zv4EGpbl0vdBIAAPiU34tDjB07VmPHji31ueJhKCUlRX6qng7AHwoL//xzWJh9RioAFA9SNqlEkFqjnvqL0ss9TlGR/TI+SXrnHem668R6UAAABCi/XqoHAKbZbPbgFCDh6UyO1aHKu7zvhBLLPcZNN9kn2Jo3lzIzfd5lAADgJoITgODjCFCGIfXs6e/elKp4kEpQpkuQKj5D5fDbb/ZqfBSTAAAgsBCcAAS39HTXIOWH8uZmFA9ShlQiSO1Uiss+jmISzEIBAOB/BCcAocVR3tzxmDTJ3z0qwVDpl/edo92lXt535izUu+8yCwUAgD8QnACEtrS0gLu0r/SL9EqGqfhSLu/bfFOawsOlBg2k48errMsAAFR7BCcA1UvxS/sMwz6VE4BKm5VyLM6774BFtc6yyGYJ04ABrAsFAICvEZwA4MyKfaUEqUCq41c8SEmGPv3Moho1LTIsfzx69vJjDwEACE0EJwAorliQsj32mAwFVoByKG1WyvhijTNEyWKx3yAFAAAqheAEABWwTZyoJYsXqzA/P+Av75Ncw5QkGZmZf4YoxyMtzX8dBAAgCBGcAMATxS/vi4ryd4/KdGaIcpo82TVIpaRUfccAAAgiBCcA8Ibc3JJFJ4LJ7t2uQcpikXr18nevAAAIGAQnAPCV4kEqgGelSrVmjWuQ4l4pAEA1RnACgKpSfFYqIaFEk4Cep8rMLDkrxb1SAIBqguAEAP6SkVFiVspZ0OGMR0Arfq8Ul/gBAEIUwQkAAokjQDkePXu6hKigCFPFL/FjZgoAEAIITgAQyNLTnSGqIM/QpMcNFckaXEFKKjkzFR3t7x4BAOAWghMABInISGnKFCncKFRWhqFuXQyFyVCeooJvViovr+SsVBj/SwIABC7+LwUAQSghQfr+e/uVfRkHcxUdaQ9RjochS3AFKenPhYXPfISH+7tXAABIIjgBQNBLTrZP4OTlSZMm2bdZZXMJUkE5KyVJRUUlwxSX+QEA/IDgBAAhIjLSXoPBMKScHGnIkD+fi1GuS5AK2lkpqfTL/FhjCgDgYwQnAAhBMTHSggX2EHXypDRgQMk2ITMrJZW+xpTFUvF+AACYRHACgBAXFyd9+umfIap//9LbmZ2VCpowJZUeplhnCgDgAe66BYBqJC5OWrbM/ufMTKl3b2n9+rLbW2Ursa1IYSoen4JqbuePdabCJV3t2JaQYF+QGACAMjDjBADV1JmV+YrfE1We4pf4TdakoJyVspzxKPNSv5QUv/YRABA4CE4AAJd7os6szmfGZKWVuMQvUwlBGaZK2L2be6cAAJIITgCAYs6szmcY0r599mDljlrKKBGmgnVmqlSlhSnWnAKAkEZwAgCUq0ED+6V8hiH9/rvUrJlnxyltZmq3GodOmCptzSkCFQCEDIITAMC02rWl7dv/vC9q2LDKHa+JdoV2mJLKDlRh/C8YAIIJf2sDADwSEyO99dafl/QdOSIlJVX+uKWFqbLWmCpZ8y+IGEbpgYqS6QAQkAhOAACvqFtXOnTIswITFSltjakwGbKoZJgK6tkphz9Kppd62V9amr97BwDVEsEJAOB1xQtMZGRIXbp4/zylhamgX7S3PEVF0uTJpYcqAhUA+BTBCQDgc2euGeUoMtG8uW/OVXydqfLKo4dMoJLKDlRc+gcAXkFwAgBUudq1pW3bXO+PSk723flKK48esvdOlaasS/8IVQBgGsEJAOB3detKBw9WXZByqHb3TpWmvFBFKXUAcCI4AQACTvEg5ctL+0pTWpgqq0x6yAYqqexS6o5HdLS/ewgAVYbgBAAIeMUv7fPGGlLuKq1MerUNVA55eQQrANUGwQkAEHSKryHlzXWk3FVWoJqsSWUGqmoRqqSKgxWLAAMIIvyNBQAICWeuI+WvWakzTVZaqYGqvJLp1SZQOZS3CDCzVgACDMEJABCSSpuVOnlS6t/f0cL2x6Pq40ppJdMrKpte7UKVQ0WzVqxjBaCKEJwAANVGXJy0bJk9ROXnF2nx4o909GihTxbn9URZZdMrmqWqtqHqTJMnKzwyUlcPGqTwyEhmrwB4HcEJAFCtFV+c13GZ35Ah/u6Zq7JmqQhVf7Kc8SiV2dkrS5lHAFCNEZwAACgmJkZasMA1TBUWSgsXShER/u5dSYQqHzAbsAhZQLVBcAIAwASrVRo8WMrPL3nf1IAB/u5d2coLVWvUs9xQRbAyyZ2QxeWCQNAiOAEAUAlxcdKnn7qGKcOQMjKkzp393bvy/UXpZYaqitaoIlh5yJ3LBR2PXr383WsAIjgBAOATCQnS+vUlA5VrZb/AVtYaVQSrKrZmjfthizWyAK/jtwoAgCp0ZmW/4vdQLVgQXN93KwpWZi4FJFz5iJk1ssp6JCb6u/dAQAqiv54BAAhdVqu9kl9RUclQ5e/FfD1V0aWAZmetCFhVLDPT89BlsUgpKf5+BYBPEJwAAAhwpS3mG8wzVWeqaNbKbHVAwlUA2b27csGLaoUIUEH61ywAAJDKn6kyDOn336Xmzf3dy8orrzqg45GpBNOzV4SsIPBHgKpwYWMqGqKKEJwAAAhhtWtL27aVHqpCKVhJUi1lmJq9CvsjNhGwgkOFCxub5UlFQyof4gwEJwAAqrGKglUgL/xbGWYD1mRNcmsWi6BVTXlS+dCTR7BekxsiePcBAECZylr4t/hj3z77vVihZrLSTIcsTy4XJGzBLZWplsjMWqWF+7sDAAAg+DVoYK/+V578fOnJJ6XJk6umT/5QSxlu71OkMHkSnyifgKpmOGbWymD2Mxku6eriG6OipNxcD3tWNZhxAgAAVSIyUkpLK3/myvHIyJA6d/Z3j6uGmcIX3riEkBkuVJalgocnx/lzY+D/UwDBCQAABJyEBGn9enMhyzCkgwel+PgiSUWSbP7uvs+5ewnhmY88RVUqdBG84G2nLdHS6dP+7kaFCE4AACDoJSdLx47ZtHjxx8rPL6owaAXrosLeEKNcj0OXt4IXAQwOOYpWrHE6KKp7BkRweumll5SSkqLo6Gh1795d69atK7f9u+++q1atWik6Olrt2rXT0qVLq6inAAAgFJS3qDCBq3yVDV6elIQnhIUmQ1IN2WeafvtNysz0b38q4vfgtGjRIqWmpmrSpEnasGGDOnTooP79++vIkSOltv/66681ZMgQ3X777frhhx80aNAgDRo0SD///HMV9xwAAFQ3ngau6njvVkW8Eb4cjzXqqQJZvRrECGW+Z5F0Sn+W47ziCv/1xQy/B6cZM2Zo1KhRGjlypNq0aaPZs2crNjZWc+fOLbX9P//5Tw0YMEAPPPCAWrduralTp6pz58568cUXq7jnAAAA7nH33q2yHkeOSElJ/n41geMvSlekCr0axs4sxGGTbwIZIU2KVa4zPO3Z4+fOVMCv5cjz8/O1fv16TZgwwbktLCxMffr00dq1a0vdZ+3atUpNTXXZ1r9/fy1evLjU9nl5ecrLy3P+nJWVJUkqKChQQUFBJV9B5TjO7+9+wLsY19DDmIYmxjX0VKcxTUyU9u713vGys6XBg8O0YoWvKpu5W3ctcExWmiYrrUrONUlpmiT/1uv3xyg5wlOfhtkqKKja4i7u/H3h1+B07NgxFRUVKanYP5kkJSXpl19+KXWfQ4cOldr+0KFDpbafNm2aJpeyYMTy5csVGxvrYc+9a8WKFf7uAnyAcQ09jGloYlxDD2PqmTFj7A9fyM2VXnmlnT7//BxV3QVPwRfWqjKklaZA4bKqyC/nDpNNY8Z8rKouXZBT0QJ0Zwj5BXAnTJjgMkOVlZWlRo0aqV+/foqPj/djz+wJd8WKFerbt68iIiL82hd4D+MaehjT0MS4hh7GNLBdd51kvxjNvS/mno5rUZH0/vsW3XabRQUFgRSgAjfQRajQD2e1X6DYrJlNW26q+qUEHFejmeHX4FSnTh1ZrVYdPnzYZfvhw4eVnJxc6j7JyclutY+KilJUVFSJ7REREQHzl2og9QXew7iGHsY0NDGuoYcxDU3ujmtEhDR0qP0RSA4dklq2lNz4vh7iLGrWTNq+3SrJWuVnd+cz5dfiEJGRkerSpYtWrVrl3Gaz2bRq1Sr16NGj1H169Ojh0l6yT8mX1R4AAAAIFMnJ9rLblS0Q4u9H5SpE2iTZ1KVLkTIypO3bvfb2+pTfL9VLTU3ViBEj1LVrV51//vmaOXOmTp06pZEjR0qShg8frgYNGmjatGmSpHHjxqlnz5569tlndcUVV2jhwoX6/vvv9eqrr/rzZQAAAADVhqNCpCcKCoq0dOlSDRw4UBERVT/L5Cm/B6fBgwfr6NGjmjhxog4dOqSOHTtq2bJlzgIQe/bsUVjYnxNjF154oRYsWKDHHntMjzzyiFq0aKHFixerbdu2/noJAAAAAEKc34OTJI0dO1Zjx44t9bn09PQS22688UbdeOONPu4VAAAAANj5fQFcAAAAAAh0BCcAAAAAqADBCQAAAAAqQHACAAAAgAoQnAAAAACgAgQnAAAAAKgAwQkAAAAAKkBwAgAAAIAKEJwAAAAAoAIEJwAAAACoAMEJAAAAACoQ7u8OVDXDMCRJWVlZfu6JVFBQoJycHGVlZSkiIsLf3YGXMK6hhzENTYxr6GFMQxPjGnoCaUwdmcCREcpT7YLTyZMnJUmNGjXyc08AAAAABIKTJ08qISGh3DYWw0y8CiE2m00HDhxQzZo1ZbFY/NqXrKwsNWrUSHv37lV8fLxf+wLvYVxDD2MamhjX0MOYhibGNfQE0pgahqGTJ0+qfv36Cgsr/y6majfjFBYWpoYNG/q7Gy7i4+P9/qGB9zGuoYcxDU2Ma+hhTEMT4xp6AmVMK5ppcqA4BAAAAABUgOAEAAAAABUgOPlRVFSUJk2apKioKH93BV7EuIYexjQ0Ma6hhzENTYxr6AnWMa12xSEAAAAAwF3MOAEAAABABQhOAAAAAFABghMAAAAAVIDgBAAAAAAVIDj50UsvvaSUlBRFR0ere/fuWrdunb+7hD988cUXuuqqq1S/fn1ZLBYtXrzY5XnDMDRx4kTVq1dPMTEx6tOnj7Zt2+bS5vjx4xo2bJji4+OVmJio22+/XdnZ2S5tfvrpJ11yySWKjo5Wo0aN9Mwzz/j6pVVb06ZNU7du3VSzZk2dffbZGjRokLZu3erSJjc3V2PGjNFZZ52luLg4XX/99Tp8+LBLmz179uiKK65QbGyszj77bD3wwAMqLCx0aZOenq7OnTsrKipKzZs317x583z98qqlWbNmqX379s4FFHv06KFPP/3U+TzjGfyeeuopWSwWjR8/3rmNcQ0+aWlpslgsLo9WrVo5n2dMg9P+/ft1yy236KyzzlJMTIzatWun77//3vl8SH5XMuAXCxcuNCIjI425c+camzdvNkaNGmUkJiYahw8f9nfXYBjG0qVLjUcffdR4//33DUnGBx984PL8U089ZSQkJBiLFy82fvzxR+Pqq682mjRpYpw+fdrZZsCAAUaHDh2Mb775xvjvf/9rNG/e3BgyZIjz+czMTCMpKckYNmyY8fPPPxtvv/22ERMTY7zyyitV9TKrlf79+xuvv/668fPPPxsbN240Bg4caJxzzjlGdna2s82dd95pNGrUyFi1apXx/fffGxdccIFx4YUXOp8vLCw02rZta/Tp08f44YcfjKVLlxp16tQxJkyY4GyzY8cOIzY21khNTTX+97//GS+88IJhtVqNZcuWVenrrQ6WLFlifPLJJ8avv/5qbN261XjkkUeMiIgI4+effzYMg/EMduvWrTNSUlKM9u3bG+PGjXNuZ1yDz6RJk4zzzjvPOHjwoPNx9OhR5/OMafA5fvy40bhxY+O2224zvv32W2PHjh3GZ599Zmzfvt3ZJhS/KxGc/OT88883xowZ4/y5qKjIqF+/vjFt2jQ/9gqlKR6cbDabkZycbPzjH/9wbsvIyDCioqKMt99+2zAMw/jf//5nSDK+++47Z5tPP/3UsFgsxv79+w3DMIyXX37ZqFWrlpGXl+ds89BDDxktW7b08SuCYRjGkSNHDEnGmjVrDMOwj2FERITx7rvvOtts2bLFkGSsXbvWMAx7oA4LCzMOHTrkbDNr1iwjPj7eOY4PPvigcd5557mca/DgwUb//v19/ZJgGEatWrWM1157jfEMcidPnjRatGhhrFixwujZs6czODGuwWnSpElGhw4dSn2OMQ1ODz30kHHxxReX+XyoflfiUj0/yM/P1/r169WnTx/ntrCwMPXp00dr1671Y89gxs6dO3Xo0CGX8UtISFD37t2d47d27VolJiaqa9euzjZ9+vRRWFiYvv32W2ebSy+9VJGRkc42/fv319atW3XixIkqejXVV2ZmpiSpdu3akqT169eroKDAZVxbtWqlc845x2Vc27Vrp6SkJGeb/v37KysrS5s3b3a2OfMYjjb8bvtWUVGRFi5cqFOnTqlHjx6MZ5AbM2aMrrjiihLvPeMavLZt26b69euradOmGjZsmPbs2SOJMQ1WS5YsUdeuXXXjjTfq7LPPVqdOnTRnzhzn86H6XYng5AfHjh1TUVGRy18AkpSUlKRDhw75qVcwyzFG5Y3foUOHdPbZZ7s8Hx4ertq1a7u0Ke0YZ54DvmGz2TR+/HhddNFFatu2rST7ex4ZGanExESXtsXHtaIxK6tNVlaWTp8+7YuXU61t2rRJcXFxioqK0p133qkPPvhAbdq0YTyD2MKFC7VhwwZNmzatxHOMa3Dq3r275s2bp2XLlmnWrFnauXOnLrnkEp08eZIxDVI7duzQrFmz1KJFC3322WcaPXq07rnnHr3xxhuSQve7UniVnxEA/GzMmDH6+eef9eWXX/q7K6ikli1bauPGjcrMzNR//vMfjRgxQmvWrPF3t+ChvXv3aty4cVqxYoWio6P93R14yeWXX+78c/v27dW9e3c1btxY77zzjmJiYvzYM3jKZrOpa9euevLJJyVJnTp10s8//6zZs2drxIgRfu6d7zDj5Ad16tSR1WotUTHm8OHDSk5O9lOvYJZjjMobv+TkZB05csTl+cLCQh0/ftylTWnHOPMc8L6xY8fq448/1ueff66GDRs6tycnJys/P18ZGRku7YuPa0VjVlab+Ph4viD4QGRkpJo3b64uXbpo2rRp6tChg/75z38ynkFq/fr1OnLkiDp37qzw8HCFh4drzZo1ev755xUeHq6kpCTGNQQkJibq3HPP1fbt2/ldDVL16tVTmzZtXLa1bt3aeQlmqH5XIjj5QWRkpLp06aJVq1Y5t9lsNq1atUo9evTwY89gRpMmTZScnOwyfllZWfr222+d49ejRw9lZGRo/fr1zjarV6+WzWZT9+7dnW2++OILFRQUONusWLFCLVu2VK1ataro1VQfhmFo7Nix+uCDD7R69Wo1adLE5fkuXbooIiLCZVy3bt2qPXv2uIzrpk2bXP6iX7FiheLj453/A+nRo4fLMRxt+N2uGjabTXl5eYxnkOrdu7c2bdqkjRs3Oh9du3bVsGHDnH9mXINfdna2fvvtN9WrV4/f1SB10UUXlVjS49dff1Xjxo0lhfB3Jb+UpICxcOFCIyoqypg3b57xv//9z7jjjjuMxMREl4ox8J+TJ08aP/zwg/HDDz8YkowZM2YYP/zwg7F7927DMOwlNhMTE40PP/zQ+Omnn4xrrrmm1BKbnTp1Mr799lvjyy+/NFq0aOFSYjMjI8NISkoybr31VuPnn382Fi5caMTGxlKO3EdGjx5tJCQkGOnp6S4lcXNycpxt7rzzTuOcc84xVq9ebXz//fdGjx49jB49ejifd5TE7devn7Fx40Zj2bJlRt26dUstifvAAw8YW7ZsMV566SVK4vrIww8/bKxZs8bYuXOn8dNPPxkPP/ywYbFYjOXLlxuGwXiGijOr6hkG4xqM7rvvPiM9Pd3YuXOn8dVXXxl9+vQx6tSpYxw5csQwDMY0GK1bt84IDw83/v73vxvbtm0z5s+fb8TGxhpvvfWWs00oflciOPnRCy+8YJxzzjlGZGSkcf755xvffPONv7uEP3z++eeGpBKPESNGGIZhL7P5+OOPG0lJSUZUVJTRu3dvY+vWrS7H+P33340hQ4YYcXFxRnx8vDFy5Ejj5MmTLm1+/PFH4+KLLzaioqKMBg0aGE899VRVvcRqp7TxlGS8/vrrzjanT5827rrrLqNWrVpGbGysce211xoHDx50Oc6uXbuMyy+/3IiJiTHq1Klj3HfffUZBQYFLm88//9zo2LGjERkZaTRt2tTlHPCev/71r0bjxo2NyMhIo27dukbv3r2dockwGM9QUTw4Ma7BZ/DgwUa9evWMyMhIo0GDBsbgwYNd1vthTIPTRx99ZLRt29aIiooyWrVqZbz66qsuz4fidyWLYRhG1c9zAQAAAEDw4B4nAAAAAKgAwQkAAAAAKkBwAgAAAIAKEJwAAAAAoAIEJwAAAACoAMEJAAAAACpAcAIAAACAChCcAAAAAKACBCcAANxgsVi0ePFif3cDAFDFCE4AgKBx2223yWKxlHgMGDDA310DAIS4cH93AAAAdwwYMECvv/66y7aoqCg/9QYAUF0w4wQACCpRUVFKTk52edSqVUuS/TK6WbNm6fLLL1dMTIyaNm2q//znPy77b9q0SZdddpliYmJ01lln6Y477lB2drZLm7lz5+q8885TVFSU6tWrp7Fjx7o8f+zYMV177bWKjY1VixYttGTJEt++aACA3xGcAAAh5fHHH9f111+vH3/8UcOGDdPNN9+sLVu2SJJOnTql/v37q1atWvruu+/07rvvauXKlS7BaNasWRozZozuuOMObdq0SUuWLFHz5s1dzjF58mTddNNN+umnnzRw4EANGzZMx48fr9LXCQCoWhbDMAx/dwIAADNuu+02vfXWW4qOjnbZ/sgjj+iRRx6RxWLRnXfeqVmzZjmfu+CCC9S5c2e9/PLLmjNnjh566CHt3btXNWrUkCQtXbpUV111lQ4cOKCkpCQ1aNBAI0eO1BNPPFFqHywWix577DFNnTpVkj2MxcXF6dNPP+VeKwAIYdzjBAAIKn/5y19cgpEk1a5d2/nnHj16uDzXo0cPbdy4UZK0ZcsWdejQwRmaJOmiiy6SzWbT1q1bZbFYdODAAfXu3bvcPrRv39755xo1aig+Pl5Hjhzx9CUBAIIAwQkAEFRq1KhR4tI5b4mJiTHVLiIiwuVni8Uim83miy4BAAIE9zgBAELKN998U+Ln1q1bS5Jat26tH3/8UadOnXI+/9VXXyksLEwtW7ZUzZo1lZKSolWrVlVpnwEAgY8ZJwBAUMnLy9OhQ4dctoWHh6tOnTqSpHfffVddu3bVxRdfrPnz52vdunX617/+JUkaNmyYJk2apBEjRigtLU1Hjx7V3XffrVtvvVVJSUmSpLS0NN155506++yzdfnll+vkyZP66quvdPfdd1ftCwUABBSCEwAgqCxbtkz16tVz2dayZUv98ssvkuwV7xYuXKi77rpL9erV09tvv602bdpIkmJjY/XZZ59p3Lhx6tatm2JjY3X99ddrxowZzmONGDFCubm5eu6553T//ferTp06uuGGG6ruBQIAAhJV9QAAIcNiseiDDz7QoEGD/N0VAECI4R4nAAAAAKgAwQkAAAAAKsA9TgCAkMHV5wAAX2HGCQAAAAAqQHACAAAAgAoQnAAAAACgAgQnAAAAAKgAwQkAAAAAKkBwAgAAAIAKEJwAAAAAoAIEJwAAAACowP8DPMURqg4RcC0AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Sample dataset\n",
        "X = torch.tensor([[1.0, 0.5],\n",
        "                  [0.9, 0.7],\n",
        "                  [0.4, 0.6],\n",
        "                  [0.3, 0.4],\n",
        "                  [1.1, 0.8],\n",
        "                  [0.6, 0.9]], dtype=torch.float32)\n",
        "\n",
        "#y = torch.tensor([[1, 1], [1, 0], [0, 1], [0, 0], [1, 1], [0, 0]], dtype=torch.float32)\n",
        "y = torch.tensor([[1, 1], [1, 1], [0, 1], [0, 0], [1, 1], [1, 1]], dtype=torch.float32)\n",
        "\n",
        "# Scaling the data\n",
        "standard_scaler = StandardScaler()\n",
        "data_standardized = standard_scaler.fit_transform(X)\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "data_normalized = min_max_scaler.fit_transform(data_standardized)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(data_normalized, dtype=torch.float32)\n",
        "y = y.clone().detach().float()\n",
        "\n",
        "# Splitting into training and testing sets (80:20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for mini-batch gradient descent\n",
        "batch_size = 2  # Adjust batch size here\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Neural Network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 3)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(3, 2)\n",
        "        self.act2 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with Mini-Batch Gradient Descent\n",
        "no_of_epoch = 6000\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "for epoch in range(no_of_epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Mini-batch training loop\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Optimizer gradients are zero till now so that gradient are calculated only for current batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs_train = model(batch_X)\n",
        "        train_loss = criterion(outputs_train, batch_y)\n",
        "\n",
        "        # Backward pass compute gradients\n",
        "        train_loss.backward()\n",
        "\n",
        "        # Optimizer update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += train_loss.item()\n",
        "\n",
        "    # Store the average training loss\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "\n",
        "    # Testing phase (without gradient calculation)\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs_test = model(batch_X)\n",
        "            batch_test_loss = criterion(outputs_test, batch_y)\n",
        "            test_loss += batch_test_loss.item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    test_loss_history.append(avg_test_loss)\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch [{epoch + 1}/{no_of_epoch}], \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "# Plotting the Training and Testing Error\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, no_of_epoch + 1), train_loss_history, label='Training Loss', color='blue', marker='o')\n",
        "plt.plot(range(1, no_of_epoch + 1), test_loss_history, label='Testing Loss', color='red', marker='x')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs. Testing Loss (Mini-Batch GD)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# With data of size 40 x 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Train Loss: 0.7574, Test Loss: 0.6318\n",
            "Epoch [2/100], Train Loss: 0.7568, Test Loss: 0.6320\n",
            "Epoch [3/100], Train Loss: 0.7563, Test Loss: 0.6321\n",
            "Epoch [4/100], Train Loss: 0.7557, Test Loss: 0.6322\n",
            "Epoch [5/100], Train Loss: 0.7552, Test Loss: 0.6324\n",
            "Epoch [6/100], Train Loss: 0.7546, Test Loss: 0.6325\n",
            "Epoch [7/100], Train Loss: 0.7541, Test Loss: 0.6327\n",
            "Epoch [8/100], Train Loss: 0.7536, Test Loss: 0.6328\n",
            "Epoch [9/100], Train Loss: 0.7531, Test Loss: 0.6330\n",
            "Epoch [10/100], Train Loss: 0.7525, Test Loss: 0.6331\n",
            "Epoch [11/100], Train Loss: 0.7520, Test Loss: 0.6333\n",
            "Epoch [12/100], Train Loss: 0.7515, Test Loss: 0.6334\n",
            "Epoch [13/100], Train Loss: 0.7510, Test Loss: 0.6336\n",
            "Epoch [14/100], Train Loss: 0.7505, Test Loss: 0.6338\n",
            "Epoch [15/100], Train Loss: 0.7500, Test Loss: 0.6339\n",
            "Epoch [16/100], Train Loss: 0.7496, Test Loss: 0.6341\n",
            "Epoch [17/100], Train Loss: 0.7491, Test Loss: 0.6343\n",
            "Epoch [18/100], Train Loss: 0.7486, Test Loss: 0.6344\n",
            "Epoch [19/100], Train Loss: 0.7481, Test Loss: 0.6346\n",
            "Epoch [20/100], Train Loss: 0.7477, Test Loss: 0.6348\n",
            "Epoch [21/100], Train Loss: 0.7472, Test Loss: 0.6349\n",
            "Epoch [22/100], Train Loss: 0.7468, Test Loss: 0.6351\n",
            "Epoch [23/100], Train Loss: 0.7463, Test Loss: 0.6353\n",
            "Epoch [24/100], Train Loss: 0.7459, Test Loss: 0.6355\n",
            "Epoch [25/100], Train Loss: 0.7454, Test Loss: 0.6357\n",
            "Epoch [26/100], Train Loss: 0.7450, Test Loss: 0.6358\n",
            "Epoch [27/100], Train Loss: 0.7445, Test Loss: 0.6360\n",
            "Epoch [28/100], Train Loss: 0.7441, Test Loss: 0.6362\n",
            "Epoch [29/100], Train Loss: 0.7437, Test Loss: 0.6364\n",
            "Epoch [30/100], Train Loss: 0.7433, Test Loss: 0.6366\n",
            "Epoch [31/100], Train Loss: 0.7429, Test Loss: 0.6368\n",
            "Epoch [32/100], Train Loss: 0.7425, Test Loss: 0.6370\n",
            "Epoch [33/100], Train Loss: 0.7421, Test Loss: 0.6372\n",
            "Epoch [34/100], Train Loss: 0.7417, Test Loss: 0.6373\n",
            "Epoch [35/100], Train Loss: 0.7413, Test Loss: 0.6375\n",
            "Epoch [36/100], Train Loss: 0.7409, Test Loss: 0.6377\n",
            "Epoch [37/100], Train Loss: 0.7405, Test Loss: 0.6379\n",
            "Epoch [38/100], Train Loss: 0.7402, Test Loss: 0.6381\n",
            "Epoch [39/100], Train Loss: 0.7398, Test Loss: 0.6383\n",
            "Epoch [40/100], Train Loss: 0.7395, Test Loss: 0.6385\n",
            "Epoch [41/100], Train Loss: 0.7391, Test Loss: 0.6387\n",
            "Epoch [42/100], Train Loss: 0.7387, Test Loss: 0.6389\n",
            "Epoch [43/100], Train Loss: 0.7384, Test Loss: 0.6391\n",
            "Epoch [44/100], Train Loss: 0.7380, Test Loss: 0.6392\n",
            "Epoch [45/100], Train Loss: 0.7377, Test Loss: 0.6394\n",
            "Epoch [46/100], Train Loss: 0.7374, Test Loss: 0.6396\n",
            "Epoch [47/100], Train Loss: 0.7370, Test Loss: 0.6398\n",
            "Epoch [48/100], Train Loss: 0.7367, Test Loss: 0.6400\n",
            "Epoch [49/100], Train Loss: 0.7364, Test Loss: 0.6402\n",
            "Epoch [50/100], Train Loss: 0.7360, Test Loss: 0.6404\n",
            "Epoch [51/100], Train Loss: 0.7357, Test Loss: 0.6406\n",
            "Epoch [52/100], Train Loss: 0.7354, Test Loss: 0.6408\n",
            "Epoch [53/100], Train Loss: 0.7351, Test Loss: 0.6410\n",
            "Epoch [54/100], Train Loss: 0.7348, Test Loss: 0.6412\n",
            "Epoch [55/100], Train Loss: 0.7345, Test Loss: 0.6414\n",
            "Epoch [56/100], Train Loss: 0.7341, Test Loss: 0.6416\n",
            "Epoch [57/100], Train Loss: 0.7338, Test Loss: 0.6418\n",
            "Epoch [58/100], Train Loss: 0.7335, Test Loss: 0.6420\n",
            "Epoch [59/100], Train Loss: 0.7332, Test Loss: 0.6422\n",
            "Epoch [60/100], Train Loss: 0.7329, Test Loss: 0.6424\n",
            "Epoch [61/100], Train Loss: 0.7327, Test Loss: 0.6426\n",
            "Epoch [62/100], Train Loss: 0.7324, Test Loss: 0.6428\n",
            "Epoch [63/100], Train Loss: 0.7321, Test Loss: 0.6430\n",
            "Epoch [64/100], Train Loss: 0.7318, Test Loss: 0.6432\n",
            "Epoch [65/100], Train Loss: 0.7315, Test Loss: 0.6434\n",
            "Epoch [66/100], Train Loss: 0.7312, Test Loss: 0.6436\n",
            "Epoch [67/100], Train Loss: 0.7309, Test Loss: 0.6438\n",
            "Epoch [68/100], Train Loss: 0.7306, Test Loss: 0.6440\n",
            "Epoch [69/100], Train Loss: 0.7303, Test Loss: 0.6442\n",
            "Epoch [70/100], Train Loss: 0.7301, Test Loss: 0.6445\n",
            "Epoch [71/100], Train Loss: 0.7298, Test Loss: 0.6447\n",
            "Epoch [72/100], Train Loss: 0.7295, Test Loss: 0.6449\n",
            "Epoch [73/100], Train Loss: 0.7292, Test Loss: 0.6451\n",
            "Epoch [74/100], Train Loss: 0.7289, Test Loss: 0.6453\n",
            "Epoch [75/100], Train Loss: 0.7287, Test Loss: 0.6455\n",
            "Epoch [76/100], Train Loss: 0.7284, Test Loss: 0.6457\n",
            "Epoch [77/100], Train Loss: 0.7281, Test Loss: 0.6459\n",
            "Epoch [78/100], Train Loss: 0.7279, Test Loss: 0.6460\n",
            "Epoch [79/100], Train Loss: 0.7276, Test Loss: 0.6462\n",
            "Epoch [80/100], Train Loss: 0.7274, Test Loss: 0.6464\n",
            "Epoch [81/100], Train Loss: 0.7271, Test Loss: 0.6466\n",
            "Epoch [82/100], Train Loss: 0.7269, Test Loss: 0.6467\n",
            "Epoch [83/100], Train Loss: 0.7266, Test Loss: 0.6469\n",
            "Epoch [84/100], Train Loss: 0.7264, Test Loss: 0.6471\n",
            "Epoch [85/100], Train Loss: 0.7261, Test Loss: 0.6473\n",
            "Epoch [86/100], Train Loss: 0.7259, Test Loss: 0.6475\n",
            "Epoch [87/100], Train Loss: 0.7256, Test Loss: 0.6476\n",
            "Epoch [88/100], Train Loss: 0.7254, Test Loss: 0.6478\n",
            "Epoch [89/100], Train Loss: 0.7252, Test Loss: 0.6480\n",
            "Epoch [90/100], Train Loss: 0.7249, Test Loss: 0.6482\n",
            "Epoch [91/100], Train Loss: 0.7247, Test Loss: 0.6483\n",
            "Epoch [92/100], Train Loss: 0.7244, Test Loss: 0.6485\n",
            "Epoch [93/100], Train Loss: 0.7242, Test Loss: 0.6487\n",
            "Epoch [94/100], Train Loss: 0.7240, Test Loss: 0.6489\n",
            "Epoch [95/100], Train Loss: 0.7237, Test Loss: 0.6491\n",
            "Epoch [96/100], Train Loss: 0.7235, Test Loss: 0.6492\n",
            "Epoch [97/100], Train Loss: 0.7233, Test Loss: 0.6494\n",
            "Epoch [98/100], Train Loss: 0.7230, Test Loss: 0.6496\n",
            "Epoch [99/100], Train Loss: 0.7228, Test Loss: 0.6498\n",
            "Epoch [100/100], Train Loss: 0.7226, Test Loss: 0.6500\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "import torch\n",
        "\n",
        "# Hard-coded dataset with 40 data points\n",
        "X = torch.tensor([\n",
        "    [1.0, 0.5], [0.9, 0.7], [0.4, 0.6], [0.3, 0.4], [1.1, 0.8],\n",
        "    [0.6, 0.9], [1.2, 0.4], [0.5, 1.0], [0.7, 0.3], [1.3, 0.6],\n",
        "    [0.8, 1.1], [1.0, 0.4], [0.2, 0.5], [1.4, 0.7], [0.3, 0.8],\n",
        "    [0.9, 0.6], [1.5, 0.2], [0.7, 0.9], [0.6, 1.2], [1.3, 0.5],\n",
        "    [0.4, 1.0], [1.1, 0.3], [0.8, 0.7], [1.2, 0.9], [0.5, 0.4],\n",
        "    [1.0, 0.6], [0.3, 0.7], [0.9, 0.5], [1.4, 1.0], [0.2, 0.3],\n",
        "    [1.1, 0.7], [0.4, 0.9], [1.3, 0.8], [0.6, 0.5], [1.5, 0.4],\n",
        "    [0.7, 1.0], [1.0, 0.2], [0.8, 0.6], [1.2, 0.3], [0.5, 0.8]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "y = torch.tensor([\n",
        "    [1, 0], [0, 1], [1, 0], [0, 1], [1, 1],\n",
        "    [0, 0], [1, 0], [0, 1], [1, 1], [0, 0],\n",
        "    [1, 0], [0, 1], [1, 1], [0, 0], [1, 0],\n",
        "    [0, 1], [1, 1], [0, 0], [1, 0], [0, 1],\n",
        "    [1, 1], [0, 0], [1, 0], [0, 1], [1, 1],\n",
        "    [0, 0], [1, 0], [0, 1], [1, 1], [0, 0],\n",
        "    [1, 0], [0, 1], [1, 1], [0, 0], [1, 0],\n",
        "    [0, 1], [1, 1], [0, 0], [1, 0], [0, 1]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Normalize the labels to sum to 1\n",
        "#y_sum = y.sum(dim=1, keepdim=True)  # Sum along rows\n",
        "#y = y / y_sum\n",
        "\n",
        "\n",
        "# Scaling the data\n",
        "standard_scaler = StandardScaler()\n",
        "data_standardized = standard_scaler.fit_transform(X)\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "data_normalized = min_max_scaler.fit_transform(data_standardized)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X = torch.tensor(data_normalized, dtype=torch.float32)\n",
        "y = y.clone().detach().float()\n",
        "\n",
        "# Splitting into training and testing sets (80:20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for mini-batch gradient descent\n",
        "batch_size = 40  # Adjust batch size here\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Neural Network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 3)\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(3, 2)\n",
        "        self.act2 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.act2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with Mini-Batch Gradient Descent\n",
        "no_of_epoch = 100\n",
        "train_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "for epoch in range(no_of_epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Mini-batch training loop\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Optimizer gradients are zero till now so that gradient are calculated only for current batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs_train = model(batch_X)\n",
        "        train_loss = criterion(outputs_train, batch_y)\n",
        "\n",
        "        # Backward pass compute gradients\n",
        "        train_loss.backward()\n",
        "\n",
        "        # Optimizer update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += train_loss.item()\n",
        "\n",
        "    # Store the average training loss\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_loss_history.append(avg_train_loss)\n",
        "\n",
        "    # Testing phase (without gradient calculation)\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs_test = model(batch_X)\n",
        "            batch_test_loss = criterion(outputs_test, batch_y)\n",
        "            test_loss += batch_test_loss.item()\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "    test_loss_history.append(avg_test_loss)\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"Epoch [{epoch + 1}/{no_of_epoch}], \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "# Plotting the Training and Testing Error\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, no_of_epoch + 1), train_loss_history, label='Training Loss', color='blue', marker='o')\n",
        "plt.plot(range(1, no_of_epoch + 1), test_loss_history, label='Testing Loss', color='red', marker='x')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs. Testing Loss (Mini-Batch GD)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
